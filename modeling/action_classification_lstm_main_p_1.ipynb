{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "action_classification_lstm_main_p_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRu72jqqkev9",
        "outputId": "0f485165-ab4a-40f8-cca4-8cda1c46db13"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BLeKduqiC6x"
      },
      "source": [
        "# 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iJyagkCiC6x"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1DPDkTjS698"
      },
      "source": [
        "## 1) Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEfD5zh-duIy"
      },
      "source": [
        "### (1) Punching\n",
        "\n",
        "- y : 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaj-jf06LWeo"
      },
      "source": [
        "punching_path ='/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/punching_sliding.csv'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "70vPpvadcgKT",
        "outputId": "c298c71a-3e43-46a6-cdc9-7a8865673d5b"
      },
      "source": [
        "df_punching = pd.read_csv(punching_path)\n",
        "\n",
        "df_punching.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>290.693</td>\n",
              "      <td>161.699</td>\n",
              "      <td>307.602</td>\n",
              "      <td>207.250</td>\n",
              "      <td>285.384</td>\n",
              "      <td>205.939</td>\n",
              "      <td>275.033</td>\n",
              "      <td>250.267</td>\n",
              "      <td>265.938</td>\n",
              "      <td>286.746</td>\n",
              "      <td>331.090</td>\n",
              "      <td>211.121</td>\n",
              "      <td>337.569</td>\n",
              "      <td>259.385</td>\n",
              "      <td>327.208</td>\n",
              "      <td>295.910</td>\n",
              "      <td>286.718</td>\n",
              "      <td>291.991</td>\n",
              "      <td>291.966</td>\n",
              "      <td>354.547</td>\n",
              "      <td>289.371</td>\n",
              "      <td>401.517</td>\n",
              "      <td>318.035</td>\n",
              "      <td>294.563</td>\n",
              "      <td>328.432</td>\n",
              "      <td>358.453</td>\n",
              "      <td>337.620</td>\n",
              "      <td>419.732</td>\n",
              "      <td>290.661</td>\n",
              "      <td>158.996</td>\n",
              "      <td>298.499</td>\n",
              "      <td>160.267</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.984</td>\n",
              "      <td>164.255</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>290.692</td>\n",
              "      <td>161.703</td>\n",
              "      <td>307.593</td>\n",
              "      <td>205.975</td>\n",
              "      <td>284.154</td>\n",
              "      <td>204.656</td>\n",
              "      <td>275.023</td>\n",
              "      <td>250.258</td>\n",
              "      <td>265.927</td>\n",
              "      <td>286.758</td>\n",
              "      <td>331.096</td>\n",
              "      <td>211.115</td>\n",
              "      <td>337.552</td>\n",
              "      <td>263.272</td>\n",
              "      <td>325.892</td>\n",
              "      <td>298.512</td>\n",
              "      <td>285.515</td>\n",
              "      <td>291.992</td>\n",
              "      <td>291.964</td>\n",
              "      <td>354.542</td>\n",
              "      <td>289.346</td>\n",
              "      <td>401.530</td>\n",
              "      <td>318.033</td>\n",
              "      <td>294.569</td>\n",
              "      <td>328.429</td>\n",
              "      <td>358.463</td>\n",
              "      <td>337.624</td>\n",
              "      <td>420.964</td>\n",
              "      <td>290.662</td>\n",
              "      <td>159.004</td>\n",
              "      <td>298.495</td>\n",
              "      <td>160.273</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.984</td>\n",
              "      <td>164.251</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>291.954</td>\n",
              "      <td>161.696</td>\n",
              "      <td>307.587</td>\n",
              "      <td>207.241</td>\n",
              "      <td>284.136</td>\n",
              "      <td>204.675</td>\n",
              "      <td>275.016</td>\n",
              "      <td>251.537</td>\n",
              "      <td>265.938</td>\n",
              "      <td>286.740</td>\n",
              "      <td>331.109</td>\n",
              "      <td>211.125</td>\n",
              "      <td>337.554</td>\n",
              "      <td>263.290</td>\n",
              "      <td>325.854</td>\n",
              "      <td>298.535</td>\n",
              "      <td>285.498</td>\n",
              "      <td>291.992</td>\n",
              "      <td>291.975</td>\n",
              "      <td>354.535</td>\n",
              "      <td>289.367</td>\n",
              "      <td>401.528</td>\n",
              "      <td>318.019</td>\n",
              "      <td>294.574</td>\n",
              "      <td>328.420</td>\n",
              "      <td>358.453</td>\n",
              "      <td>337.624</td>\n",
              "      <td>420.957</td>\n",
              "      <td>290.669</td>\n",
              "      <td>158.995</td>\n",
              "      <td>298.510</td>\n",
              "      <td>160.266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.990</td>\n",
              "      <td>164.251</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>290.694</td>\n",
              "      <td>161.694</td>\n",
              "      <td>307.602</td>\n",
              "      <td>207.250</td>\n",
              "      <td>284.168</td>\n",
              "      <td>205.932</td>\n",
              "      <td>275.020</td>\n",
              "      <td>251.551</td>\n",
              "      <td>265.923</td>\n",
              "      <td>286.781</td>\n",
              "      <td>331.107</td>\n",
              "      <td>211.127</td>\n",
              "      <td>337.545</td>\n",
              "      <td>263.302</td>\n",
              "      <td>325.848</td>\n",
              "      <td>299.779</td>\n",
              "      <td>285.515</td>\n",
              "      <td>293.261</td>\n",
              "      <td>291.966</td>\n",
              "      <td>354.561</td>\n",
              "      <td>289.356</td>\n",
              "      <td>401.517</td>\n",
              "      <td>318.030</td>\n",
              "      <td>294.579</td>\n",
              "      <td>328.419</td>\n",
              "      <td>358.455</td>\n",
              "      <td>337.629</td>\n",
              "      <td>420.955</td>\n",
              "      <td>290.658</td>\n",
              "      <td>158.992</td>\n",
              "      <td>298.504</td>\n",
              "      <td>160.261</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>316.802</td>\n",
              "      <td>164.252</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>290.694</td>\n",
              "      <td>161.683</td>\n",
              "      <td>307.600</td>\n",
              "      <td>207.254</td>\n",
              "      <td>284.173</td>\n",
              "      <td>205.931</td>\n",
              "      <td>275.016</td>\n",
              "      <td>251.550</td>\n",
              "      <td>265.919</td>\n",
              "      <td>286.792</td>\n",
              "      <td>331.101</td>\n",
              "      <td>211.132</td>\n",
              "      <td>337.561</td>\n",
              "      <td>263.289</td>\n",
              "      <td>325.891</td>\n",
              "      <td>299.781</td>\n",
              "      <td>286.713</td>\n",
              "      <td>293.268</td>\n",
              "      <td>291.954</td>\n",
              "      <td>354.529</td>\n",
              "      <td>289.358</td>\n",
              "      <td>401.508</td>\n",
              "      <td>318.028</td>\n",
              "      <td>294.586</td>\n",
              "      <td>328.429</td>\n",
              "      <td>358.462</td>\n",
              "      <td>337.632</td>\n",
              "      <td>420.970</td>\n",
              "      <td>290.667</td>\n",
              "      <td>158.981</td>\n",
              "      <td>298.509</td>\n",
              "      <td>160.255</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.985</td>\n",
              "      <td>164.270</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0        1        2        3        4  ...   32   33       34       35  y\n",
              "0  290.693  161.699  307.602  207.250  285.384  ...  0.0  0.0  317.984  164.255  2\n",
              "1  290.692  161.703  307.593  205.975  284.154  ...  0.0  0.0  317.984  164.251  2\n",
              "2  291.954  161.696  307.587  207.241  284.136  ...  0.0  0.0  317.990  164.251  2\n",
              "3  290.694  161.694  307.602  207.250  284.168  ...  0.0  0.0  316.802  164.252  2\n",
              "4  290.694  161.683  307.600  207.254  284.173  ...  0.0  0.0  317.985  164.270  2\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glILS8CxdyFv"
      },
      "source": [
        "### (2) Smoking\n",
        "\n",
        "- y : 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs0clNtcfJpV"
      },
      "source": [
        "smoking_path ='/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/smoking_sliding.csv'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "clOyg9k8L31I",
        "outputId": "99243a8d-e04f-41f3-c2e2-0f31b4742b0c"
      },
      "source": [
        "df_smoking = pd.read_csv(smoking_path)\n",
        "\n",
        "df_smoking.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>333.770000</td>\n",
              "      <td>85.241778</td>\n",
              "      <td>324.893667</td>\n",
              "      <td>117.939111</td>\n",
              "      <td>305.293000</td>\n",
              "      <td>119.228000</td>\n",
              "      <td>295.470667</td>\n",
              "      <td>162.394222</td>\n",
              "      <td>297.418667</td>\n",
              "      <td>201.668889</td>\n",
              "      <td>341.613333</td>\n",
              "      <td>116.635111</td>\n",
              "      <td>351.406667</td>\n",
              "      <td>154.591556</td>\n",
              "      <td>349.486667</td>\n",
              "      <td>132.359556</td>\n",
              "      <td>309.233000</td>\n",
              "      <td>204.285778</td>\n",
              "      <td>303.348000</td>\n",
              "      <td>267.131556</td>\n",
              "      <td>302.326000</td>\n",
              "      <td>323.346222</td>\n",
              "      <td>333.786667</td>\n",
              "      <td>204.287556</td>\n",
              "      <td>332.769000</td>\n",
              "      <td>267.078667</td>\n",
              "      <td>326.903333</td>\n",
              "      <td>320.686222</td>\n",
              "      <td>330.772000</td>\n",
              "      <td>78.676444</td>\n",
              "      <td>337.650000</td>\n",
              "      <td>78.681778</td>\n",
              "      <td>951.296</td>\n",
              "      <td>180.159</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>333.790000</td>\n",
              "      <td>85.257778</td>\n",
              "      <td>324.900667</td>\n",
              "      <td>117.922667</td>\n",
              "      <td>306.279667</td>\n",
              "      <td>119.208000</td>\n",
              "      <td>295.499333</td>\n",
              "      <td>162.396889</td>\n",
              "      <td>297.438667</td>\n",
              "      <td>202.923111</td>\n",
              "      <td>341.596667</td>\n",
              "      <td>116.629778</td>\n",
              "      <td>352.383333</td>\n",
              "      <td>153.326222</td>\n",
              "      <td>349.453333</td>\n",
              "      <td>129.706667</td>\n",
              "      <td>309.247667</td>\n",
              "      <td>204.291111</td>\n",
              "      <td>304.301000</td>\n",
              "      <td>267.088000</td>\n",
              "      <td>302.311000</td>\n",
              "      <td>323.341333</td>\n",
              "      <td>333.783333</td>\n",
              "      <td>204.279111</td>\n",
              "      <td>332.776000</td>\n",
              "      <td>267.048444</td>\n",
              "      <td>327.850000</td>\n",
              "      <td>319.456000</td>\n",
              "      <td>330.793667</td>\n",
              "      <td>78.677333</td>\n",
              "      <td>338.630000</td>\n",
              "      <td>78.707111</td>\n",
              "      <td>954.022</td>\n",
              "      <td>180.145</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>334.726667</td>\n",
              "      <td>86.532000</td>\n",
              "      <td>324.929333</td>\n",
              "      <td>117.944444</td>\n",
              "      <td>307.238000</td>\n",
              "      <td>119.218222</td>\n",
              "      <td>295.492333</td>\n",
              "      <td>162.445333</td>\n",
              "      <td>297.437000</td>\n",
              "      <td>203.006222</td>\n",
              "      <td>342.553333</td>\n",
              "      <td>116.648000</td>\n",
              "      <td>356.276667</td>\n",
              "      <td>153.281333</td>\n",
              "      <td>349.423333</td>\n",
              "      <td>127.101333</td>\n",
              "      <td>309.245000</td>\n",
              "      <td>204.280444</td>\n",
              "      <td>304.305333</td>\n",
              "      <td>267.110667</td>\n",
              "      <td>302.328333</td>\n",
              "      <td>323.337333</td>\n",
              "      <td>333.783333</td>\n",
              "      <td>204.265333</td>\n",
              "      <td>332.779667</td>\n",
              "      <td>267.073778</td>\n",
              "      <td>327.848000</td>\n",
              "      <td>319.466667</td>\n",
              "      <td>331.762333</td>\n",
              "      <td>78.708889</td>\n",
              "      <td>339.593333</td>\n",
              "      <td>78.735111</td>\n",
              "      <td>954.154</td>\n",
              "      <td>180.149</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>334.750000</td>\n",
              "      <td>86.560000</td>\n",
              "      <td>325.887000</td>\n",
              "      <td>116.626667</td>\n",
              "      <td>308.246000</td>\n",
              "      <td>117.935111</td>\n",
              "      <td>296.467333</td>\n",
              "      <td>161.117333</td>\n",
              "      <td>296.472000</td>\n",
              "      <td>201.634222</td>\n",
              "      <td>344.516667</td>\n",
              "      <td>115.312889</td>\n",
              "      <td>356.326667</td>\n",
              "      <td>153.299111</td>\n",
              "      <td>349.396667</td>\n",
              "      <td>125.814222</td>\n",
              "      <td>309.236667</td>\n",
              "      <td>203.004889</td>\n",
              "      <td>304.324333</td>\n",
              "      <td>267.065333</td>\n",
              "      <td>302.322000</td>\n",
              "      <td>322.046222</td>\n",
              "      <td>334.706667</td>\n",
              "      <td>203.014222</td>\n",
              "      <td>332.779000</td>\n",
              "      <td>267.043111</td>\n",
              "      <td>327.851000</td>\n",
              "      <td>320.692000</td>\n",
              "      <td>331.787000</td>\n",
              "      <td>78.740889</td>\n",
              "      <td>339.616667</td>\n",
              "      <td>78.747556</td>\n",
              "      <td>954.270</td>\n",
              "      <td>183.041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>335.693333</td>\n",
              "      <td>86.576000</td>\n",
              "      <td>325.897333</td>\n",
              "      <td>116.635556</td>\n",
              "      <td>309.174667</td>\n",
              "      <td>117.969778</td>\n",
              "      <td>296.454333</td>\n",
              "      <td>161.153778</td>\n",
              "      <td>296.440000</td>\n",
              "      <td>201.635556</td>\n",
              "      <td>344.510000</td>\n",
              "      <td>114.017778</td>\n",
              "      <td>357.293333</td>\n",
              "      <td>153.281778</td>\n",
              "      <td>348.490000</td>\n",
              "      <td>121.908000</td>\n",
              "      <td>309.245667</td>\n",
              "      <td>202.997778</td>\n",
              "      <td>306.263000</td>\n",
              "      <td>265.787556</td>\n",
              "      <td>302.312667</td>\n",
              "      <td>322.045333</td>\n",
              "      <td>334.703333</td>\n",
              "      <td>203.001333</td>\n",
              "      <td>332.793667</td>\n",
              "      <td>265.820000</td>\n",
              "      <td>326.898667</td>\n",
              "      <td>320.699556</td>\n",
              "      <td>332.704000</td>\n",
              "      <td>78.756000</td>\n",
              "      <td>340.543333</td>\n",
              "      <td>78.752889</td>\n",
              "      <td>957.030</td>\n",
              "      <td>183.052</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0          1           2           3  ...       33   34   35  y\n",
              "0  333.770000  85.241778  324.893667  117.939111  ...  180.159  0.0  0.0  1\n",
              "1  333.790000  85.257778  324.900667  117.922667  ...  180.145  0.0  0.0  1\n",
              "2  334.726667  86.532000  324.929333  117.944444  ...  180.149  0.0  0.0  1\n",
              "3  334.750000  86.560000  325.887000  116.626667  ...  183.041  0.0  0.0  1\n",
              "4  335.693333  86.576000  325.897333  116.635556  ...  183.052  0.0  0.0  1\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K61ouwsPfDtJ"
      },
      "source": [
        "### (3) Walking\n",
        "\n",
        "- y : 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMq2pTeffG9x"
      },
      "source": [
        "walking_path ='/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/walking_sliding.csv'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "hiUMb5omfSvs",
        "outputId": "e9c908fc-7659-4cfb-a080-d08072511fa2"
      },
      "source": [
        "df_walking = pd.read_csv(walking_path)\n",
        "\n",
        "df_walking.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.591563</td>\n",
              "      <td>268.375111</td>\n",
              "      <td>34.471000</td>\n",
              "      <td>318.115556</td>\n",
              "      <td>18.768733</td>\n",
              "      <td>412.252000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.119073</td>\n",
              "      <td>276.197333</td>\n",
              "      <td>38.427667</td>\n",
              "      <td>324.640444</td>\n",
              "      <td>40.388333</td>\n",
              "      <td>413.647111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.973267</td>\n",
              "      <td>261.836000</td>\n",
              "      <td>35.534667</td>\n",
              "      <td>331.182667</td>\n",
              "      <td>56.094333</td>\n",
              "      <td>409.643111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.865767</td>\n",
              "      <td>48.626222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.950300</td>\n",
              "      <td>36.842400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.81100</td>\n",
              "      <td>74.0119</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25.678567</td>\n",
              "      <td>46.038667</td>\n",
              "      <td>2.602713</td>\n",
              "      <td>87.830667</td>\n",
              "      <td>2.610417</td>\n",
              "      <td>87.808889</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.9581</td>\n",
              "      <td>87.867111</td>\n",
              "      <td>18.7978</td>\n",
              "      <td>214.746667</td>\n",
              "      <td>57.039</td>\n",
              "      <td>244.859556</td>\n",
              "      <td>9.970833</td>\n",
              "      <td>251.376889</td>\n",
              "      <td>51.176333</td>\n",
              "      <td>325.939111</td>\n",
              "      <td>90.450667</td>\n",
              "      <td>410.959111</td>\n",
              "      <td>12.930667</td>\n",
              "      <td>246.168444</td>\n",
              "      <td>53.159000</td>\n",
              "      <td>325.941778</td>\n",
              "      <td>91.442333</td>\n",
              "      <td>407.053333</td>\n",
              "      <td>19.800967</td>\n",
              "      <td>36.828222</td>\n",
              "      <td>26.649067</td>\n",
              "      <td>38.136844</td>\n",
              "      <td>7.78672</td>\n",
              "      <td>82.8243</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0          1         2          3  ...       33   34   35  y\n",
              "0   0.000000   0.000000  0.000000   0.000000  ...   0.0000  0.0  0.0  0\n",
              "1   0.000000   0.000000  0.000000   0.000000  ...   0.0000  0.0  0.0  0\n",
              "2   0.000000   0.000000  0.000000   0.000000  ...   0.0000  0.0  0.0  0\n",
              "3  14.865767  48.626222  0.000000   0.000000  ...  74.0119  0.0  0.0  0\n",
              "4  25.678567  46.038667  2.602713  87.830667  ...  82.8243  0.0  0.0  0\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUS48VoXEWoN"
      },
      "source": [
        "### (4) Running\n",
        "\n",
        "- y : 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzJRAHywEcJK"
      },
      "source": [
        "running_path ='/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/running_sliding.csv'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "csOwXUtyEf6s",
        "outputId": "d8052994-4614-42dd-da83-59e0f1138d25"
      },
      "source": [
        "df_running = pd.read_csv(running_path)\n",
        "df_running['y'] = 3\n",
        "df_running.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>325.900333</td>\n",
              "      <td>124.469778</td>\n",
              "      <td>318.036000</td>\n",
              "      <td>146.749333</td>\n",
              "      <td>308.189333</td>\n",
              "      <td>146.718222</td>\n",
              "      <td>310.156000</td>\n",
              "      <td>170.284444</td>\n",
              "      <td>325.919333</td>\n",
              "      <td>161.073333</td>\n",
              "      <td>328.830000</td>\n",
              "      <td>148.038667</td>\n",
              "      <td>340.590000</td>\n",
              "      <td>172.875111</td>\n",
              "      <td>334.736667</td>\n",
              "      <td>161.076889</td>\n",
              "      <td>311.188667</td>\n",
              "      <td>200.336444</td>\n",
              "      <td>310.189333</td>\n",
              "      <td>240.907556</td>\n",
              "      <td>309.188000</td>\n",
              "      <td>280.174667</td>\n",
              "      <td>327.853000</td>\n",
              "      <td>201.628889</td>\n",
              "      <td>325.877000</td>\n",
              "      <td>243.486667</td>\n",
              "      <td>322.958667</td>\n",
              "      <td>280.183111</td>\n",
              "      <td>323.943667</td>\n",
              "      <td>120.527556</td>\n",
              "      <td>326.896000</td>\n",
              "      <td>120.584444</td>\n",
              "      <td>948.193</td>\n",
              "      <td>274.182</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>325.903000</td>\n",
              "      <td>123.200000</td>\n",
              "      <td>318.053333</td>\n",
              "      <td>146.752000</td>\n",
              "      <td>308.189667</td>\n",
              "      <td>146.707111</td>\n",
              "      <td>310.153333</td>\n",
              "      <td>170.289333</td>\n",
              "      <td>325.910000</td>\n",
              "      <td>161.082667</td>\n",
              "      <td>330.769000</td>\n",
              "      <td>148.043556</td>\n",
              "      <td>340.646667</td>\n",
              "      <td>172.876444</td>\n",
              "      <td>335.683333</td>\n",
              "      <td>161.121333</td>\n",
              "      <td>311.207000</td>\n",
              "      <td>200.380889</td>\n",
              "      <td>310.202000</td>\n",
              "      <td>240.925333</td>\n",
              "      <td>309.192000</td>\n",
              "      <td>280.164889</td>\n",
              "      <td>328.833333</td>\n",
              "      <td>201.680889</td>\n",
              "      <td>325.884000</td>\n",
              "      <td>243.501778</td>\n",
              "      <td>323.884000</td>\n",
              "      <td>280.173333</td>\n",
              "      <td>323.938333</td>\n",
              "      <td>120.514222</td>\n",
              "      <td>326.901000</td>\n",
              "      <td>120.569333</td>\n",
              "      <td>948.198</td>\n",
              "      <td>271.443</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>325.922667</td>\n",
              "      <td>123.186667</td>\n",
              "      <td>318.065333</td>\n",
              "      <td>148.030667</td>\n",
              "      <td>307.251667</td>\n",
              "      <td>146.726222</td>\n",
              "      <td>310.170333</td>\n",
              "      <td>171.532889</td>\n",
              "      <td>326.864667</td>\n",
              "      <td>161.104889</td>\n",
              "      <td>331.738333</td>\n",
              "      <td>149.313333</td>\n",
              "      <td>340.646667</td>\n",
              "      <td>172.936444</td>\n",
              "      <td>337.663333</td>\n",
              "      <td>162.415556</td>\n",
              "      <td>311.202000</td>\n",
              "      <td>201.646222</td>\n",
              "      <td>310.202667</td>\n",
              "      <td>242.205778</td>\n",
              "      <td>309.188667</td>\n",
              "      <td>280.180444</td>\n",
              "      <td>328.832667</td>\n",
              "      <td>202.920444</td>\n",
              "      <td>325.897667</td>\n",
              "      <td>243.519111</td>\n",
              "      <td>323.887000</td>\n",
              "      <td>280.178667</td>\n",
              "      <td>324.858667</td>\n",
              "      <td>119.318222</td>\n",
              "      <td>327.836667</td>\n",
              "      <td>120.565333</td>\n",
              "      <td>948.231</td>\n",
              "      <td>271.429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>325.927000</td>\n",
              "      <td>123.180000</td>\n",
              "      <td>318.074000</td>\n",
              "      <td>148.011111</td>\n",
              "      <td>308.189333</td>\n",
              "      <td>146.708889</td>\n",
              "      <td>310.176667</td>\n",
              "      <td>171.586222</td>\n",
              "      <td>326.862333</td>\n",
              "      <td>161.103111</td>\n",
              "      <td>331.748667</td>\n",
              "      <td>149.312889</td>\n",
              "      <td>340.630000</td>\n",
              "      <td>174.172889</td>\n",
              "      <td>337.660000</td>\n",
              "      <td>159.832889</td>\n",
              "      <td>312.145000</td>\n",
              "      <td>201.636889</td>\n",
              "      <td>310.212333</td>\n",
              "      <td>242.233333</td>\n",
              "      <td>309.204667</td>\n",
              "      <td>280.169778</td>\n",
              "      <td>329.793333</td>\n",
              "      <td>202.911556</td>\n",
              "      <td>325.906000</td>\n",
              "      <td>243.528000</td>\n",
              "      <td>322.959333</td>\n",
              "      <td>280.157333</td>\n",
              "      <td>324.864333</td>\n",
              "      <td>119.316444</td>\n",
              "      <td>327.837333</td>\n",
              "      <td>120.555556</td>\n",
              "      <td>948.276</td>\n",
              "      <td>271.415</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>326.850333</td>\n",
              "      <td>123.169778</td>\n",
              "      <td>319.001667</td>\n",
              "      <td>148.024444</td>\n",
              "      <td>307.250000</td>\n",
              "      <td>146.709778</td>\n",
              "      <td>309.241667</td>\n",
              "      <td>171.608000</td>\n",
              "      <td>325.923333</td>\n",
              "      <td>161.119111</td>\n",
              "      <td>332.717667</td>\n",
              "      <td>149.334667</td>\n",
              "      <td>340.606667</td>\n",
              "      <td>174.235556</td>\n",
              "      <td>338.636667</td>\n",
              "      <td>162.440444</td>\n",
              "      <td>311.200000</td>\n",
              "      <td>201.633333</td>\n",
              "      <td>310.226000</td>\n",
              "      <td>242.231556</td>\n",
              "      <td>309.211667</td>\n",
              "      <td>280.166222</td>\n",
              "      <td>329.794333</td>\n",
              "      <td>201.690667</td>\n",
              "      <td>325.908667</td>\n",
              "      <td>243.503556</td>\n",
              "      <td>322.957667</td>\n",
              "      <td>280.145778</td>\n",
              "      <td>324.881000</td>\n",
              "      <td>119.313778</td>\n",
              "      <td>327.863000</td>\n",
              "      <td>120.553778</td>\n",
              "      <td>951.038</td>\n",
              "      <td>271.414</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0           1           2           3  ...       33   34   35  y\n",
              "0  325.900333  124.469778  318.036000  146.749333  ...  274.182  0.0  0.0  3\n",
              "1  325.903000  123.200000  318.053333  146.752000  ...  271.443  0.0  0.0  3\n",
              "2  325.922667  123.186667  318.065333  148.030667  ...  271.429  0.0  0.0  3\n",
              "3  325.927000  123.180000  318.074000  148.011111  ...  271.415  0.0  0.0  3\n",
              "4  326.850333  123.169778  319.001667  148.024444  ...  271.414  0.0  0.0  3\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLkryM51Qt2t"
      },
      "source": [
        "### (5) Kicking\n",
        "- y : 4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiU1uE4QRdYi"
      },
      "source": [
        "kicking_path ='/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/kicking_sliding.csv'"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "Jgyd2hemRdgd",
        "outputId": "30c1710e-4835-4976-bf12-c5d28b9997af"
      },
      "source": [
        "df_kicking = pd.read_csv(kicking_path)\n",
        "df_kicking['y'] = 4\n",
        "df_kicking.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>226.751000</td>\n",
              "      <td>8.047956</td>\n",
              "      <td>231.340333</td>\n",
              "      <td>50.718222</td>\n",
              "      <td>206.518333</td>\n",
              "      <td>45.540000</td>\n",
              "      <td>179.081333</td>\n",
              "      <td>77.709333</td>\n",
              "      <td>175.216333</td>\n",
              "      <td>65.553778</td>\n",
              "      <td>255.503000</td>\n",
              "      <td>52.476889</td>\n",
              "      <td>268.586333</td>\n",
              "      <td>99.458222</td>\n",
              "      <td>273.788000</td>\n",
              "      <td>142.175556</td>\n",
              "      <td>212.417333</td>\n",
              "      <td>149.987556</td>\n",
              "      <td>221.521000</td>\n",
              "      <td>216.165333</td>\n",
              "      <td>227.423000</td>\n",
              "      <td>266.641778</td>\n",
              "      <td>237.855333</td>\n",
              "      <td>153.462222</td>\n",
              "      <td>236.557000</td>\n",
              "      <td>223.126222</td>\n",
              "      <td>237.865333</td>\n",
              "      <td>275.322222</td>\n",
              "      <td>222.223000</td>\n",
              "      <td>2.377347</td>\n",
              "      <td>232.009333</td>\n",
              "      <td>2.379378</td>\n",
              "      <td>652.956</td>\n",
              "      <td>5.34914</td>\n",
              "      <td>735.208</td>\n",
              "      <td>5.34283</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>228.073667</td>\n",
              "      <td>8.893156</td>\n",
              "      <td>232.000000</td>\n",
              "      <td>50.710667</td>\n",
              "      <td>207.168333</td>\n",
              "      <td>45.508444</td>\n",
              "      <td>179.078000</td>\n",
              "      <td>75.108889</td>\n",
              "      <td>181.073000</td>\n",
              "      <td>58.553333</td>\n",
              "      <td>257.446000</td>\n",
              "      <td>52.501778</td>\n",
              "      <td>269.202667</td>\n",
              "      <td>99.459111</td>\n",
              "      <td>274.428667</td>\n",
              "      <td>141.272444</td>\n",
              "      <td>211.780333</td>\n",
              "      <td>151.723556</td>\n",
              "      <td>218.936667</td>\n",
              "      <td>217.913778</td>\n",
              "      <td>227.405000</td>\n",
              "      <td>266.656889</td>\n",
              "      <td>237.867333</td>\n",
              "      <td>155.196444</td>\n",
              "      <td>236.545333</td>\n",
              "      <td>224.844889</td>\n",
              "      <td>237.193667</td>\n",
              "      <td>298.020000</td>\n",
              "      <td>225.459000</td>\n",
              "      <td>2.379049</td>\n",
              "      <td>233.967000</td>\n",
              "      <td>2.379662</td>\n",
              "      <td>660.705</td>\n",
              "      <td>5.35190</td>\n",
              "      <td>742.914</td>\n",
              "      <td>5.33893</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>231.341667</td>\n",
              "      <td>8.944178</td>\n",
              "      <td>232.646667</td>\n",
              "      <td>51.578222</td>\n",
              "      <td>208.482333</td>\n",
              "      <td>45.532444</td>\n",
              "      <td>179.086333</td>\n",
              "      <td>73.365333</td>\n",
              "      <td>188.907333</td>\n",
              "      <td>51.592444</td>\n",
              "      <td>258.088667</td>\n",
              "      <td>53.362667</td>\n",
              "      <td>269.207667</td>\n",
              "      <td>98.618222</td>\n",
              "      <td>275.091667</td>\n",
              "      <td>138.657333</td>\n",
              "      <td>211.751000</td>\n",
              "      <td>151.732000</td>\n",
              "      <td>216.981667</td>\n",
              "      <td>217.916889</td>\n",
              "      <td>226.794667</td>\n",
              "      <td>266.660000</td>\n",
              "      <td>237.892667</td>\n",
              "      <td>155.200444</td>\n",
              "      <td>237.195333</td>\n",
              "      <td>223.127556</td>\n",
              "      <td>236.565667</td>\n",
              "      <td>299.737333</td>\n",
              "      <td>226.784667</td>\n",
              "      <td>2.373804</td>\n",
              "      <td>237.206667</td>\n",
              "      <td>2.377391</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>744.950</td>\n",
              "      <td>5.33622</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>232.662667</td>\n",
              "      <td>8.921378</td>\n",
              "      <td>232.676000</td>\n",
              "      <td>51.603556</td>\n",
              "      <td>209.784333</td>\n",
              "      <td>46.368889</td>\n",
              "      <td>180.414333</td>\n",
              "      <td>72.452444</td>\n",
              "      <td>196.113667</td>\n",
              "      <td>46.384444</td>\n",
              "      <td>258.115000</td>\n",
              "      <td>55.066222</td>\n",
              "      <td>269.212000</td>\n",
              "      <td>98.622667</td>\n",
              "      <td>277.047667</td>\n",
              "      <td>135.199556</td>\n",
              "      <td>211.744667</td>\n",
              "      <td>150.847556</td>\n",
              "      <td>213.068333</td>\n",
              "      <td>217.900000</td>\n",
              "      <td>226.759667</td>\n",
              "      <td>264.060444</td>\n",
              "      <td>239.191333</td>\n",
              "      <td>155.192889</td>\n",
              "      <td>237.236000</td>\n",
              "      <td>219.679111</td>\n",
              "      <td>236.570333</td>\n",
              "      <td>300.612444</td>\n",
              "      <td>228.101000</td>\n",
              "      <td>2.375556</td>\n",
              "      <td>238.523000</td>\n",
              "      <td>2.378511</td>\n",
              "      <td>666.620</td>\n",
              "      <td>14.17530</td>\n",
              "      <td>748.852</td>\n",
              "      <td>5.33723</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>233.963667</td>\n",
              "      <td>9.754933</td>\n",
              "      <td>234.606000</td>\n",
              "      <td>51.568889</td>\n",
              "      <td>211.745333</td>\n",
              "      <td>45.517778</td>\n",
              "      <td>183.660667</td>\n",
              "      <td>72.451111</td>\n",
              "      <td>204.578000</td>\n",
              "      <td>46.396889</td>\n",
              "      <td>258.752000</td>\n",
              "      <td>55.080889</td>\n",
              "      <td>269.209333</td>\n",
              "      <td>99.467556</td>\n",
              "      <td>278.362667</td>\n",
              "      <td>131.711111</td>\n",
              "      <td>211.760333</td>\n",
              "      <td>151.734222</td>\n",
              "      <td>207.840333</td>\n",
              "      <td>219.661333</td>\n",
              "      <td>222.855333</td>\n",
              "      <td>256.222667</td>\n",
              "      <td>241.772333</td>\n",
              "      <td>156.074667</td>\n",
              "      <td>239.183000</td>\n",
              "      <td>234.444000</td>\n",
              "      <td>236.565667</td>\n",
              "      <td>301.473778</td>\n",
              "      <td>230.686333</td>\n",
              "      <td>2.374538</td>\n",
              "      <td>240.484667</td>\n",
              "      <td>2.377596</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>752.783</td>\n",
              "      <td>5.33532</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1           2          3  ...        33       34       35  y\n",
              "0  226.751000  8.047956  231.340333  50.718222  ...   5.34914  735.208  5.34283  4\n",
              "1  228.073667  8.893156  232.000000  50.710667  ...   5.35190  742.914  5.33893  4\n",
              "2  231.341667  8.944178  232.646667  51.578222  ...   0.00000  744.950  5.33622  4\n",
              "3  232.662667  8.921378  232.676000  51.603556  ...  14.17530  748.852  5.33723  4\n",
              "4  233.963667  9.754933  234.606000  51.568889  ...   0.00000  752.783  5.33532  4\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxu9Jgb76p9H"
      },
      "source": [
        "## 2) Check Null Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A59c0QCEtDtE",
        "outputId": "938580f2-a654-4bef-fc01-53c936c299d6"
      },
      "source": [
        "df_punching.isnull().sum().sum()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKzkGN87tXKJ",
        "outputId": "7e60439f-82cd-45e5-d0f9-1b1e37a996f2"
      },
      "source": [
        "df_smoking.isnull().sum().sum()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK3ovUmvfeWC",
        "outputId": "f8838799-ed3a-4531-e47d-784e02d6ba11"
      },
      "source": [
        "df_walking.isnull().sum().sum()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JlLj2lWFR0L",
        "outputId": "a229b760-2c33-4fe7-8b9c-a262448f6cf4"
      },
      "source": [
        "df_running.isnull().sum().sum()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz66vx6uRrje",
        "outputId": "6ba114d1-2165-43c9-b58a-986ec24295d3"
      },
      "source": [
        "df_kicking.isnull().sum().sum()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qE8J35cOIFM"
      },
      "source": [
        "## 3) Concatenate Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxdeYglpOk-2",
        "outputId": "8588e3d2-f1f0-4352-e472-ad5c7c813b6a"
      },
      "source": [
        "print(df_punching.shape)\n",
        "print(df_smoking.shape)\n",
        "print(df_walking.shape)\n",
        "print(df_running.shape)\n",
        "print(df_kicking.shape)\n",
        "print(df_punching.shape[0] + df_smoking.shape[0] + df_walking.shape[0] + df_running.shape[0] + df_kicking.shape[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(190080, 37)\n",
            "(2141152, 37)\n",
            "(2234848, 37)\n",
            "(2866784, 37)\n",
            "(160288, 37)\n",
            "7593152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "RCAhgJw3ONsM",
        "outputId": "ef944747-b911-4e7e-b3c4-0fe0ab83cb9a"
      },
      "source": [
        "df = pd.concat((df_punching, df_smoking, df_walking, df_running, df_kicking), axis=0, ignore_index=True)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>290.693</td>\n",
              "      <td>161.699</td>\n",
              "      <td>307.602</td>\n",
              "      <td>207.250</td>\n",
              "      <td>285.384</td>\n",
              "      <td>205.939</td>\n",
              "      <td>275.033</td>\n",
              "      <td>250.267</td>\n",
              "      <td>265.938</td>\n",
              "      <td>286.746</td>\n",
              "      <td>331.090</td>\n",
              "      <td>211.121</td>\n",
              "      <td>337.569</td>\n",
              "      <td>259.385</td>\n",
              "      <td>327.208</td>\n",
              "      <td>295.910</td>\n",
              "      <td>286.718</td>\n",
              "      <td>291.991</td>\n",
              "      <td>291.966</td>\n",
              "      <td>354.547</td>\n",
              "      <td>289.371</td>\n",
              "      <td>401.517</td>\n",
              "      <td>318.035</td>\n",
              "      <td>294.563</td>\n",
              "      <td>328.432</td>\n",
              "      <td>358.453</td>\n",
              "      <td>337.620</td>\n",
              "      <td>419.732</td>\n",
              "      <td>290.661</td>\n",
              "      <td>158.996</td>\n",
              "      <td>298.499</td>\n",
              "      <td>160.267</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.984</td>\n",
              "      <td>164.255</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>290.692</td>\n",
              "      <td>161.703</td>\n",
              "      <td>307.593</td>\n",
              "      <td>205.975</td>\n",
              "      <td>284.154</td>\n",
              "      <td>204.656</td>\n",
              "      <td>275.023</td>\n",
              "      <td>250.258</td>\n",
              "      <td>265.927</td>\n",
              "      <td>286.758</td>\n",
              "      <td>331.096</td>\n",
              "      <td>211.115</td>\n",
              "      <td>337.552</td>\n",
              "      <td>263.272</td>\n",
              "      <td>325.892</td>\n",
              "      <td>298.512</td>\n",
              "      <td>285.515</td>\n",
              "      <td>291.992</td>\n",
              "      <td>291.964</td>\n",
              "      <td>354.542</td>\n",
              "      <td>289.346</td>\n",
              "      <td>401.530</td>\n",
              "      <td>318.033</td>\n",
              "      <td>294.569</td>\n",
              "      <td>328.429</td>\n",
              "      <td>358.463</td>\n",
              "      <td>337.624</td>\n",
              "      <td>420.964</td>\n",
              "      <td>290.662</td>\n",
              "      <td>159.004</td>\n",
              "      <td>298.495</td>\n",
              "      <td>160.273</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.984</td>\n",
              "      <td>164.251</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>291.954</td>\n",
              "      <td>161.696</td>\n",
              "      <td>307.587</td>\n",
              "      <td>207.241</td>\n",
              "      <td>284.136</td>\n",
              "      <td>204.675</td>\n",
              "      <td>275.016</td>\n",
              "      <td>251.537</td>\n",
              "      <td>265.938</td>\n",
              "      <td>286.740</td>\n",
              "      <td>331.109</td>\n",
              "      <td>211.125</td>\n",
              "      <td>337.554</td>\n",
              "      <td>263.290</td>\n",
              "      <td>325.854</td>\n",
              "      <td>298.535</td>\n",
              "      <td>285.498</td>\n",
              "      <td>291.992</td>\n",
              "      <td>291.975</td>\n",
              "      <td>354.535</td>\n",
              "      <td>289.367</td>\n",
              "      <td>401.528</td>\n",
              "      <td>318.019</td>\n",
              "      <td>294.574</td>\n",
              "      <td>328.420</td>\n",
              "      <td>358.453</td>\n",
              "      <td>337.624</td>\n",
              "      <td>420.957</td>\n",
              "      <td>290.669</td>\n",
              "      <td>158.995</td>\n",
              "      <td>298.510</td>\n",
              "      <td>160.266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.990</td>\n",
              "      <td>164.251</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>290.694</td>\n",
              "      <td>161.694</td>\n",
              "      <td>307.602</td>\n",
              "      <td>207.250</td>\n",
              "      <td>284.168</td>\n",
              "      <td>205.932</td>\n",
              "      <td>275.020</td>\n",
              "      <td>251.551</td>\n",
              "      <td>265.923</td>\n",
              "      <td>286.781</td>\n",
              "      <td>331.107</td>\n",
              "      <td>211.127</td>\n",
              "      <td>337.545</td>\n",
              "      <td>263.302</td>\n",
              "      <td>325.848</td>\n",
              "      <td>299.779</td>\n",
              "      <td>285.515</td>\n",
              "      <td>293.261</td>\n",
              "      <td>291.966</td>\n",
              "      <td>354.561</td>\n",
              "      <td>289.356</td>\n",
              "      <td>401.517</td>\n",
              "      <td>318.030</td>\n",
              "      <td>294.579</td>\n",
              "      <td>328.419</td>\n",
              "      <td>358.455</td>\n",
              "      <td>337.629</td>\n",
              "      <td>420.955</td>\n",
              "      <td>290.658</td>\n",
              "      <td>158.992</td>\n",
              "      <td>298.504</td>\n",
              "      <td>160.261</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>316.802</td>\n",
              "      <td>164.252</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>290.694</td>\n",
              "      <td>161.683</td>\n",
              "      <td>307.600</td>\n",
              "      <td>207.254</td>\n",
              "      <td>284.173</td>\n",
              "      <td>205.931</td>\n",
              "      <td>275.016</td>\n",
              "      <td>251.550</td>\n",
              "      <td>265.919</td>\n",
              "      <td>286.792</td>\n",
              "      <td>331.101</td>\n",
              "      <td>211.132</td>\n",
              "      <td>337.561</td>\n",
              "      <td>263.289</td>\n",
              "      <td>325.891</td>\n",
              "      <td>299.781</td>\n",
              "      <td>286.713</td>\n",
              "      <td>293.268</td>\n",
              "      <td>291.954</td>\n",
              "      <td>354.529</td>\n",
              "      <td>289.358</td>\n",
              "      <td>401.508</td>\n",
              "      <td>318.028</td>\n",
              "      <td>294.586</td>\n",
              "      <td>328.429</td>\n",
              "      <td>358.462</td>\n",
              "      <td>337.632</td>\n",
              "      <td>420.970</td>\n",
              "      <td>290.667</td>\n",
              "      <td>158.981</td>\n",
              "      <td>298.509</td>\n",
              "      <td>160.255</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.985</td>\n",
              "      <td>164.270</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0        1        2        3        4  ...   32   33       34       35  y\n",
              "0  290.693  161.699  307.602  207.250  285.384  ...  0.0  0.0  317.984  164.255  2\n",
              "1  290.692  161.703  307.593  205.975  284.154  ...  0.0  0.0  317.984  164.251  2\n",
              "2  291.954  161.696  307.587  207.241  284.136  ...  0.0  0.0  317.990  164.251  2\n",
              "3  290.694  161.694  307.602  207.250  284.168  ...  0.0  0.0  316.802  164.252  2\n",
              "4  290.694  161.683  307.600  207.254  284.173  ...  0.0  0.0  317.985  164.270  2\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfaX5mhgO0LG",
        "outputId": "7f8b2daa-2f8a-4bf6-bc72-df581c2f89bf"
      },
      "source": [
        "df.shape[0] / 32"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "237286.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guk0W6FHqp_V",
        "outputId": "f9aaae24-3db0-48e3-d4bd-1d5722533c1e"
      },
      "source": [
        "df['y'].value_counts()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    2866784\n",
              "0    2234848\n",
              "1    2141152\n",
              "2     190080\n",
              "4     160288\n",
              "Name: y, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnGB3EbWt_uI"
      },
      "source": [
        "## 4) Joint(x, y) Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alY3YQCCvjFH"
      },
      "source": [
        "#### (1) Delete Nose, Eye, Ear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxuuinSdu01a"
      },
      "source": [
        "```\n",
        "{0,  \"Nose\"}, -> '0', '1' delete\n",
        "{1,  \"Neck\"}, -> '2', '3'\n",
        "{2,  \"RShoulder\"}, -> '4', '5'\n",
        "{3,  \"RElbow\"}, -> '6', '7'\n",
        "{4,  \"RWrist\"}, -> '8', '9'\n",
        "{5,  \"LShoulder\"}, -> '10', '11'\n",
        "{6,  \"LElbow\"}, -> '12', '13'\n",
        "{7,  \"LWrist\"}, -> '14', '15'\n",
        "{8,  \"RHip\"}, -> '16', '17'\n",
        "{9,  \"RKnee\"}, -> '18', '19'\n",
        "{10, \"RAnkle\"}, -> '20', '21'\n",
        "{11, \"LHip\"}, -> '22', '23'\n",
        "{12, \"LKnee\"}, -> '24', '25'\n",
        "{13, \"LAnkle\"}, -> '26', '27'\n",
        "{14, \"REye\"}, -> '28', '29' delete\n",
        "{15, \"LEye\"}, -> '30', '31' delete\n",
        "{16, \"REar\"}, -> '32', '33' delete\n",
        "{17, \"LEar\"}, -> '34', '35' delete\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOHQWcrQuDwO"
      },
      "source": [
        "def deleteJoint(df):\n",
        "    df.drop(['0', '1'] + [str(i) for i in range(28, 36)], axis=1, inplace=True)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "VtzslML_uSCM",
        "outputId": "769758e3-95fc-4d78-fa0e-8cc8c0c71aad"
      },
      "source": [
        "df = deleteJoint(df)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>307.602</td>\n",
              "      <td>207.250</td>\n",
              "      <td>285.384</td>\n",
              "      <td>205.939</td>\n",
              "      <td>275.033</td>\n",
              "      <td>250.267</td>\n",
              "      <td>265.938</td>\n",
              "      <td>286.746</td>\n",
              "      <td>331.090</td>\n",
              "      <td>211.121</td>\n",
              "      <td>337.569</td>\n",
              "      <td>259.385</td>\n",
              "      <td>327.208</td>\n",
              "      <td>295.910</td>\n",
              "      <td>286.718</td>\n",
              "      <td>291.991</td>\n",
              "      <td>291.966</td>\n",
              "      <td>354.547</td>\n",
              "      <td>289.371</td>\n",
              "      <td>401.517</td>\n",
              "      <td>318.035</td>\n",
              "      <td>294.563</td>\n",
              "      <td>328.432</td>\n",
              "      <td>358.453</td>\n",
              "      <td>337.620</td>\n",
              "      <td>419.732</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>307.593</td>\n",
              "      <td>205.975</td>\n",
              "      <td>284.154</td>\n",
              "      <td>204.656</td>\n",
              "      <td>275.023</td>\n",
              "      <td>250.258</td>\n",
              "      <td>265.927</td>\n",
              "      <td>286.758</td>\n",
              "      <td>331.096</td>\n",
              "      <td>211.115</td>\n",
              "      <td>337.552</td>\n",
              "      <td>263.272</td>\n",
              "      <td>325.892</td>\n",
              "      <td>298.512</td>\n",
              "      <td>285.515</td>\n",
              "      <td>291.992</td>\n",
              "      <td>291.964</td>\n",
              "      <td>354.542</td>\n",
              "      <td>289.346</td>\n",
              "      <td>401.530</td>\n",
              "      <td>318.033</td>\n",
              "      <td>294.569</td>\n",
              "      <td>328.429</td>\n",
              "      <td>358.463</td>\n",
              "      <td>337.624</td>\n",
              "      <td>420.964</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>307.587</td>\n",
              "      <td>207.241</td>\n",
              "      <td>284.136</td>\n",
              "      <td>204.675</td>\n",
              "      <td>275.016</td>\n",
              "      <td>251.537</td>\n",
              "      <td>265.938</td>\n",
              "      <td>286.740</td>\n",
              "      <td>331.109</td>\n",
              "      <td>211.125</td>\n",
              "      <td>337.554</td>\n",
              "      <td>263.290</td>\n",
              "      <td>325.854</td>\n",
              "      <td>298.535</td>\n",
              "      <td>285.498</td>\n",
              "      <td>291.992</td>\n",
              "      <td>291.975</td>\n",
              "      <td>354.535</td>\n",
              "      <td>289.367</td>\n",
              "      <td>401.528</td>\n",
              "      <td>318.019</td>\n",
              "      <td>294.574</td>\n",
              "      <td>328.420</td>\n",
              "      <td>358.453</td>\n",
              "      <td>337.624</td>\n",
              "      <td>420.957</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>307.602</td>\n",
              "      <td>207.250</td>\n",
              "      <td>284.168</td>\n",
              "      <td>205.932</td>\n",
              "      <td>275.020</td>\n",
              "      <td>251.551</td>\n",
              "      <td>265.923</td>\n",
              "      <td>286.781</td>\n",
              "      <td>331.107</td>\n",
              "      <td>211.127</td>\n",
              "      <td>337.545</td>\n",
              "      <td>263.302</td>\n",
              "      <td>325.848</td>\n",
              "      <td>299.779</td>\n",
              "      <td>285.515</td>\n",
              "      <td>293.261</td>\n",
              "      <td>291.966</td>\n",
              "      <td>354.561</td>\n",
              "      <td>289.356</td>\n",
              "      <td>401.517</td>\n",
              "      <td>318.030</td>\n",
              "      <td>294.579</td>\n",
              "      <td>328.419</td>\n",
              "      <td>358.455</td>\n",
              "      <td>337.629</td>\n",
              "      <td>420.955</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>307.600</td>\n",
              "      <td>207.254</td>\n",
              "      <td>284.173</td>\n",
              "      <td>205.931</td>\n",
              "      <td>275.016</td>\n",
              "      <td>251.550</td>\n",
              "      <td>265.919</td>\n",
              "      <td>286.792</td>\n",
              "      <td>331.101</td>\n",
              "      <td>211.132</td>\n",
              "      <td>337.561</td>\n",
              "      <td>263.289</td>\n",
              "      <td>325.891</td>\n",
              "      <td>299.781</td>\n",
              "      <td>286.713</td>\n",
              "      <td>293.268</td>\n",
              "      <td>291.954</td>\n",
              "      <td>354.529</td>\n",
              "      <td>289.358</td>\n",
              "      <td>401.508</td>\n",
              "      <td>318.028</td>\n",
              "      <td>294.586</td>\n",
              "      <td>328.429</td>\n",
              "      <td>358.462</td>\n",
              "      <td>337.632</td>\n",
              "      <td>420.970</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         2        3        4        5  ...       25       26       27  y\n",
              "0  307.602  207.250  285.384  205.939  ...  358.453  337.620  419.732  2\n",
              "1  307.593  205.975  284.154  204.656  ...  358.463  337.624  420.964  2\n",
              "2  307.587  207.241  284.136  204.675  ...  358.453  337.624  420.957  2\n",
              "3  307.602  207.250  284.168  205.932  ...  358.455  337.629  420.955  2\n",
              "4  307.600  207.254  284.173  205.931  ...  358.462  337.632  420.970  2\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJoIxNckvz4j"
      },
      "source": [
        "### (2) Extract Angle from Joint (x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yznX20nawL_9"
      },
      "source": [
        "#### - Extract Angle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNC9sH3rwkaQ"
      },
      "source": [
        "Joints\n",
        "\n",
        "```\n",
        "{1,  \"Neck\"}, -> '2', '3'\n",
        "{2,  \"RShoulder\"}, -> '4', '5'\n",
        "{3,  \"RElbow\"}, -> '6', '7'\n",
        "{4,  \"RWrist\"}, -> '8', '9'\n",
        "{5,  \"LShoulder\"}, -> '10', '11'\n",
        "{6,  \"LElbow\"}, -> '12', '13'\n",
        "{7,  \"LWrist\"}, -> '14', '15'\n",
        "{8,  \"RHip\"}, -> '16', '17'\n",
        "{9,  \"RKnee\"}, -> '18', '19'\n",
        "{10, \"RAnkle\"}, -> '20', '21'\n",
        "{11, \"LHip\"}, -> '22', '23'\n",
        "{12, \"LKnee\"}, -> '24', '25'\n",
        "{13, \"LAnkle\"}, -> '26', '27'\n",
        "```\n",
        "\n",
        "Angle\n",
        "\n",
        "```\n",
        "Angle__RShoulder : Neck - RShoulder - RElbow\n",
        "Angle_RElbow : RShoulder - RElbow - RWrist\n",
        "Angle_RHip : Neck - RHip - RKnee\n",
        "Angle_RKnee : RHip - RKnee - RAnkle\n",
        "\n",
        "Angle__LShoulder : Neck - LShoulder - LElbow\n",
        "Angle_LElbow : LShoulder - LElbow - LWrist\n",
        "Angle_LHip : Neck - LHip - LKnee\n",
        "Angle_LKnee : LHip - LKnee - LAnkle\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWibAyN_whpX"
      },
      "source": [
        "def makeJointXY(df):\n",
        "    df['Neck'] = [np.array(list(i)) for i in tqdm(zip(df['2'], df['3']))]\n",
        "\n",
        "    df['RShoulder'] = [np.array(list(i)) for i in tqdm(zip(df['4'], df['5']))]\n",
        "    df['RElbow'] = [np.array(list(i)) for i in tqdm(zip(df['6'], df['7']))]\n",
        "    df['RWrist'] = [np.array(list(i)) for i in tqdm(zip(df['8'], df['9']))]\n",
        "    df['RHip'] = [np.array(list(i)) for i in tqdm(zip(df['16'], df['17']))]\n",
        "    df['RKnee'] = [np.array(list(i)) for i in tqdm(zip(df['18'], df['19']))]\n",
        "    df['RAnkle'] = [np.array(list(i)) for i in tqdm(zip(df['20'], df['21']))]\n",
        "\n",
        "    df['LShoulder'] = [np.array(list(i)) for i in tqdm(zip(df['10'], df['11']))]\n",
        "    df['LElbow'] = [np.array(list(i)) for i in tqdm(zip(df['12'], df['13']))]\n",
        "    df['LWrist'] = [np.array(list(i)) for i in tqdm(zip(df['14'], df['15']))]\n",
        "    df['LHip'] = [np.array(list(i)) for i in tqdm(zip(df['22'], df['23']))]\n",
        "    df['LKnee'] = [np.array(list(i)) for i in tqdm(zip(df['24'], df['25']))]\n",
        "    df['LAnkle'] = [np.array(list(i)) for i in tqdm(zip(df['26'], df['27']))]\n",
        "\n",
        "    df.drop([str(i) for i in range(2, 28)], axis=1, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d8zf6k77-Ub"
      },
      "source": [
        "def extractAngle(a, joint, c):\n",
        "    a = np.array(a)\n",
        "    joint = np.array(joint)\n",
        "    c = np.array(c)\n",
        "    \n",
        "    a_j = a - joint\n",
        "    c_j = c - joint\n",
        "\n",
        "    theta = []\n",
        "\n",
        "    for i in tqdm(range(a.shape[0])):\n",
        "        th_a_j = math.atan2(a_j[i][1], a_j[i][0])\n",
        "        th_c_j = math.atan2(c_j[i][1], c_j[i][0])\n",
        "        theta.append(th_a_j - th_c_j)\n",
        "\n",
        "    return theta"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFLOecwCv1TT"
      },
      "source": [
        "def jointAngles(df):\n",
        "    df['Ang_RShoulder'] = extractAngle(df['Neck'], df['RShoulder'], df['RElbow'])\n",
        "    df['Ang_RElbow'] = extractAngle(df['RShoulder'], df['RElbow'], df['RWrist'])\n",
        "    df['Ang_RHip'] = extractAngle(df['Neck'], df['RHip'], df['RKnee'])\n",
        "    df['Ang_RKnee'] = extractAngle(df['RHip'], df['RKnee'], df['RAnkle'])\n",
        "\n",
        "    df['Ang_LShoulder'] = extractAngle(df['Neck'], df['LShoulder'], df['LElbow'])\n",
        "    df['Ang_LElbow'] = extractAngle(df['LShoulder'], df['LElbow'], df['LWrist'])\n",
        "    df['Ang_LHip'] = extractAngle(df['Neck'], df['LHip'], df['LKnee'])\n",
        "    df['Ang_LKnee'] = extractAngle(df['LHip'], df['LKnee'], df['LAnkle'])\n",
        "    \n",
        "    df.drop(df.columns[1:14], axis=1, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5p6jNfmVgKC",
        "outputId": "cd2403ed-b177-4985-e377-1e99dc61ab0c"
      },
      "source": [
        "df = makeJointXY(df)\n",
        "df = jointAngles(df)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7593152it [00:15, 485712.99it/s]\n",
            "7593152it [00:15, 492086.71it/s]\n",
            "7593152it [00:15, 495430.79it/s]\n",
            "7593152it [00:15, 492572.78it/s]\n",
            "7593152it [00:15, 489965.82it/s]\n",
            "7593152it [00:15, 484056.49it/s]\n",
            "7593152it [00:15, 487396.46it/s]\n",
            "7593152it [00:15, 481949.14it/s]\n",
            "7593152it [00:18, 407098.97it/s]\n",
            "7593152it [00:15, 482237.04it/s]\n",
            "7593152it [00:15, 483231.60it/s]\n",
            "7593152it [00:15, 484112.94it/s]\n",
            "7593152it [00:16, 472312.89it/s]\n",
            "100%|| 7593152/7593152 [00:13<00:00, 569744.26it/s]\n",
            "100%|| 7593152/7593152 [00:13<00:00, 581942.41it/s]\n",
            "100%|| 7593152/7593152 [00:12<00:00, 598374.00it/s]\n",
            "100%|| 7593152/7593152 [00:12<00:00, 585316.97it/s]\n",
            "100%|| 7593152/7593152 [00:12<00:00, 590793.14it/s]\n",
            "100%|| 7593152/7593152 [00:12<00:00, 584634.55it/s]\n",
            "100%|| 7593152/7593152 [00:12<00:00, 591612.71it/s]\n",
            "100%|| 7593152/7593152 [00:12<00:00, 592491.66it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XEofd5q62bT"
      },
      "source": [
        "## 5) Make 'y'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPTNRjbhpJ4q"
      },
      "source": [
        "def makeY(df, n_frame=32):\n",
        "    y = df['y'].to_list()\n",
        "    y = [y[i * n_frame:(i + 1) * n_frame] for i in range((len(y) + n_frame - 1) // n_frame )]\n",
        "    y = [ys[0] for ys in y]\n",
        "\n",
        "    encoder = LabelEncoder()    \n",
        "    encoder.fit(y)\n",
        "    classes = encoder.classes_\n",
        "    y = encoder.transform(y)\n",
        "\n",
        "    y = to_categorical(y)\n",
        "\n",
        "    return y, classes"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-plaqsxHUmj_"
      },
      "source": [
        "y, classes = makeY(df)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj1pU3y3Vf9n",
        "outputId": "bf7a35e8-aa4e-481c-cb69-962bf7521da6"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(237286, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUI83OYlarQg",
        "outputId": "d03cbba1-ae8a-48eb-ecfe-21737c7b0061"
      },
      "source": [
        "classes"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "helgUckxQoA2",
        "outputId": "7937b2b8-7070-4caa-b17f-b6ec49382074"
      },
      "source": [
        "y"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVd-N0ln7C5T"
      },
      "source": [
        "## 6) Make 'X'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv_RBSw3gF-o"
      },
      "source": [
        "def makeX(df, n_frame=32):\n",
        "    x_cat = df.iloc[:,1:].values\n",
        "\n",
        "    data = []\n",
        "    for rows in tqdm(x_cat):\n",
        "        for element in rows:\n",
        "            data.append(np.float32(element))\n",
        "\n",
        "    x = np.array(data)\n",
        "    x = x.reshape(-1, 32, 8)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixK4Trj8VZGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "712f43c5-a0dc-4788-b4b9-f4fbc991a34e"
      },
      "source": [
        "X = makeX(df)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 7593152/7593152 [01:03<00:00, 118973.01it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDD7HngbX4sl",
        "outputId": "60fb1952-d7fd-409e-ccd2-862dc1f51a28"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(237286, 32, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4_fHZh-7LDl"
      },
      "source": [
        "## 7) Data Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwoxLdCjsgvD"
      },
      "source": [
        "random_seed = 0\n",
        "n_frame = 32"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oErPJ9Xxln6u"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_seed, stratify=y)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=random_seed, stratify=y_train)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gkoxTHK8jBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ec34bd-052f-44a1-845f-acc886c2b9ad"
      },
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((170845, 32, 8), (170845, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3SReDThx8ib",
        "outputId": "cde6966e-76a0-48a5-9ab1-2b46f5578dd7"
      },
      "source": [
        "X_valid.shape, y_valid.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((42712, 32, 8), (42712, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NovA26_Mx8qJ",
        "outputId": "aeff468d-0d89-4691-fc73-068b9846e549"
      },
      "source": [
        "X_test.shape, y_test.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((23729, 32, 8), (23729, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K5fXeUoqBr6",
        "outputId": "4d859ff3-24eb-4224-e0a8-177519d1e21d"
      },
      "source": [
        "y_t = np.argmax(y_train,axis=1).reshape(-1,1)\n",
        "y_t\n",
        "\n",
        "unique, counts = np.unique(y_t, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 50284, 1: 48176, 2: 4277, 3: 64502, 4: 3606}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFl18u8DsVae",
        "outputId": "375f8fee-451f-4ff0-bd5f-54b3b1e7e2ed"
      },
      "source": [
        "y_v = np.argmax(y_valid,axis=1).reshape(-1,1)\n",
        "y_v\n",
        "\n",
        "unique, counts = np.unique(y_v, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 12571, 1: 12044, 2: 1069, 3: 16126, 4: 902}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFPuxKwOsXsg",
        "outputId": "39246029-a57f-4027-920f-147c414f1aeb"
      },
      "source": [
        "y_te = np.argmax(y_test,axis=1).reshape(-1,1)\n",
        "y_te\n",
        "\n",
        "unique, counts = np.unique(y_te, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 6984, 1: 6691, 2: 594, 3: 8959, 4: 501}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVhrXnYoXd3n"
      },
      "source": [
        "### 8) Save Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnxZO3BXXr2m"
      },
      "source": [
        "np.save('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/X_train.npy', X_train)\n",
        "np.save('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/X_valid.npy', X_valid)\n",
        "np.save('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/X_test.npy', X_test)\n",
        "\n",
        "np.save('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/y_train.npy', y_train)\n",
        "np.save('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/y_valid.npy', y_valid)\n",
        "np.save('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/y_test.npy', y_test)\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF7jVbTkWpDw"
      },
      "source": [
        "# 2. LSTM Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MmjG0qCYU_l"
      },
      "source": [
        "## 0) Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJbI-5-7YWyt"
      },
      "source": [
        "X_train = np.load('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/X_train.npy')\n",
        "X_valid = np.load('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/X_valid.npy')\n",
        "X_test = np.load('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/X_test.npy')\n",
        "\n",
        "y_train = np.load('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/y_train.npy')\n",
        "y_valid = np.load('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/y_valid.npy')\n",
        "y_test = np.load('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/y_test.npy')\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_GPIteQ_FVm"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, CuDNNLSTM\n",
        "\n",
        "import keras.backend as K"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oESk5HdJ_1CK"
      },
      "source": [
        "n_input = 8 # angles\n",
        "n_steps = 32 # per frames\n",
        "\n",
        "n_hidden = 32\n",
        "n_classes = len(y[0])\n",
        "\n",
        "epochs = 1000\n",
        "batch_size = 16384\n",
        "\n",
        "lambda_loss_amount = 0.0015"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S609oD-_Wsbd"
      },
      "source": [
        "## 1) Model define"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DxO7PrMWuCR"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(n_hidden,\n",
        "                activation='relu',\n",
        "                bias_initializer='random_normal', # set initial bias by normal distribution\n",
        "                input_shape=(n_steps, n_input)))\n",
        "model.add(LSTM(n_hidden,\n",
        "               return_sequences=True, # return sequences\n",
        "               unit_forget_bias=1.0)) # bias_initializer=\"zeros\"\n",
        "model.add(LSTM(n_hidden,\n",
        "               unit_forget_bias=1.0)) # bias_initializer=\"zeros\"\n",
        "\n",
        "model.add(Dense(n_classes,\n",
        "                kernel_initializer='random_normal', # set initial weights by normal distribution\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(lambda_loss_amount), # weight regularizer\n",
        "                bias_regularizer=tf.keras.regularizers.l2(lambda_loss_amount), # bias regularizer\n",
        "                activation = 'softmax'))"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNGSpLQoWyM8",
        "outputId": "eeea3e1f-089c-476f-b790-7ec6406e2ba2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32, 32)            288       \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32, 32)            8320      \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 17,093\n",
            "Trainable params: 17,093\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "901bRI1DWy2G"
      },
      "source": [
        "## 2) Model Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNSl7h72W3Z8"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvbHXQ4hW0Ps"
      },
      "source": [
        "## 3) Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVatnxqSW85b",
        "outputId": "f1a9b26a-9693-42d6-eb5a-7ef222bb154f"
      },
      "source": [
        "%%time\n",
        "hist = model.fit(X_train, y_train,\n",
        "                 epochs = epochs,\n",
        "                 batch_size = batch_size,\n",
        "                 validation_data = (X_valid, y_valid))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "11/11 [==============================] - 6s 178ms/step - loss: 1.6017 - accuracy: 0.2047 - val_loss: 1.4759 - val_accuracy: 0.3781\n",
            "Epoch 2/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 1.4405 - accuracy: 0.3798 - val_loss: 1.3450 - val_accuracy: 0.3776\n",
            "Epoch 3/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 1.3216 - accuracy: 0.3785 - val_loss: 1.2557 - val_accuracy: 0.3891\n",
            "Epoch 4/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 1.2391 - accuracy: 0.4172 - val_loss: 1.1936 - val_accuracy: 0.4881\n",
            "Epoch 5/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 1.1844 - accuracy: 0.4880 - val_loss: 1.1561 - val_accuracy: 0.4897\n",
            "Epoch 6/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 1.1478 - accuracy: 0.4937 - val_loss: 1.1170 - val_accuracy: 0.5109\n",
            "Epoch 7/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 1.1072 - accuracy: 0.5180 - val_loss: 1.0783 - val_accuracy: 0.5361\n",
            "Epoch 8/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 1.0714 - accuracy: 0.5386 - val_loss: 1.0408 - val_accuracy: 0.5468\n",
            "Epoch 9/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 1.0308 - accuracy: 0.5510 - val_loss: 0.9964 - val_accuracy: 0.5607\n",
            "Epoch 10/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.9884 - accuracy: 0.6008 - val_loss: 0.9472 - val_accuracy: 0.6690\n",
            "Epoch 11/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.9406 - accuracy: 0.6684 - val_loss: 0.8943 - val_accuracy: 0.6919\n",
            "Epoch 12/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.8894 - accuracy: 0.6891 - val_loss: 0.8578 - val_accuracy: 0.6998\n",
            "Epoch 13/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.8485 - accuracy: 0.7018 - val_loss: 0.8178 - val_accuracy: 0.7159\n",
            "Epoch 14/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.8134 - accuracy: 0.7146 - val_loss: 0.7887 - val_accuracy: 0.7224\n",
            "Epoch 15/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.7830 - accuracy: 0.7253 - val_loss: 0.7607 - val_accuracy: 0.7297\n",
            "Epoch 16/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.7531 - accuracy: 0.7365 - val_loss: 0.7335 - val_accuracy: 0.7462\n",
            "Epoch 17/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.7331 - accuracy: 0.7433 - val_loss: 0.7091 - val_accuracy: 0.7537\n",
            "Epoch 18/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.7048 - accuracy: 0.7559 - val_loss: 0.6832 - val_accuracy: 0.7666\n",
            "Epoch 19/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.6831 - accuracy: 0.7647 - val_loss: 0.6635 - val_accuracy: 0.7733\n",
            "Epoch 20/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.6660 - accuracy: 0.7715 - val_loss: 0.6509 - val_accuracy: 0.7806\n",
            "Epoch 21/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.6447 - accuracy: 0.7823 - val_loss: 0.6317 - val_accuracy: 0.7865\n",
            "Epoch 22/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.6278 - accuracy: 0.7901 - val_loss: 0.6116 - val_accuracy: 0.7968\n",
            "Epoch 23/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.6134 - accuracy: 0.7962 - val_loss: 0.6016 - val_accuracy: 0.8020\n",
            "Epoch 24/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.5991 - accuracy: 0.8002 - val_loss: 0.5933 - val_accuracy: 0.8023\n",
            "Epoch 25/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.5856 - accuracy: 0.8053 - val_loss: 0.5790 - val_accuracy: 0.8056\n",
            "Epoch 26/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.5843 - accuracy: 0.8045 - val_loss: 0.5712 - val_accuracy: 0.8101\n",
            "Epoch 27/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.5631 - accuracy: 0.8135 - val_loss: 0.5530 - val_accuracy: 0.8161\n",
            "Epoch 28/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.5472 - accuracy: 0.8181 - val_loss: 0.5334 - val_accuracy: 0.8234\n",
            "Epoch 29/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.5382 - accuracy: 0.8206 - val_loss: 0.5395 - val_accuracy: 0.8198\n",
            "Epoch 30/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.5313 - accuracy: 0.8232 - val_loss: 0.5167 - val_accuracy: 0.8279\n",
            "Epoch 31/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.5242 - accuracy: 0.8234 - val_loss: 0.5067 - val_accuracy: 0.8318\n",
            "Epoch 32/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.5113 - accuracy: 0.8289 - val_loss: 0.5036 - val_accuracy: 0.8300\n",
            "Epoch 33/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.5052 - accuracy: 0.8295 - val_loss: 0.4891 - val_accuracy: 0.8376\n",
            "Epoch 34/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.4864 - accuracy: 0.8375 - val_loss: 0.4814 - val_accuracy: 0.8382\n",
            "Epoch 35/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.4794 - accuracy: 0.8382 - val_loss: 0.4762 - val_accuracy: 0.8389\n",
            "Epoch 36/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4736 - accuracy: 0.8402 - val_loss: 0.4803 - val_accuracy: 0.8379\n",
            "Epoch 37/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4837 - accuracy: 0.8341 - val_loss: 0.4677 - val_accuracy: 0.8407\n",
            "Epoch 38/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.4628 - accuracy: 0.8430 - val_loss: 0.4543 - val_accuracy: 0.8465\n",
            "Epoch 39/1000\n",
            "11/11 [==============================] - 1s 74ms/step - loss: 0.4522 - accuracy: 0.8480 - val_loss: 0.4494 - val_accuracy: 0.8499\n",
            "Epoch 40/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.4461 - accuracy: 0.8492 - val_loss: 0.4424 - val_accuracy: 0.8505\n",
            "Epoch 41/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4404 - accuracy: 0.8513 - val_loss: 0.4454 - val_accuracy: 0.8487\n",
            "Epoch 42/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4359 - accuracy: 0.8525 - val_loss: 0.4372 - val_accuracy: 0.8511\n",
            "Epoch 43/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.4366 - accuracy: 0.8516 - val_loss: 0.4202 - val_accuracy: 0.8592\n",
            "Epoch 44/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4212 - accuracy: 0.8581 - val_loss: 0.4181 - val_accuracy: 0.8589\n",
            "Epoch 45/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4142 - accuracy: 0.8610 - val_loss: 0.4249 - val_accuracy: 0.8548\n",
            "Epoch 46/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.4146 - accuracy: 0.8601 - val_loss: 0.4114 - val_accuracy: 0.8635\n",
            "Epoch 47/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.4069 - accuracy: 0.8638 - val_loss: 0.4195 - val_accuracy: 0.8586\n",
            "Epoch 48/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4066 - accuracy: 0.8631 - val_loss: 0.4174 - val_accuracy: 0.8588\n",
            "Epoch 49/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4059 - accuracy: 0.8634 - val_loss: 0.4060 - val_accuracy: 0.8628\n",
            "Epoch 50/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3957 - accuracy: 0.8681 - val_loss: 0.3908 - val_accuracy: 0.8702\n",
            "Epoch 51/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3866 - accuracy: 0.8702 - val_loss: 0.4139 - val_accuracy: 0.8594\n",
            "Epoch 52/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4007 - accuracy: 0.8636 - val_loss: 0.4283 - val_accuracy: 0.8503\n",
            "Epoch 53/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4161 - accuracy: 0.8574 - val_loss: 0.3825 - val_accuracy: 0.8742\n",
            "Epoch 54/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3787 - accuracy: 0.8742 - val_loss: 0.3728 - val_accuracy: 0.8749\n",
            "Epoch 55/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.3693 - accuracy: 0.8762 - val_loss: 0.3719 - val_accuracy: 0.8756\n",
            "Epoch 56/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.3673 - accuracy: 0.8778 - val_loss: 0.3675 - val_accuracy: 0.8781\n",
            "Epoch 57/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3666 - accuracy: 0.8778 - val_loss: 0.3630 - val_accuracy: 0.8811\n",
            "Epoch 58/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.3598 - accuracy: 0.8804 - val_loss: 0.3573 - val_accuracy: 0.8820\n",
            "Epoch 59/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3547 - accuracy: 0.8825 - val_loss: 0.3579 - val_accuracy: 0.8803\n",
            "Epoch 60/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3539 - accuracy: 0.8817 - val_loss: 0.3513 - val_accuracy: 0.8838\n",
            "Epoch 61/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3534 - accuracy: 0.8832 - val_loss: 0.3469 - val_accuracy: 0.8855\n",
            "Epoch 62/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.3476 - accuracy: 0.8854 - val_loss: 0.3529 - val_accuracy: 0.8842\n",
            "Epoch 63/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3447 - accuracy: 0.8864 - val_loss: 0.3432 - val_accuracy: 0.8865\n",
            "Epoch 64/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3429 - accuracy: 0.8867 - val_loss: 0.3458 - val_accuracy: 0.8861\n",
            "Epoch 65/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3400 - accuracy: 0.8882 - val_loss: 0.3634 - val_accuracy: 0.8761\n",
            "Epoch 66/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3632 - accuracy: 0.8774 - val_loss: 0.3590 - val_accuracy: 0.8770\n",
            "Epoch 67/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3408 - accuracy: 0.8865 - val_loss: 0.3328 - val_accuracy: 0.8898\n",
            "Epoch 68/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3290 - accuracy: 0.8921 - val_loss: 0.3336 - val_accuracy: 0.8893\n",
            "Epoch 69/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3263 - accuracy: 0.8929 - val_loss: 0.3254 - val_accuracy: 0.8939\n",
            "Epoch 70/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3274 - accuracy: 0.8920 - val_loss: 0.3247 - val_accuracy: 0.8944\n",
            "Epoch 71/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.3287 - accuracy: 0.8917 - val_loss: 0.3222 - val_accuracy: 0.8948\n",
            "Epoch 72/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3320 - accuracy: 0.8895 - val_loss: 0.3381 - val_accuracy: 0.8857\n",
            "Epoch 73/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3311 - accuracy: 0.8898 - val_loss: 0.3208 - val_accuracy: 0.8961\n",
            "Epoch 74/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.3275 - accuracy: 0.8908 - val_loss: 0.3326 - val_accuracy: 0.8909\n",
            "Epoch 75/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3274 - accuracy: 0.8917 - val_loss: 0.3164 - val_accuracy: 0.8965\n",
            "Epoch 76/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3172 - accuracy: 0.8949 - val_loss: 0.3289 - val_accuracy: 0.8905\n",
            "Epoch 77/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3150 - accuracy: 0.8970 - val_loss: 0.3077 - val_accuracy: 0.9003\n",
            "Epoch 78/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3030 - accuracy: 0.9017 - val_loss: 0.3054 - val_accuracy: 0.9017\n",
            "Epoch 79/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3002 - accuracy: 0.9025 - val_loss: 0.3036 - val_accuracy: 0.9008\n",
            "Epoch 80/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.3001 - accuracy: 0.9018 - val_loss: 0.3126 - val_accuracy: 0.8972\n",
            "Epoch 81/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.3089 - accuracy: 0.8977 - val_loss: 0.2993 - val_accuracy: 0.9031\n",
            "Epoch 82/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2981 - accuracy: 0.9023 - val_loss: 0.3004 - val_accuracy: 0.9017\n",
            "Epoch 83/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2960 - accuracy: 0.9039 - val_loss: 0.3019 - val_accuracy: 0.9005\n",
            "Epoch 84/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2923 - accuracy: 0.9047 - val_loss: 0.3090 - val_accuracy: 0.8991\n",
            "Epoch 85/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.2966 - accuracy: 0.9027 - val_loss: 0.2906 - val_accuracy: 0.9054\n",
            "Epoch 86/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2855 - accuracy: 0.9083 - val_loss: 0.2911 - val_accuracy: 0.9045\n",
            "Epoch 87/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2852 - accuracy: 0.9084 - val_loss: 0.2886 - val_accuracy: 0.9060\n",
            "Epoch 88/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2846 - accuracy: 0.9083 - val_loss: 0.3075 - val_accuracy: 0.8969\n",
            "Epoch 89/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2926 - accuracy: 0.9046 - val_loss: 0.3049 - val_accuracy: 0.8996\n",
            "Epoch 90/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2881 - accuracy: 0.9066 - val_loss: 0.2886 - val_accuracy: 0.9054\n",
            "Epoch 91/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2817 - accuracy: 0.9098 - val_loss: 0.3001 - val_accuracy: 0.8983\n",
            "Epoch 92/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2871 - accuracy: 0.9064 - val_loss: 0.2772 - val_accuracy: 0.9110\n",
            "Epoch 93/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2700 - accuracy: 0.9146 - val_loss: 0.2812 - val_accuracy: 0.9085\n",
            "Epoch 94/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.2741 - accuracy: 0.9124 - val_loss: 0.2835 - val_accuracy: 0.9073\n",
            "Epoch 95/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.2781 - accuracy: 0.9112 - val_loss: 0.2754 - val_accuracy: 0.9115\n",
            "Epoch 96/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2701 - accuracy: 0.9137 - val_loss: 0.2712 - val_accuracy: 0.9140\n",
            "Epoch 97/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.2628 - accuracy: 0.9171 - val_loss: 0.2737 - val_accuracy: 0.9124\n",
            "Epoch 98/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2625 - accuracy: 0.9175 - val_loss: 0.2732 - val_accuracy: 0.9122\n",
            "Epoch 99/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2650 - accuracy: 0.9167 - val_loss: 0.2825 - val_accuracy: 0.9065\n",
            "Epoch 100/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2727 - accuracy: 0.9124 - val_loss: 0.2676 - val_accuracy: 0.9141\n",
            "Epoch 101/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2592 - accuracy: 0.9187 - val_loss: 0.2597 - val_accuracy: 0.9177\n",
            "Epoch 102/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2535 - accuracy: 0.9210 - val_loss: 0.2776 - val_accuracy: 0.9082\n",
            "Epoch 103/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2613 - accuracy: 0.9171 - val_loss: 0.2581 - val_accuracy: 0.9184\n",
            "Epoch 104/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2515 - accuracy: 0.9221 - val_loss: 0.2525 - val_accuracy: 0.9216\n",
            "Epoch 105/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.2496 - accuracy: 0.9231 - val_loss: 0.2712 - val_accuracy: 0.9125\n",
            "Epoch 106/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2643 - accuracy: 0.9157 - val_loss: 0.2774 - val_accuracy: 0.9109\n",
            "Epoch 107/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2597 - accuracy: 0.9182 - val_loss: 0.2582 - val_accuracy: 0.9178\n",
            "Epoch 108/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2528 - accuracy: 0.9207 - val_loss: 0.2591 - val_accuracy: 0.9186\n",
            "Epoch 109/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2520 - accuracy: 0.9220 - val_loss: 0.2498 - val_accuracy: 0.9225\n",
            "Epoch 110/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2451 - accuracy: 0.9252 - val_loss: 0.2452 - val_accuracy: 0.9239\n",
            "Epoch 111/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2432 - accuracy: 0.9255 - val_loss: 0.2433 - val_accuracy: 0.9237\n",
            "Epoch 112/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2388 - accuracy: 0.9265 - val_loss: 0.2489 - val_accuracy: 0.9221\n",
            "Epoch 113/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.2439 - accuracy: 0.9232 - val_loss: 0.2595 - val_accuracy: 0.9159\n",
            "Epoch 114/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2485 - accuracy: 0.9224 - val_loss: 0.2376 - val_accuracy: 0.9264\n",
            "Epoch 115/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2372 - accuracy: 0.9275 - val_loss: 0.2452 - val_accuracy: 0.9244\n",
            "Epoch 116/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2415 - accuracy: 0.9256 - val_loss: 0.2351 - val_accuracy: 0.9277\n",
            "Epoch 117/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2315 - accuracy: 0.9300 - val_loss: 0.2381 - val_accuracy: 0.9250\n",
            "Epoch 118/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2421 - accuracy: 0.9239 - val_loss: 0.2441 - val_accuracy: 0.9214\n",
            "Epoch 119/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2346 - accuracy: 0.9281 - val_loss: 0.2336 - val_accuracy: 0.9272\n",
            "Epoch 120/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.2250 - accuracy: 0.9321 - val_loss: 0.2298 - val_accuracy: 0.9293\n",
            "Epoch 121/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2287 - accuracy: 0.9310 - val_loss: 0.2317 - val_accuracy: 0.9285\n",
            "Epoch 122/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2241 - accuracy: 0.9315 - val_loss: 0.2532 - val_accuracy: 0.9165\n",
            "Epoch 123/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2338 - accuracy: 0.9268 - val_loss: 0.2375 - val_accuracy: 0.9240\n",
            "Epoch 124/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2276 - accuracy: 0.9300 - val_loss: 0.2297 - val_accuracy: 0.9300\n",
            "Epoch 125/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2256 - accuracy: 0.9313 - val_loss: 0.2290 - val_accuracy: 0.9312\n",
            "Epoch 126/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2223 - accuracy: 0.9327 - val_loss: 0.2351 - val_accuracy: 0.9259\n",
            "Epoch 127/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2216 - accuracy: 0.9327 - val_loss: 0.2263 - val_accuracy: 0.9299\n",
            "Epoch 128/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2178 - accuracy: 0.9345 - val_loss: 0.2296 - val_accuracy: 0.9296\n",
            "Epoch 129/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2217 - accuracy: 0.9331 - val_loss: 0.2228 - val_accuracy: 0.9317\n",
            "Epoch 130/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2264 - accuracy: 0.9302 - val_loss: 0.2262 - val_accuracy: 0.9295\n",
            "Epoch 131/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2202 - accuracy: 0.9323 - val_loss: 0.2165 - val_accuracy: 0.9334\n",
            "Epoch 132/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2116 - accuracy: 0.9369 - val_loss: 0.2241 - val_accuracy: 0.9313\n",
            "Epoch 133/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2286 - accuracy: 0.9293 - val_loss: 0.2188 - val_accuracy: 0.9323\n",
            "Epoch 134/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2112 - accuracy: 0.9370 - val_loss: 0.2127 - val_accuracy: 0.9348\n",
            "Epoch 135/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2107 - accuracy: 0.9371 - val_loss: 0.2091 - val_accuracy: 0.9374\n",
            "Epoch 136/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2054 - accuracy: 0.9396 - val_loss: 0.2107 - val_accuracy: 0.9359\n",
            "Epoch 137/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2053 - accuracy: 0.9389 - val_loss: 0.2280 - val_accuracy: 0.9295\n",
            "Epoch 138/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2218 - accuracy: 0.9308 - val_loss: 0.2327 - val_accuracy: 0.9263\n",
            "Epoch 139/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2229 - accuracy: 0.9307 - val_loss: 0.2165 - val_accuracy: 0.9337\n",
            "Epoch 140/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.2125 - accuracy: 0.9362 - val_loss: 0.2125 - val_accuracy: 0.9358\n",
            "Epoch 141/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2093 - accuracy: 0.9372 - val_loss: 0.2229 - val_accuracy: 0.9292\n",
            "Epoch 142/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2106 - accuracy: 0.9368 - val_loss: 0.2065 - val_accuracy: 0.9374\n",
            "Epoch 143/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1961 - accuracy: 0.9430 - val_loss: 0.2097 - val_accuracy: 0.9377\n",
            "Epoch 144/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2034 - accuracy: 0.9394 - val_loss: 0.2158 - val_accuracy: 0.9331\n",
            "Epoch 145/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2060 - accuracy: 0.9380 - val_loss: 0.2029 - val_accuracy: 0.9391\n",
            "Epoch 146/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1986 - accuracy: 0.9420 - val_loss: 0.1996 - val_accuracy: 0.9418\n",
            "Epoch 147/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1987 - accuracy: 0.9419 - val_loss: 0.1989 - val_accuracy: 0.9417\n",
            "Epoch 148/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1957 - accuracy: 0.9424 - val_loss: 0.1956 - val_accuracy: 0.9428\n",
            "Epoch 149/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2024 - accuracy: 0.9390 - val_loss: 0.2000 - val_accuracy: 0.9408\n",
            "Epoch 150/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2022 - accuracy: 0.9392 - val_loss: 0.1946 - val_accuracy: 0.9438\n",
            "Epoch 151/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1896 - accuracy: 0.9457 - val_loss: 0.1982 - val_accuracy: 0.9412\n",
            "Epoch 152/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1938 - accuracy: 0.9441 - val_loss: 0.1998 - val_accuracy: 0.9418\n",
            "Epoch 153/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1946 - accuracy: 0.9433 - val_loss: 0.2144 - val_accuracy: 0.9351\n",
            "Epoch 154/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1954 - accuracy: 0.9430 - val_loss: 0.1920 - val_accuracy: 0.9455\n",
            "Epoch 155/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1856 - accuracy: 0.9483 - val_loss: 0.1906 - val_accuracy: 0.9451\n",
            "Epoch 156/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1837 - accuracy: 0.9478 - val_loss: 0.1899 - val_accuracy: 0.9442\n",
            "Epoch 157/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1877 - accuracy: 0.9463 - val_loss: 0.1924 - val_accuracy: 0.9433\n",
            "Epoch 158/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1932 - accuracy: 0.9426 - val_loss: 0.1911 - val_accuracy: 0.9450\n",
            "Epoch 159/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1844 - accuracy: 0.9469 - val_loss: 0.1869 - val_accuracy: 0.9451\n",
            "Epoch 160/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1886 - accuracy: 0.9456 - val_loss: 0.1887 - val_accuracy: 0.9450\n",
            "Epoch 161/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1828 - accuracy: 0.9476 - val_loss: 0.1867 - val_accuracy: 0.9462\n",
            "Epoch 162/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1824 - accuracy: 0.9483 - val_loss: 0.1854 - val_accuracy: 0.9470\n",
            "Epoch 163/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1801 - accuracy: 0.9490 - val_loss: 0.1837 - val_accuracy: 0.9480\n",
            "Epoch 164/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1773 - accuracy: 0.9502 - val_loss: 0.1828 - val_accuracy: 0.9471\n",
            "Epoch 165/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1894 - accuracy: 0.9442 - val_loss: 0.2106 - val_accuracy: 0.9339\n",
            "Epoch 166/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1918 - accuracy: 0.9435 - val_loss: 0.1872 - val_accuracy: 0.9447\n",
            "Epoch 167/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1863 - accuracy: 0.9452 - val_loss: 0.1862 - val_accuracy: 0.9451\n",
            "Epoch 168/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1803 - accuracy: 0.9487 - val_loss: 0.2130 - val_accuracy: 0.9335\n",
            "Epoch 169/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2005 - accuracy: 0.9389 - val_loss: 0.1865 - val_accuracy: 0.9456\n",
            "Epoch 170/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1830 - accuracy: 0.9470 - val_loss: 0.1886 - val_accuracy: 0.9443\n",
            "Epoch 171/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1866 - accuracy: 0.9451 - val_loss: 0.1824 - val_accuracy: 0.9466\n",
            "Epoch 172/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1801 - accuracy: 0.9480 - val_loss: 0.1869 - val_accuracy: 0.9449\n",
            "Epoch 173/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1782 - accuracy: 0.9481 - val_loss: 0.1757 - val_accuracy: 0.9506\n",
            "Epoch 174/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1721 - accuracy: 0.9525 - val_loss: 0.1914 - val_accuracy: 0.9432\n",
            "Epoch 175/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1821 - accuracy: 0.9474 - val_loss: 0.1814 - val_accuracy: 0.9467\n",
            "Epoch 176/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1764 - accuracy: 0.9504 - val_loss: 0.1796 - val_accuracy: 0.9472\n",
            "Epoch 177/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1733 - accuracy: 0.9510 - val_loss: 0.1746 - val_accuracy: 0.9503\n",
            "Epoch 178/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1689 - accuracy: 0.9533 - val_loss: 0.1702 - val_accuracy: 0.9524\n",
            "Epoch 179/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.1646 - accuracy: 0.9544 - val_loss: 0.1759 - val_accuracy: 0.9496\n",
            "Epoch 180/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1667 - accuracy: 0.9539 - val_loss: 0.1730 - val_accuracy: 0.9511\n",
            "Epoch 181/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1662 - accuracy: 0.9542 - val_loss: 0.1699 - val_accuracy: 0.9513\n",
            "Epoch 182/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1653 - accuracy: 0.9548 - val_loss: 0.1700 - val_accuracy: 0.9518\n",
            "Epoch 183/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1670 - accuracy: 0.9535 - val_loss: 0.1784 - val_accuracy: 0.9479\n",
            "Epoch 184/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1697 - accuracy: 0.9519 - val_loss: 0.1763 - val_accuracy: 0.9486\n",
            "Epoch 185/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1742 - accuracy: 0.9496 - val_loss: 0.2072 - val_accuracy: 0.9362\n",
            "Epoch 186/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1973 - accuracy: 0.9401 - val_loss: 0.1812 - val_accuracy: 0.9458\n",
            "Epoch 187/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1784 - accuracy: 0.9482 - val_loss: 0.1740 - val_accuracy: 0.9502\n",
            "Epoch 188/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1685 - accuracy: 0.9527 - val_loss: 0.1683 - val_accuracy: 0.9527\n",
            "Epoch 189/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1633 - accuracy: 0.9548 - val_loss: 0.1791 - val_accuracy: 0.9464\n",
            "Epoch 190/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1676 - accuracy: 0.9524 - val_loss: 0.1669 - val_accuracy: 0.9531\n",
            "Epoch 191/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1640 - accuracy: 0.9545 - val_loss: 0.1641 - val_accuracy: 0.9546\n",
            "Epoch 192/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1585 - accuracy: 0.9569 - val_loss: 0.1656 - val_accuracy: 0.9537\n",
            "Epoch 193/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1603 - accuracy: 0.9562 - val_loss: 0.1673 - val_accuracy: 0.9521\n",
            "Epoch 194/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1620 - accuracy: 0.9551 - val_loss: 0.1648 - val_accuracy: 0.9534\n",
            "Epoch 195/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1641 - accuracy: 0.9541 - val_loss: 0.1700 - val_accuracy: 0.9503\n",
            "Epoch 196/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1598 - accuracy: 0.9556 - val_loss: 0.1696 - val_accuracy: 0.9511\n",
            "Epoch 197/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1602 - accuracy: 0.9552 - val_loss: 0.1594 - val_accuracy: 0.9554\n",
            "Epoch 198/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1554 - accuracy: 0.9576 - val_loss: 0.1639 - val_accuracy: 0.9545\n",
            "Epoch 199/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1561 - accuracy: 0.9576 - val_loss: 0.1605 - val_accuracy: 0.9558\n",
            "Epoch 200/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1530 - accuracy: 0.9594 - val_loss: 0.1587 - val_accuracy: 0.9559\n",
            "Epoch 201/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1540 - accuracy: 0.9579 - val_loss: 0.1711 - val_accuracy: 0.9506\n",
            "Epoch 202/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1623 - accuracy: 0.9540 - val_loss: 0.1603 - val_accuracy: 0.9553\n",
            "Epoch 203/1000\n",
            "11/11 [==============================] - 1s 85ms/step - loss: 0.1509 - accuracy: 0.9596 - val_loss: 0.1588 - val_accuracy: 0.9561\n",
            "Epoch 204/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1529 - accuracy: 0.9590 - val_loss: 0.1583 - val_accuracy: 0.9559\n",
            "Epoch 205/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1503 - accuracy: 0.9595 - val_loss: 0.1577 - val_accuracy: 0.9560\n",
            "Epoch 206/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1496 - accuracy: 0.9594 - val_loss: 0.1636 - val_accuracy: 0.9536\n",
            "Epoch 207/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1678 - accuracy: 0.9515 - val_loss: 0.2006 - val_accuracy: 0.9395\n",
            "Epoch 208/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1850 - accuracy: 0.9443 - val_loss: 0.1649 - val_accuracy: 0.9539\n",
            "Epoch 209/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1573 - accuracy: 0.9571 - val_loss: 0.1641 - val_accuracy: 0.9522\n",
            "Epoch 210/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1533 - accuracy: 0.9578 - val_loss: 0.1565 - val_accuracy: 0.9561\n",
            "Epoch 211/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1551 - accuracy: 0.9569 - val_loss: 0.1557 - val_accuracy: 0.9569\n",
            "Epoch 212/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1475 - accuracy: 0.9611 - val_loss: 0.1585 - val_accuracy: 0.9550\n",
            "Epoch 213/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1519 - accuracy: 0.9577 - val_loss: 0.1529 - val_accuracy: 0.9583\n",
            "Epoch 214/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1513 - accuracy: 0.9585 - val_loss: 0.1589 - val_accuracy: 0.9554\n",
            "Epoch 215/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1530 - accuracy: 0.9581 - val_loss: 0.1537 - val_accuracy: 0.9577\n",
            "Epoch 216/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1515 - accuracy: 0.9585 - val_loss: 0.1544 - val_accuracy: 0.9585\n",
            "Epoch 217/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1477 - accuracy: 0.9600 - val_loss: 0.1546 - val_accuracy: 0.9565\n",
            "Epoch 218/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1481 - accuracy: 0.9600 - val_loss: 0.1617 - val_accuracy: 0.9532\n",
            "Epoch 219/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1505 - accuracy: 0.9582 - val_loss: 0.1500 - val_accuracy: 0.9590\n",
            "Epoch 220/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1435 - accuracy: 0.9613 - val_loss: 0.1585 - val_accuracy: 0.9557\n",
            "Epoch 221/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1525 - accuracy: 0.9580 - val_loss: 0.1701 - val_accuracy: 0.9508\n",
            "Epoch 222/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1573 - accuracy: 0.9563 - val_loss: 0.1523 - val_accuracy: 0.9583\n",
            "Epoch 223/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1454 - accuracy: 0.9610 - val_loss: 0.1473 - val_accuracy: 0.9607\n",
            "Epoch 224/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1436 - accuracy: 0.9614 - val_loss: 0.1500 - val_accuracy: 0.9587\n",
            "Epoch 225/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1434 - accuracy: 0.9622 - val_loss: 0.1499 - val_accuracy: 0.9587\n",
            "Epoch 226/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1440 - accuracy: 0.9614 - val_loss: 0.1486 - val_accuracy: 0.9594\n",
            "Epoch 227/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1437 - accuracy: 0.9616 - val_loss: 0.1476 - val_accuracy: 0.9597\n",
            "Epoch 228/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1439 - accuracy: 0.9614 - val_loss: 0.1469 - val_accuracy: 0.9605\n",
            "Epoch 229/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1427 - accuracy: 0.9620 - val_loss: 0.1515 - val_accuracy: 0.9570\n",
            "Epoch 230/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1424 - accuracy: 0.9620 - val_loss: 0.1502 - val_accuracy: 0.9596\n",
            "Epoch 231/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1419 - accuracy: 0.9622 - val_loss: 0.1441 - val_accuracy: 0.9610\n",
            "Epoch 232/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1714 - accuracy: 0.9486 - val_loss: 0.1647 - val_accuracy: 0.9520\n",
            "Epoch 233/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1558 - accuracy: 0.9555 - val_loss: 0.1550 - val_accuracy: 0.9562\n",
            "Epoch 234/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1476 - accuracy: 0.9593 - val_loss: 0.1458 - val_accuracy: 0.9603\n",
            "Epoch 235/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1403 - accuracy: 0.9630 - val_loss: 0.1463 - val_accuracy: 0.9593\n",
            "Epoch 236/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1385 - accuracy: 0.9634 - val_loss: 0.1418 - val_accuracy: 0.9623\n",
            "Epoch 237/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1415 - accuracy: 0.9618 - val_loss: 0.1469 - val_accuracy: 0.9591\n",
            "Epoch 238/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1451 - accuracy: 0.9601 - val_loss: 0.1474 - val_accuracy: 0.9587\n",
            "Epoch 239/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1412 - accuracy: 0.9625 - val_loss: 0.1423 - val_accuracy: 0.9619\n",
            "Epoch 240/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1352 - accuracy: 0.9649 - val_loss: 0.1407 - val_accuracy: 0.9621\n",
            "Epoch 241/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1347 - accuracy: 0.9656 - val_loss: 0.1439 - val_accuracy: 0.9605\n",
            "Epoch 242/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1377 - accuracy: 0.9636 - val_loss: 0.1428 - val_accuracy: 0.9605\n",
            "Epoch 243/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1367 - accuracy: 0.9643 - val_loss: 0.1414 - val_accuracy: 0.9613\n",
            "Epoch 244/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1323 - accuracy: 0.9658 - val_loss: 0.1414 - val_accuracy: 0.9626\n",
            "Epoch 245/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1309 - accuracy: 0.9670 - val_loss: 0.1372 - val_accuracy: 0.9635\n",
            "Epoch 246/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1312 - accuracy: 0.9664 - val_loss: 0.1509 - val_accuracy: 0.9561\n",
            "Epoch 247/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1471 - accuracy: 0.9583 - val_loss: 0.1790 - val_accuracy: 0.9467\n",
            "Epoch 248/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1605 - accuracy: 0.9530 - val_loss: 0.1424 - val_accuracy: 0.9621\n",
            "Epoch 249/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1389 - accuracy: 0.9627 - val_loss: 0.1387 - val_accuracy: 0.9627\n",
            "Epoch 250/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1323 - accuracy: 0.9655 - val_loss: 0.1391 - val_accuracy: 0.9629\n",
            "Epoch 251/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1325 - accuracy: 0.9655 - val_loss: 0.1364 - val_accuracy: 0.9634\n",
            "Epoch 252/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1304 - accuracy: 0.9667 - val_loss: 0.1410 - val_accuracy: 0.9610\n",
            "Epoch 253/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1318 - accuracy: 0.9658 - val_loss: 0.1394 - val_accuracy: 0.9618\n",
            "Epoch 254/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1337 - accuracy: 0.9646 - val_loss: 0.1459 - val_accuracy: 0.9582\n",
            "Epoch 255/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1376 - accuracy: 0.9635 - val_loss: 0.1377 - val_accuracy: 0.9626\n",
            "Epoch 256/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1306 - accuracy: 0.9665 - val_loss: 0.1445 - val_accuracy: 0.9597\n",
            "Epoch 257/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1355 - accuracy: 0.9634 - val_loss: 0.1353 - val_accuracy: 0.9639\n",
            "Epoch 258/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1302 - accuracy: 0.9665 - val_loss: 0.1640 - val_accuracy: 0.9499\n",
            "Epoch 259/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1491 - accuracy: 0.9582 - val_loss: 0.1431 - val_accuracy: 0.9609\n",
            "Epoch 260/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1322 - accuracy: 0.9656 - val_loss: 0.1426 - val_accuracy: 0.9603\n",
            "Epoch 261/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1376 - accuracy: 0.9628 - val_loss: 0.1460 - val_accuracy: 0.9590\n",
            "Epoch 262/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1358 - accuracy: 0.9637 - val_loss: 0.1377 - val_accuracy: 0.9630\n",
            "Epoch 263/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1284 - accuracy: 0.9671 - val_loss: 0.1360 - val_accuracy: 0.9634\n",
            "Epoch 264/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1282 - accuracy: 0.9667 - val_loss: 0.1345 - val_accuracy: 0.9646\n",
            "Epoch 265/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1293 - accuracy: 0.9670 - val_loss: 0.1415 - val_accuracy: 0.9605\n",
            "Epoch 266/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1358 - accuracy: 0.9630 - val_loss: 0.1425 - val_accuracy: 0.9595\n",
            "Epoch 267/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1306 - accuracy: 0.9658 - val_loss: 0.1313 - val_accuracy: 0.9647\n",
            "Epoch 268/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1229 - accuracy: 0.9690 - val_loss: 0.1349 - val_accuracy: 0.9634\n",
            "Epoch 269/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1263 - accuracy: 0.9676 - val_loss: 0.1346 - val_accuracy: 0.9632\n",
            "Epoch 270/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1302 - accuracy: 0.9653 - val_loss: 0.1413 - val_accuracy: 0.9602\n",
            "Epoch 271/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1301 - accuracy: 0.9658 - val_loss: 0.1341 - val_accuracy: 0.9640\n",
            "Epoch 272/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1313 - accuracy: 0.9651 - val_loss: 0.1346 - val_accuracy: 0.9631\n",
            "Epoch 273/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1255 - accuracy: 0.9686 - val_loss: 0.1337 - val_accuracy: 0.9639\n",
            "Epoch 274/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1274 - accuracy: 0.9668 - val_loss: 0.1343 - val_accuracy: 0.9635\n",
            "Epoch 275/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1317 - accuracy: 0.9650 - val_loss: 0.1302 - val_accuracy: 0.9662\n",
            "Epoch 276/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1242 - accuracy: 0.9685 - val_loss: 0.1306 - val_accuracy: 0.9661\n",
            "Epoch 277/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1234 - accuracy: 0.9688 - val_loss: 0.1324 - val_accuracy: 0.9641\n",
            "Epoch 278/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1292 - accuracy: 0.9659 - val_loss: 0.1363 - val_accuracy: 0.9630\n",
            "Epoch 279/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1280 - accuracy: 0.9671 - val_loss: 0.1308 - val_accuracy: 0.9653\n",
            "Epoch 280/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1215 - accuracy: 0.9690 - val_loss: 0.1282 - val_accuracy: 0.9661\n",
            "Epoch 281/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1196 - accuracy: 0.9705 - val_loss: 0.1365 - val_accuracy: 0.9634\n",
            "Epoch 282/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1253 - accuracy: 0.9676 - val_loss: 0.1348 - val_accuracy: 0.9627\n",
            "Epoch 283/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1252 - accuracy: 0.9667 - val_loss: 0.1306 - val_accuracy: 0.9654\n",
            "Epoch 284/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1232 - accuracy: 0.9689 - val_loss: 0.1258 - val_accuracy: 0.9679\n",
            "Epoch 285/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1190 - accuracy: 0.9699 - val_loss: 0.1270 - val_accuracy: 0.9661\n",
            "Epoch 286/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1201 - accuracy: 0.9700 - val_loss: 0.1278 - val_accuracy: 0.9663\n",
            "Epoch 287/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1189 - accuracy: 0.9703 - val_loss: 0.1315 - val_accuracy: 0.9637\n",
            "Epoch 288/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1218 - accuracy: 0.9685 - val_loss: 0.1338 - val_accuracy: 0.9638\n",
            "Epoch 289/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1240 - accuracy: 0.9679 - val_loss: 0.1308 - val_accuracy: 0.9640\n",
            "Epoch 290/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1196 - accuracy: 0.9701 - val_loss: 0.1294 - val_accuracy: 0.9650\n",
            "Epoch 291/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1181 - accuracy: 0.9701 - val_loss: 0.1271 - val_accuracy: 0.9660\n",
            "Epoch 292/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1196 - accuracy: 0.9695 - val_loss: 0.1340 - val_accuracy: 0.9628\n",
            "Epoch 293/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1267 - accuracy: 0.9665 - val_loss: 0.1293 - val_accuracy: 0.9665\n",
            "Epoch 294/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1201 - accuracy: 0.9694 - val_loss: 0.1392 - val_accuracy: 0.9605\n",
            "Epoch 295/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1338 - accuracy: 0.9632 - val_loss: 0.1321 - val_accuracy: 0.9650\n",
            "Epoch 296/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1220 - accuracy: 0.9688 - val_loss: 0.1228 - val_accuracy: 0.9690\n",
            "Epoch 297/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1143 - accuracy: 0.9716 - val_loss: 0.1289 - val_accuracy: 0.9653\n",
            "Epoch 298/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1216 - accuracy: 0.9684 - val_loss: 0.1288 - val_accuracy: 0.9652\n",
            "Epoch 299/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1202 - accuracy: 0.9692 - val_loss: 0.1236 - val_accuracy: 0.9677\n",
            "Epoch 300/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1144 - accuracy: 0.9720 - val_loss: 0.1219 - val_accuracy: 0.9688\n",
            "Epoch 301/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1132 - accuracy: 0.9722 - val_loss: 0.1210 - val_accuracy: 0.9689\n",
            "Epoch 302/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1137 - accuracy: 0.9716 - val_loss: 0.1305 - val_accuracy: 0.9640\n",
            "Epoch 303/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1215 - accuracy: 0.9682 - val_loss: 0.1246 - val_accuracy: 0.9672\n",
            "Epoch 304/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1313 - accuracy: 0.9643 - val_loss: 0.1290 - val_accuracy: 0.9655\n",
            "Epoch 305/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1264 - accuracy: 0.9672 - val_loss: 0.1273 - val_accuracy: 0.9659\n",
            "Epoch 306/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1222 - accuracy: 0.9684 - val_loss: 0.1234 - val_accuracy: 0.9671\n",
            "Epoch 307/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1152 - accuracy: 0.9709 - val_loss: 0.1218 - val_accuracy: 0.9680\n",
            "Epoch 308/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1119 - accuracy: 0.9724 - val_loss: 0.1234 - val_accuracy: 0.9671\n",
            "Epoch 309/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1138 - accuracy: 0.9712 - val_loss: 0.1205 - val_accuracy: 0.9685\n",
            "Epoch 310/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1100 - accuracy: 0.9730 - val_loss: 0.1193 - val_accuracy: 0.9692\n",
            "Epoch 311/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1107 - accuracy: 0.9726 - val_loss: 0.1203 - val_accuracy: 0.9682\n",
            "Epoch 312/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1128 - accuracy: 0.9717 - val_loss: 0.1206 - val_accuracy: 0.9685\n",
            "Epoch 313/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1134 - accuracy: 0.9715 - val_loss: 0.1238 - val_accuracy: 0.9668\n",
            "Epoch 314/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1140 - accuracy: 0.9710 - val_loss: 0.1253 - val_accuracy: 0.9662\n",
            "Epoch 315/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1146 - accuracy: 0.9715 - val_loss: 0.1205 - val_accuracy: 0.9682\n",
            "Epoch 316/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1128 - accuracy: 0.9721 - val_loss: 0.1173 - val_accuracy: 0.9698\n",
            "Epoch 317/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1101 - accuracy: 0.9727 - val_loss: 0.1244 - val_accuracy: 0.9660\n",
            "Epoch 318/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1112 - accuracy: 0.9721 - val_loss: 0.1188 - val_accuracy: 0.9696\n",
            "Epoch 319/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1239 - accuracy: 0.9676 - val_loss: 0.1546 - val_accuracy: 0.9554\n",
            "Epoch 320/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1322 - accuracy: 0.9631 - val_loss: 0.1256 - val_accuracy: 0.9666\n",
            "Epoch 321/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1150 - accuracy: 0.9715 - val_loss: 0.1158 - val_accuracy: 0.9704\n",
            "Epoch 322/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1079 - accuracy: 0.9733 - val_loss: 0.1189 - val_accuracy: 0.9680\n",
            "Epoch 323/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1099 - accuracy: 0.9725 - val_loss: 0.1157 - val_accuracy: 0.9701\n",
            "Epoch 324/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1074 - accuracy: 0.9741 - val_loss: 0.1165 - val_accuracy: 0.9692\n",
            "Epoch 325/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1079 - accuracy: 0.9736 - val_loss: 0.1346 - val_accuracy: 0.9621\n",
            "Epoch 326/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1177 - accuracy: 0.9697 - val_loss: 0.1178 - val_accuracy: 0.9691\n",
            "Epoch 327/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1069 - accuracy: 0.9739 - val_loss: 0.1131 - val_accuracy: 0.9711\n",
            "Epoch 328/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1034 - accuracy: 0.9755 - val_loss: 0.1152 - val_accuracy: 0.9697\n",
            "Epoch 329/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1043 - accuracy: 0.9747 - val_loss: 0.1133 - val_accuracy: 0.9707\n",
            "Epoch 330/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1091 - accuracy: 0.9727 - val_loss: 0.1306 - val_accuracy: 0.9646\n",
            "Epoch 331/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1228 - accuracy: 0.9671 - val_loss: 0.1237 - val_accuracy: 0.9665\n",
            "Epoch 332/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1220 - accuracy: 0.9673 - val_loss: 0.1361 - val_accuracy: 0.9612\n",
            "Epoch 333/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1184 - accuracy: 0.9690 - val_loss: 0.1230 - val_accuracy: 0.9658\n",
            "Epoch 334/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1121 - accuracy: 0.9717 - val_loss: 0.1168 - val_accuracy: 0.9698\n",
            "Epoch 335/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1072 - accuracy: 0.9739 - val_loss: 0.1175 - val_accuracy: 0.9688\n",
            "Epoch 336/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1060 - accuracy: 0.9738 - val_loss: 0.1128 - val_accuracy: 0.9707\n",
            "Epoch 337/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1046 - accuracy: 0.9745 - val_loss: 0.1151 - val_accuracy: 0.9692\n",
            "Epoch 338/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1064 - accuracy: 0.9734 - val_loss: 0.1215 - val_accuracy: 0.9670\n",
            "Epoch 339/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1163 - accuracy: 0.9695 - val_loss: 0.1155 - val_accuracy: 0.9705\n",
            "Epoch 340/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1104 - accuracy: 0.9726 - val_loss: 0.1161 - val_accuracy: 0.9692\n",
            "Epoch 341/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1079 - accuracy: 0.9730 - val_loss: 0.1216 - val_accuracy: 0.9669\n",
            "Epoch 342/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1093 - accuracy: 0.9723 - val_loss: 0.1187 - val_accuracy: 0.9685\n",
            "Epoch 343/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1067 - accuracy: 0.9739 - val_loss: 0.1160 - val_accuracy: 0.9698\n",
            "Epoch 344/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1067 - accuracy: 0.9733 - val_loss: 0.1199 - val_accuracy: 0.9677\n",
            "Epoch 345/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1059 - accuracy: 0.9737 - val_loss: 0.1136 - val_accuracy: 0.9712\n",
            "Epoch 346/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1016 - accuracy: 0.9758 - val_loss: 0.1118 - val_accuracy: 0.9716\n",
            "Epoch 347/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1034 - accuracy: 0.9744 - val_loss: 0.1179 - val_accuracy: 0.9681\n",
            "Epoch 348/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1052 - accuracy: 0.9738 - val_loss: 0.1128 - val_accuracy: 0.9701\n",
            "Epoch 349/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1013 - accuracy: 0.9756 - val_loss: 0.1111 - val_accuracy: 0.9709\n",
            "Epoch 350/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1046 - accuracy: 0.9744 - val_loss: 0.1142 - val_accuracy: 0.9702\n",
            "Epoch 351/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1041 - accuracy: 0.9744 - val_loss: 0.1095 - val_accuracy: 0.9719\n",
            "Epoch 352/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1029 - accuracy: 0.9750 - val_loss: 0.1183 - val_accuracy: 0.9686\n",
            "Epoch 353/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1127 - accuracy: 0.9708 - val_loss: 0.1166 - val_accuracy: 0.9694\n",
            "Epoch 354/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1042 - accuracy: 0.9743 - val_loss: 0.1117 - val_accuracy: 0.9713\n",
            "Epoch 355/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1035 - accuracy: 0.9747 - val_loss: 0.1083 - val_accuracy: 0.9728\n",
            "Epoch 356/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1018 - accuracy: 0.9754 - val_loss: 0.1137 - val_accuracy: 0.9700\n",
            "Epoch 357/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1076 - accuracy: 0.9729 - val_loss: 0.1126 - val_accuracy: 0.9706\n",
            "Epoch 358/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1015 - accuracy: 0.9758 - val_loss: 0.1190 - val_accuracy: 0.9682\n",
            "Epoch 359/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1027 - accuracy: 0.9746 - val_loss: 0.1085 - val_accuracy: 0.9725\n",
            "Epoch 360/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1012 - accuracy: 0.9754 - val_loss: 0.1113 - val_accuracy: 0.9711\n",
            "Epoch 361/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1007 - accuracy: 0.9759 - val_loss: 0.1090 - val_accuracy: 0.9718\n",
            "Epoch 362/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1013 - accuracy: 0.9754 - val_loss: 0.1212 - val_accuracy: 0.9673\n",
            "Epoch 363/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1093 - accuracy: 0.9719 - val_loss: 0.1297 - val_accuracy: 0.9631\n",
            "Epoch 364/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1114 - accuracy: 0.9714 - val_loss: 0.1152 - val_accuracy: 0.9697\n",
            "Epoch 365/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1056 - accuracy: 0.9736 - val_loss: 0.1149 - val_accuracy: 0.9706\n",
            "Epoch 366/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1098 - accuracy: 0.9717 - val_loss: 0.1154 - val_accuracy: 0.9697\n",
            "Epoch 367/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1072 - accuracy: 0.9732 - val_loss: 0.1314 - val_accuracy: 0.9637\n",
            "Epoch 368/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1106 - accuracy: 0.9710 - val_loss: 0.1178 - val_accuracy: 0.9675\n",
            "Epoch 369/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1070 - accuracy: 0.9727 - val_loss: 0.1071 - val_accuracy: 0.9738\n",
            "Epoch 370/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0990 - accuracy: 0.9766 - val_loss: 0.1075 - val_accuracy: 0.9725\n",
            "Epoch 371/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0970 - accuracy: 0.9769 - val_loss: 0.1073 - val_accuracy: 0.9728\n",
            "Epoch 372/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0999 - accuracy: 0.9757 - val_loss: 0.1123 - val_accuracy: 0.9713\n",
            "Epoch 373/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0986 - accuracy: 0.9768 - val_loss: 0.1045 - val_accuracy: 0.9736\n",
            "Epoch 374/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0953 - accuracy: 0.9776 - val_loss: 0.1060 - val_accuracy: 0.9730\n",
            "Epoch 375/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0951 - accuracy: 0.9781 - val_loss: 0.1053 - val_accuracy: 0.9732\n",
            "Epoch 376/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0974 - accuracy: 0.9771 - val_loss: 0.1197 - val_accuracy: 0.9677\n",
            "Epoch 377/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1170 - accuracy: 0.9683 - val_loss: 0.1345 - val_accuracy: 0.9602\n",
            "Epoch 378/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1159 - accuracy: 0.9688 - val_loss: 0.1095 - val_accuracy: 0.9717\n",
            "Epoch 379/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1024 - accuracy: 0.9744 - val_loss: 0.1082 - val_accuracy: 0.9727\n",
            "Epoch 380/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0997 - accuracy: 0.9757 - val_loss: 0.1083 - val_accuracy: 0.9720\n",
            "Epoch 381/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0974 - accuracy: 0.9768 - val_loss: 0.1051 - val_accuracy: 0.9733\n",
            "Epoch 382/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0955 - accuracy: 0.9778 - val_loss: 0.1031 - val_accuracy: 0.9744\n",
            "Epoch 383/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0959 - accuracy: 0.9772 - val_loss: 0.1105 - val_accuracy: 0.9712\n",
            "Epoch 384/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1013 - accuracy: 0.9747 - val_loss: 0.1087 - val_accuracy: 0.9720\n",
            "Epoch 385/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0959 - accuracy: 0.9776 - val_loss: 0.1099 - val_accuracy: 0.9708\n",
            "Epoch 386/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0945 - accuracy: 0.9777 - val_loss: 0.1038 - val_accuracy: 0.9735\n",
            "Epoch 387/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0949 - accuracy: 0.9779 - val_loss: 0.1054 - val_accuracy: 0.9735\n",
            "Epoch 388/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0964 - accuracy: 0.9768 - val_loss: 0.1027 - val_accuracy: 0.9745\n",
            "Epoch 389/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0911 - accuracy: 0.9796 - val_loss: 0.1033 - val_accuracy: 0.9741\n",
            "Epoch 390/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0916 - accuracy: 0.9793 - val_loss: 0.1011 - val_accuracy: 0.9748\n",
            "Epoch 391/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0949 - accuracy: 0.9778 - val_loss: 0.1024 - val_accuracy: 0.9745\n",
            "Epoch 392/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1018 - accuracy: 0.9747 - val_loss: 0.1076 - val_accuracy: 0.9726\n",
            "Epoch 393/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0996 - accuracy: 0.9759 - val_loss: 0.1047 - val_accuracy: 0.9734\n",
            "Epoch 394/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0945 - accuracy: 0.9775 - val_loss: 0.1002 - val_accuracy: 0.9750\n",
            "Epoch 395/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0905 - accuracy: 0.9798 - val_loss: 0.1142 - val_accuracy: 0.9692\n",
            "Epoch 396/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1111 - accuracy: 0.9715 - val_loss: 0.1263 - val_accuracy: 0.9638\n",
            "Epoch 397/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1326 - accuracy: 0.9620 - val_loss: 0.1170 - val_accuracy: 0.9693\n",
            "Epoch 398/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1073 - accuracy: 0.9727 - val_loss: 0.1079 - val_accuracy: 0.9725\n",
            "Epoch 399/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0973 - accuracy: 0.9769 - val_loss: 0.1048 - val_accuracy: 0.9728\n",
            "Epoch 400/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0933 - accuracy: 0.9786 - val_loss: 0.1012 - val_accuracy: 0.9750\n",
            "Epoch 401/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0918 - accuracy: 0.9791 - val_loss: 0.1016 - val_accuracy: 0.9741\n",
            "Epoch 402/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0919 - accuracy: 0.9790 - val_loss: 0.1134 - val_accuracy: 0.9684\n",
            "Epoch 403/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0985 - accuracy: 0.9756 - val_loss: 0.1348 - val_accuracy: 0.9601\n",
            "Epoch 404/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1196 - accuracy: 0.9669 - val_loss: 0.1241 - val_accuracy: 0.9652\n",
            "Epoch 405/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1087 - accuracy: 0.9716 - val_loss: 0.1038 - val_accuracy: 0.9740\n",
            "Epoch 406/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0970 - accuracy: 0.9766 - val_loss: 0.1024 - val_accuracy: 0.9743\n",
            "Epoch 407/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0913 - accuracy: 0.9788 - val_loss: 0.1032 - val_accuracy: 0.9735\n",
            "Epoch 408/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0913 - accuracy: 0.9793 - val_loss: 0.1035 - val_accuracy: 0.9738\n",
            "Epoch 409/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0912 - accuracy: 0.9793 - val_loss: 0.0977 - val_accuracy: 0.9763\n",
            "Epoch 410/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0883 - accuracy: 0.9802 - val_loss: 0.0997 - val_accuracy: 0.9761\n",
            "Epoch 411/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0881 - accuracy: 0.9803 - val_loss: 0.0986 - val_accuracy: 0.9757\n",
            "Epoch 412/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0894 - accuracy: 0.9797 - val_loss: 0.1057 - val_accuracy: 0.9728\n",
            "Epoch 413/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0923 - accuracy: 0.9786 - val_loss: 0.1024 - val_accuracy: 0.9738\n",
            "Epoch 414/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0915 - accuracy: 0.9787 - val_loss: 0.0978 - val_accuracy: 0.9759\n",
            "Epoch 415/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0882 - accuracy: 0.9798 - val_loss: 0.0979 - val_accuracy: 0.9766\n",
            "Epoch 416/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0889 - accuracy: 0.9801 - val_loss: 0.0997 - val_accuracy: 0.9753\n",
            "Epoch 417/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0941 - accuracy: 0.9773 - val_loss: 0.0995 - val_accuracy: 0.9756\n",
            "Epoch 418/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0874 - accuracy: 0.9805 - val_loss: 0.0957 - val_accuracy: 0.9763\n",
            "Epoch 419/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0861 - accuracy: 0.9811 - val_loss: 0.0953 - val_accuracy: 0.9768\n",
            "Epoch 420/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0847 - accuracy: 0.9820 - val_loss: 0.0945 - val_accuracy: 0.9774\n",
            "Epoch 421/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0865 - accuracy: 0.9812 - val_loss: 0.0970 - val_accuracy: 0.9760\n",
            "Epoch 422/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0877 - accuracy: 0.9802 - val_loss: 0.1086 - val_accuracy: 0.9722\n",
            "Epoch 423/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0944 - accuracy: 0.9772 - val_loss: 0.1001 - val_accuracy: 0.9752\n",
            "Epoch 424/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0886 - accuracy: 0.9802 - val_loss: 0.1037 - val_accuracy: 0.9732\n",
            "Epoch 425/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0901 - accuracy: 0.9792 - val_loss: 0.1079 - val_accuracy: 0.9713\n",
            "Epoch 426/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0910 - accuracy: 0.9790 - val_loss: 0.0971 - val_accuracy: 0.9761\n",
            "Epoch 427/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0850 - accuracy: 0.9815 - val_loss: 0.0994 - val_accuracy: 0.9749\n",
            "Epoch 428/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0907 - accuracy: 0.9787 - val_loss: 0.1108 - val_accuracy: 0.9701\n",
            "Epoch 429/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1024 - accuracy: 0.9739 - val_loss: 0.1145 - val_accuracy: 0.9687\n",
            "Epoch 430/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0983 - accuracy: 0.9756 - val_loss: 0.0982 - val_accuracy: 0.9763\n",
            "Epoch 431/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0872 - accuracy: 0.9807 - val_loss: 0.0989 - val_accuracy: 0.9749\n",
            "Epoch 432/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0880 - accuracy: 0.9800 - val_loss: 0.0947 - val_accuracy: 0.9776\n",
            "Epoch 433/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0830 - accuracy: 0.9824 - val_loss: 0.0943 - val_accuracy: 0.9770\n",
            "Epoch 434/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0828 - accuracy: 0.9826 - val_loss: 0.0964 - val_accuracy: 0.9758\n",
            "Epoch 435/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0841 - accuracy: 0.9812 - val_loss: 0.0935 - val_accuracy: 0.9773\n",
            "Epoch 436/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0858 - accuracy: 0.9810 - val_loss: 0.0934 - val_accuracy: 0.9776\n",
            "Epoch 437/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0844 - accuracy: 0.9817 - val_loss: 0.1027 - val_accuracy: 0.9736\n",
            "Epoch 438/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1047 - accuracy: 0.9728 - val_loss: 0.1198 - val_accuracy: 0.9668\n",
            "Epoch 439/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1069 - accuracy: 0.9717 - val_loss: 0.1063 - val_accuracy: 0.9720\n",
            "Epoch 440/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0951 - accuracy: 0.9777 - val_loss: 0.1013 - val_accuracy: 0.9739\n",
            "Epoch 441/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0882 - accuracy: 0.9797 - val_loss: 0.0933 - val_accuracy: 0.9778\n",
            "Epoch 442/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0840 - accuracy: 0.9823 - val_loss: 0.0944 - val_accuracy: 0.9771\n",
            "Epoch 443/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0852 - accuracy: 0.9812 - val_loss: 0.0968 - val_accuracy: 0.9760\n",
            "Epoch 444/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0845 - accuracy: 0.9815 - val_loss: 0.0941 - val_accuracy: 0.9775\n",
            "Epoch 445/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0834 - accuracy: 0.9818 - val_loss: 0.0944 - val_accuracy: 0.9768\n",
            "Epoch 446/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0846 - accuracy: 0.9814 - val_loss: 0.0923 - val_accuracy: 0.9780\n",
            "Epoch 447/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0816 - accuracy: 0.9828 - val_loss: 0.0924 - val_accuracy: 0.9779\n",
            "Epoch 448/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0825 - accuracy: 0.9818 - val_loss: 0.1000 - val_accuracy: 0.9749\n",
            "Epoch 449/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0856 - accuracy: 0.9808 - val_loss: 0.0958 - val_accuracy: 0.9765\n",
            "Epoch 450/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0815 - accuracy: 0.9826 - val_loss: 0.0941 - val_accuracy: 0.9770\n",
            "Epoch 451/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0819 - accuracy: 0.9823 - val_loss: 0.0970 - val_accuracy: 0.9757\n",
            "Epoch 452/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0825 - accuracy: 0.9824 - val_loss: 0.0899 - val_accuracy: 0.9786\n",
            "Epoch 453/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0826 - accuracy: 0.9819 - val_loss: 0.0963 - val_accuracy: 0.9760\n",
            "Epoch 454/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0845 - accuracy: 0.9811 - val_loss: 0.0976 - val_accuracy: 0.9753\n",
            "Epoch 455/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0904 - accuracy: 0.9786 - val_loss: 0.0981 - val_accuracy: 0.9756\n",
            "Epoch 456/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0867 - accuracy: 0.9800 - val_loss: 0.0926 - val_accuracy: 0.9780\n",
            "Epoch 457/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0816 - accuracy: 0.9828 - val_loss: 0.0926 - val_accuracy: 0.9776\n",
            "Epoch 458/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0815 - accuracy: 0.9823 - val_loss: 0.0897 - val_accuracy: 0.9790\n",
            "Epoch 459/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0787 - accuracy: 0.9836 - val_loss: 0.0931 - val_accuracy: 0.9772\n",
            "Epoch 460/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0827 - accuracy: 0.9820 - val_loss: 0.0910 - val_accuracy: 0.9779\n",
            "Epoch 461/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0807 - accuracy: 0.9827 - val_loss: 0.1014 - val_accuracy: 0.9732\n",
            "Epoch 462/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0883 - accuracy: 0.9794 - val_loss: 0.0987 - val_accuracy: 0.9751\n",
            "Epoch 463/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0872 - accuracy: 0.9798 - val_loss: 0.1232 - val_accuracy: 0.9643\n",
            "Epoch 464/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1038 - accuracy: 0.9731 - val_loss: 0.1064 - val_accuracy: 0.9732\n",
            "Epoch 465/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0924 - accuracy: 0.9778 - val_loss: 0.1020 - val_accuracy: 0.9732\n",
            "Epoch 466/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0859 - accuracy: 0.9800 - val_loss: 0.0890 - val_accuracy: 0.9790\n",
            "Epoch 467/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0794 - accuracy: 0.9832 - val_loss: 0.0913 - val_accuracy: 0.9783\n",
            "Epoch 468/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0796 - accuracy: 0.9830 - val_loss: 0.0922 - val_accuracy: 0.9774\n",
            "Epoch 469/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0995 - accuracy: 0.9754 - val_loss: 0.1229 - val_accuracy: 0.9658\n",
            "Epoch 470/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1215 - accuracy: 0.9662 - val_loss: 0.1039 - val_accuracy: 0.9739\n",
            "Epoch 471/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0939 - accuracy: 0.9772 - val_loss: 0.0964 - val_accuracy: 0.9763\n",
            "Epoch 472/1000\n",
            "11/11 [==============================] - 1s 74ms/step - loss: 0.0845 - accuracy: 0.9811 - val_loss: 0.0930 - val_accuracy: 0.9774\n",
            "Epoch 473/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0818 - accuracy: 0.9822 - val_loss: 0.0995 - val_accuracy: 0.9741\n",
            "Epoch 474/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0867 - accuracy: 0.9795 - val_loss: 0.0896 - val_accuracy: 0.9790\n",
            "Epoch 475/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0798 - accuracy: 0.9831 - val_loss: 0.0902 - val_accuracy: 0.9785\n",
            "Epoch 476/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0810 - accuracy: 0.9824 - val_loss: 0.0878 - val_accuracy: 0.9796\n",
            "Epoch 477/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0779 - accuracy: 0.9840 - val_loss: 0.0885 - val_accuracy: 0.9791\n",
            "Epoch 478/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0768 - accuracy: 0.9838 - val_loss: 0.0875 - val_accuracy: 0.9795\n",
            "Epoch 479/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0776 - accuracy: 0.9837 - val_loss: 0.0937 - val_accuracy: 0.9769\n",
            "Epoch 480/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0836 - accuracy: 0.9810 - val_loss: 0.1059 - val_accuracy: 0.9711\n",
            "Epoch 481/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0938 - accuracy: 0.9769 - val_loss: 0.0976 - val_accuracy: 0.9757\n",
            "Epoch 482/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0861 - accuracy: 0.9803 - val_loss: 0.0941 - val_accuracy: 0.9764\n",
            "Epoch 483/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0818 - accuracy: 0.9820 - val_loss: 0.0932 - val_accuracy: 0.9774\n",
            "Epoch 484/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0809 - accuracy: 0.9821 - val_loss: 0.0938 - val_accuracy: 0.9764\n",
            "Epoch 485/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0800 - accuracy: 0.9820 - val_loss: 0.0863 - val_accuracy: 0.9802\n",
            "Epoch 486/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0779 - accuracy: 0.9837 - val_loss: 0.0897 - val_accuracy: 0.9792\n",
            "Epoch 487/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0775 - accuracy: 0.9837 - val_loss: 0.0865 - val_accuracy: 0.9803\n",
            "Epoch 488/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0756 - accuracy: 0.9845 - val_loss: 0.0911 - val_accuracy: 0.9777\n",
            "Epoch 489/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0787 - accuracy: 0.9828 - val_loss: 0.0889 - val_accuracy: 0.9789\n",
            "Epoch 490/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0797 - accuracy: 0.9826 - val_loss: 0.0970 - val_accuracy: 0.9753\n",
            "Epoch 491/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0807 - accuracy: 0.9823 - val_loss: 0.0880 - val_accuracy: 0.9794\n",
            "Epoch 492/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0763 - accuracy: 0.9841 - val_loss: 0.0873 - val_accuracy: 0.9798\n",
            "Epoch 493/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0766 - accuracy: 0.9844 - val_loss: 0.0853 - val_accuracy: 0.9799\n",
            "Epoch 494/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0811 - accuracy: 0.9818 - val_loss: 0.1116 - val_accuracy: 0.9698\n",
            "Epoch 495/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0980 - accuracy: 0.9749 - val_loss: 0.1009 - val_accuracy: 0.9735\n",
            "Epoch 496/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0850 - accuracy: 0.9807 - val_loss: 0.0896 - val_accuracy: 0.9789\n",
            "Epoch 497/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0772 - accuracy: 0.9837 - val_loss: 0.0864 - val_accuracy: 0.9801\n",
            "Epoch 498/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0759 - accuracy: 0.9844 - val_loss: 0.0882 - val_accuracy: 0.9790\n",
            "Epoch 499/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0754 - accuracy: 0.9844 - val_loss: 0.0870 - val_accuracy: 0.9796\n",
            "Epoch 500/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0752 - accuracy: 0.9845 - val_loss: 0.0891 - val_accuracy: 0.9783\n",
            "Epoch 501/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0762 - accuracy: 0.9838 - val_loss: 0.0888 - val_accuracy: 0.9786\n",
            "Epoch 502/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0795 - accuracy: 0.9825 - val_loss: 0.0873 - val_accuracy: 0.9798\n",
            "Epoch 503/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0780 - accuracy: 0.9835 - val_loss: 0.0964 - val_accuracy: 0.9757\n",
            "Epoch 504/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0866 - accuracy: 0.9796 - val_loss: 0.1010 - val_accuracy: 0.9736\n",
            "Epoch 505/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0930 - accuracy: 0.9767 - val_loss: 0.1114 - val_accuracy: 0.9689\n",
            "Epoch 506/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0955 - accuracy: 0.9759 - val_loss: 0.1014 - val_accuracy: 0.9742\n",
            "Epoch 507/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0852 - accuracy: 0.9806 - val_loss: 0.0871 - val_accuracy: 0.9804\n",
            "Epoch 508/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0759 - accuracy: 0.9842 - val_loss: 0.0873 - val_accuracy: 0.9787\n",
            "Epoch 509/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0749 - accuracy: 0.9844 - val_loss: 0.0881 - val_accuracy: 0.9786\n",
            "Epoch 510/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0746 - accuracy: 0.9844 - val_loss: 0.0823 - val_accuracy: 0.9817\n",
            "Epoch 511/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0720 - accuracy: 0.9859 - val_loss: 0.0827 - val_accuracy: 0.9812\n",
            "Epoch 512/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0720 - accuracy: 0.9855 - val_loss: 0.0876 - val_accuracy: 0.9793\n",
            "Epoch 513/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0764 - accuracy: 0.9834 - val_loss: 0.0873 - val_accuracy: 0.9793\n",
            "Epoch 514/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0782 - accuracy: 0.9827 - val_loss: 0.0867 - val_accuracy: 0.9797\n",
            "Epoch 515/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0767 - accuracy: 0.9839 - val_loss: 0.0867 - val_accuracy: 0.9795\n",
            "Epoch 516/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0770 - accuracy: 0.9835 - val_loss: 0.0863 - val_accuracy: 0.9808\n",
            "Epoch 517/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0762 - accuracy: 0.9843 - val_loss: 0.0862 - val_accuracy: 0.9799\n",
            "Epoch 518/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0742 - accuracy: 0.9848 - val_loss: 0.0895 - val_accuracy: 0.9782\n",
            "Epoch 519/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0765 - accuracy: 0.9837 - val_loss: 0.0838 - val_accuracy: 0.9809\n",
            "Epoch 520/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0727 - accuracy: 0.9850 - val_loss: 0.0854 - val_accuracy: 0.9797\n",
            "Epoch 521/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0728 - accuracy: 0.9849 - val_loss: 0.0877 - val_accuracy: 0.9784\n",
            "Epoch 522/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0734 - accuracy: 0.9850 - val_loss: 0.0832 - val_accuracy: 0.9808\n",
            "Epoch 523/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0721 - accuracy: 0.9857 - val_loss: 0.0891 - val_accuracy: 0.9779\n",
            "Epoch 524/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0754 - accuracy: 0.9844 - val_loss: 0.0856 - val_accuracy: 0.9799\n",
            "Epoch 525/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0741 - accuracy: 0.9847 - val_loss: 0.0867 - val_accuracy: 0.9790\n",
            "Epoch 526/1000\n",
            "11/11 [==============================] - 1s 84ms/step - loss: 0.0738 - accuracy: 0.9850 - val_loss: 0.0873 - val_accuracy: 0.9792\n",
            "Epoch 527/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0762 - accuracy: 0.9837 - val_loss: 0.0838 - val_accuracy: 0.9807\n",
            "Epoch 528/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0706 - accuracy: 0.9865 - val_loss: 0.0819 - val_accuracy: 0.9819\n",
            "Epoch 529/1000\n",
            "11/11 [==============================] - 1s 85ms/step - loss: 0.0702 - accuracy: 0.9861 - val_loss: 0.0810 - val_accuracy: 0.9816\n",
            "Epoch 530/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0703 - accuracy: 0.9862 - val_loss: 0.0832 - val_accuracy: 0.9809\n",
            "Epoch 531/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0713 - accuracy: 0.9858 - val_loss: 0.0836 - val_accuracy: 0.9805\n",
            "Epoch 532/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0708 - accuracy: 0.9854 - val_loss: 0.0797 - val_accuracy: 0.9828\n",
            "Epoch 533/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0683 - accuracy: 0.9869 - val_loss: 0.0876 - val_accuracy: 0.9788\n",
            "Epoch 534/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0766 - accuracy: 0.9834 - val_loss: 0.0864 - val_accuracy: 0.9792\n",
            "Epoch 535/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1777 - accuracy: 0.9534 - val_loss: 0.3837 - val_accuracy: 0.8778\n",
            "Epoch 536/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.2738 - accuracy: 0.9089 - val_loss: 0.1748 - val_accuracy: 0.9452\n",
            "Epoch 537/1000\n",
            "11/11 [==============================] - 1s 74ms/step - loss: 0.1582 - accuracy: 0.9516 - val_loss: 0.1246 - val_accuracy: 0.9648\n",
            "Epoch 538/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1082 - accuracy: 0.9713 - val_loss: 0.0988 - val_accuracy: 0.9760\n",
            "Epoch 539/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0869 - accuracy: 0.9801 - val_loss: 0.0867 - val_accuracy: 0.9803\n",
            "Epoch 540/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0776 - accuracy: 0.9836 - val_loss: 0.0826 - val_accuracy: 0.9812\n",
            "Epoch 541/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0734 - accuracy: 0.9854 - val_loss: 0.0819 - val_accuracy: 0.9819\n",
            "Epoch 542/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0712 - accuracy: 0.9862 - val_loss: 0.0809 - val_accuracy: 0.9819\n",
            "Epoch 543/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0711 - accuracy: 0.9860 - val_loss: 0.0794 - val_accuracy: 0.9827\n",
            "Epoch 544/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0706 - accuracy: 0.9864 - val_loss: 0.0805 - val_accuracy: 0.9815\n",
            "Epoch 545/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0703 - accuracy: 0.9860 - val_loss: 0.0814 - val_accuracy: 0.9815\n",
            "Epoch 546/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0704 - accuracy: 0.9862 - val_loss: 0.0827 - val_accuracy: 0.9808\n",
            "Epoch 547/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0743 - accuracy: 0.9842 - val_loss: 0.0912 - val_accuracy: 0.9766\n",
            "Epoch 548/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0766 - accuracy: 0.9834 - val_loss: 0.0823 - val_accuracy: 0.9814\n",
            "Epoch 549/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0715 - accuracy: 0.9859 - val_loss: 0.0841 - val_accuracy: 0.9796\n",
            "Epoch 550/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0701 - accuracy: 0.9859 - val_loss: 0.0789 - val_accuracy: 0.9828\n",
            "Epoch 551/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0686 - accuracy: 0.9871 - val_loss: 0.0829 - val_accuracy: 0.9804\n",
            "Epoch 552/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0726 - accuracy: 0.9847 - val_loss: 0.0852 - val_accuracy: 0.9796\n",
            "Epoch 553/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0728 - accuracy: 0.9851 - val_loss: 0.0799 - val_accuracy: 0.9817\n",
            "Epoch 554/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0685 - accuracy: 0.9866 - val_loss: 0.0789 - val_accuracy: 0.9827\n",
            "Epoch 555/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0683 - accuracy: 0.9867 - val_loss: 0.0783 - val_accuracy: 0.9826\n",
            "Epoch 556/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0679 - accuracy: 0.9867 - val_loss: 0.0788 - val_accuracy: 0.9823\n",
            "Epoch 557/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0674 - accuracy: 0.9869 - val_loss: 0.0849 - val_accuracy: 0.9800\n",
            "Epoch 558/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0720 - accuracy: 0.9853 - val_loss: 0.0813 - val_accuracy: 0.9816\n",
            "Epoch 559/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0715 - accuracy: 0.9855 - val_loss: 0.0805 - val_accuracy: 0.9816\n",
            "Epoch 560/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0690 - accuracy: 0.9861 - val_loss: 0.0831 - val_accuracy: 0.9805\n",
            "Epoch 561/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0735 - accuracy: 0.9842 - val_loss: 0.0880 - val_accuracy: 0.9783\n",
            "Epoch 562/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0721 - accuracy: 0.9854 - val_loss: 0.0810 - val_accuracy: 0.9815\n",
            "Epoch 563/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0677 - accuracy: 0.9869 - val_loss: 0.0790 - val_accuracy: 0.9821\n",
            "Epoch 564/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0688 - accuracy: 0.9864 - val_loss: 0.0848 - val_accuracy: 0.9797\n",
            "Epoch 565/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0709 - accuracy: 0.9856 - val_loss: 0.0775 - val_accuracy: 0.9832\n",
            "Epoch 566/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0669 - accuracy: 0.9871 - val_loss: 0.0771 - val_accuracy: 0.9827\n",
            "Epoch 567/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0668 - accuracy: 0.9874 - val_loss: 0.0814 - val_accuracy: 0.9809\n",
            "Epoch 568/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0685 - accuracy: 0.9863 - val_loss: 0.0789 - val_accuracy: 0.9821\n",
            "Epoch 569/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0673 - accuracy: 0.9865 - val_loss: 0.0776 - val_accuracy: 0.9827\n",
            "Epoch 570/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0693 - accuracy: 0.9858 - val_loss: 0.0878 - val_accuracy: 0.9787\n",
            "Epoch 571/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0736 - accuracy: 0.9842 - val_loss: 0.0803 - val_accuracy: 0.9815\n",
            "Epoch 572/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0687 - accuracy: 0.9866 - val_loss: 0.0769 - val_accuracy: 0.9828\n",
            "Epoch 573/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0685 - accuracy: 0.9863 - val_loss: 0.1228 - val_accuracy: 0.9653\n",
            "Epoch 574/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1606 - accuracy: 0.9542 - val_loss: 0.1612 - val_accuracy: 0.9535\n",
            "Epoch 575/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1297 - accuracy: 0.9635 - val_loss: 0.1111 - val_accuracy: 0.9695\n",
            "Epoch 576/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0914 - accuracy: 0.9778 - val_loss: 0.0855 - val_accuracy: 0.9795\n",
            "Epoch 577/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0757 - accuracy: 0.9837 - val_loss: 0.0821 - val_accuracy: 0.9808\n",
            "Epoch 578/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0733 - accuracy: 0.9845 - val_loss: 0.0794 - val_accuracy: 0.9825\n",
            "Epoch 579/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0681 - accuracy: 0.9867 - val_loss: 0.0760 - val_accuracy: 0.9831\n",
            "Epoch 580/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0651 - accuracy: 0.9878 - val_loss: 0.0752 - val_accuracy: 0.9839\n",
            "Epoch 581/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0648 - accuracy: 0.9879 - val_loss: 0.0754 - val_accuracy: 0.9835\n",
            "Epoch 582/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0650 - accuracy: 0.9881 - val_loss: 0.0775 - val_accuracy: 0.9823\n",
            "Epoch 583/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0641 - accuracy: 0.9879 - val_loss: 0.0750 - val_accuracy: 0.9839\n",
            "Epoch 584/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0644 - accuracy: 0.9882 - val_loss: 0.0753 - val_accuracy: 0.9839\n",
            "Epoch 585/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0637 - accuracy: 0.9887 - val_loss: 0.0848 - val_accuracy: 0.9790\n",
            "Epoch 586/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0706 - accuracy: 0.9852 - val_loss: 0.0775 - val_accuracy: 0.9824\n",
            "Epoch 587/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0681 - accuracy: 0.9864 - val_loss: 0.0811 - val_accuracy: 0.9813\n",
            "Epoch 588/1000\n",
            "11/11 [==============================] - 1s 83ms/step - loss: 0.0681 - accuracy: 0.9861 - val_loss: 0.0756 - val_accuracy: 0.9836\n",
            "Epoch 589/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0660 - accuracy: 0.9879 - val_loss: 0.0746 - val_accuracy: 0.9833\n",
            "Epoch 590/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0637 - accuracy: 0.9883 - val_loss: 0.0766 - val_accuracy: 0.9831\n",
            "Epoch 591/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0658 - accuracy: 0.9875 - val_loss: 0.0781 - val_accuracy: 0.9817\n",
            "Epoch 592/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0673 - accuracy: 0.9868 - val_loss: 0.0819 - val_accuracy: 0.9801\n",
            "Epoch 593/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0696 - accuracy: 0.9856 - val_loss: 0.0841 - val_accuracy: 0.9800\n",
            "Epoch 594/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0703 - accuracy: 0.9854 - val_loss: 0.0822 - val_accuracy: 0.9806\n",
            "Epoch 595/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0777 - accuracy: 0.9815 - val_loss: 0.0881 - val_accuracy: 0.9778\n",
            "Epoch 596/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0714 - accuracy: 0.9852 - val_loss: 0.0746 - val_accuracy: 0.9840\n",
            "Epoch 597/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0669 - accuracy: 0.9871 - val_loss: 0.0855 - val_accuracy: 0.9790\n",
            "Epoch 598/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0702 - accuracy: 0.9852 - val_loss: 0.0784 - val_accuracy: 0.9825\n",
            "Epoch 599/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0666 - accuracy: 0.9873 - val_loss: 0.0742 - val_accuracy: 0.9838\n",
            "Epoch 600/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0638 - accuracy: 0.9882 - val_loss: 0.0724 - val_accuracy: 0.9844\n",
            "Epoch 601/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0650 - accuracy: 0.9878 - val_loss: 0.0740 - val_accuracy: 0.9837\n",
            "Epoch 602/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0662 - accuracy: 0.9873 - val_loss: 0.0790 - val_accuracy: 0.9820\n",
            "Epoch 603/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0657 - accuracy: 0.9872 - val_loss: 0.0760 - val_accuracy: 0.9826\n",
            "Epoch 604/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0637 - accuracy: 0.9885 - val_loss: 0.0831 - val_accuracy: 0.9797\n",
            "Epoch 605/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0693 - accuracy: 0.9856 - val_loss: 0.0794 - val_accuracy: 0.9815\n",
            "Epoch 606/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0648 - accuracy: 0.9879 - val_loss: 0.0751 - val_accuracy: 0.9836\n",
            "Epoch 607/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0627 - accuracy: 0.9887 - val_loss: 0.0741 - val_accuracy: 0.9840\n",
            "Epoch 608/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0626 - accuracy: 0.9886 - val_loss: 0.0704 - val_accuracy: 0.9853\n",
            "Epoch 609/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0615 - accuracy: 0.9892 - val_loss: 0.0725 - val_accuracy: 0.9844\n",
            "Epoch 610/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0624 - accuracy: 0.9888 - val_loss: 0.0743 - val_accuracy: 0.9835\n",
            "Epoch 611/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0619 - accuracy: 0.9890 - val_loss: 0.0771 - val_accuracy: 0.9822\n",
            "Epoch 612/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0685 - accuracy: 0.9857 - val_loss: 0.0768 - val_accuracy: 0.9825\n",
            "Epoch 613/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0660 - accuracy: 0.9868 - val_loss: 0.0719 - val_accuracy: 0.9849\n",
            "Epoch 614/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0616 - accuracy: 0.9888 - val_loss: 0.0748 - val_accuracy: 0.9834\n",
            "Epoch 615/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0617 - accuracy: 0.9890 - val_loss: 0.0833 - val_accuracy: 0.9797\n",
            "Epoch 616/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0732 - accuracy: 0.9840 - val_loss: 0.0862 - val_accuracy: 0.9787\n",
            "Epoch 617/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0728 - accuracy: 0.9841 - val_loss: 0.0790 - val_accuracy: 0.9813\n",
            "Epoch 618/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0697 - accuracy: 0.9853 - val_loss: 0.0882 - val_accuracy: 0.9774\n",
            "Epoch 619/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0705 - accuracy: 0.9852 - val_loss: 0.0778 - val_accuracy: 0.9817\n",
            "Epoch 620/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0691 - accuracy: 0.9857 - val_loss: 0.0870 - val_accuracy: 0.9785\n",
            "Epoch 621/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0784 - accuracy: 0.9816 - val_loss: 0.0800 - val_accuracy: 0.9816\n",
            "Epoch 622/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0684 - accuracy: 0.9864 - val_loss: 0.0743 - val_accuracy: 0.9831\n",
            "Epoch 623/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0663 - accuracy: 0.9870 - val_loss: 0.0708 - val_accuracy: 0.9855\n",
            "Epoch 624/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0603 - accuracy: 0.9894 - val_loss: 0.0767 - val_accuracy: 0.9830\n",
            "Epoch 625/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0625 - accuracy: 0.9887 - val_loss: 0.0729 - val_accuracy: 0.9844\n",
            "Epoch 626/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0611 - accuracy: 0.9892 - val_loss: 0.0776 - val_accuracy: 0.9825\n",
            "Epoch 627/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0629 - accuracy: 0.9880 - val_loss: 0.0718 - val_accuracy: 0.9844\n",
            "Epoch 628/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0603 - accuracy: 0.9895 - val_loss: 0.0697 - val_accuracy: 0.9855\n",
            "Epoch 629/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0606 - accuracy: 0.9892 - val_loss: 0.0700 - val_accuracy: 0.9854\n",
            "Epoch 630/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0606 - accuracy: 0.9895 - val_loss: 0.0766 - val_accuracy: 0.9828\n",
            "Epoch 631/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0712 - accuracy: 0.9843 - val_loss: 0.0996 - val_accuracy: 0.9742\n",
            "Epoch 632/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0811 - accuracy: 0.9807 - val_loss: 0.0852 - val_accuracy: 0.9785\n",
            "Epoch 633/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0730 - accuracy: 0.9842 - val_loss: 0.0816 - val_accuracy: 0.9802\n",
            "Epoch 634/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0704 - accuracy: 0.9854 - val_loss: 0.0830 - val_accuracy: 0.9798\n",
            "Epoch 635/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0729 - accuracy: 0.9839 - val_loss: 0.0819 - val_accuracy: 0.9806\n",
            "Epoch 636/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0666 - accuracy: 0.9869 - val_loss: 0.0729 - val_accuracy: 0.9840\n",
            "Epoch 637/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0621 - accuracy: 0.9886 - val_loss: 0.0719 - val_accuracy: 0.9848\n",
            "Epoch 638/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0644 - accuracy: 0.9875 - val_loss: 0.0725 - val_accuracy: 0.9844\n",
            "Epoch 639/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0618 - accuracy: 0.9887 - val_loss: 0.0723 - val_accuracy: 0.9838\n",
            "Epoch 640/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0618 - accuracy: 0.9889 - val_loss: 0.0687 - val_accuracy: 0.9859\n",
            "Epoch 641/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0590 - accuracy: 0.9896 - val_loss: 0.0735 - val_accuracy: 0.9834\n",
            "Epoch 642/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0618 - accuracy: 0.9884 - val_loss: 0.0676 - val_accuracy: 0.9866\n",
            "Epoch 643/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0606 - accuracy: 0.9894 - val_loss: 0.0691 - val_accuracy: 0.9853\n",
            "Epoch 644/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0638 - accuracy: 0.9879 - val_loss: 0.0963 - val_accuracy: 0.9741\n",
            "Epoch 645/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0808 - accuracy: 0.9805 - val_loss: 0.0761 - val_accuracy: 0.9834\n",
            "Epoch 646/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0674 - accuracy: 0.9865 - val_loss: 0.0781 - val_accuracy: 0.9823\n",
            "Epoch 647/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0643 - accuracy: 0.9877 - val_loss: 0.0748 - val_accuracy: 0.9828\n",
            "Epoch 648/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0615 - accuracy: 0.9888 - val_loss: 0.0788 - val_accuracy: 0.9815\n",
            "Epoch 649/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0670 - accuracy: 0.9866 - val_loss: 0.0745 - val_accuracy: 0.9834\n",
            "Epoch 650/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0635 - accuracy: 0.9874 - val_loss: 0.0748 - val_accuracy: 0.9833\n",
            "Epoch 651/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0720 - accuracy: 0.9847 - val_loss: 0.0822 - val_accuracy: 0.9812\n",
            "Epoch 652/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0733 - accuracy: 0.9835 - val_loss: 0.0832 - val_accuracy: 0.9797\n",
            "Epoch 653/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0726 - accuracy: 0.9840 - val_loss: 0.0769 - val_accuracy: 0.9825\n",
            "Epoch 654/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0637 - accuracy: 0.9879 - val_loss: 0.0712 - val_accuracy: 0.9839\n",
            "Epoch 655/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0601 - accuracy: 0.9894 - val_loss: 0.0678 - val_accuracy: 0.9860\n",
            "Epoch 656/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0580 - accuracy: 0.9902 - val_loss: 0.0683 - val_accuracy: 0.9857\n",
            "Epoch 657/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0593 - accuracy: 0.9897 - val_loss: 0.0732 - val_accuracy: 0.9835\n",
            "Epoch 658/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0630 - accuracy: 0.9880 - val_loss: 0.0765 - val_accuracy: 0.9830\n",
            "Epoch 659/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0607 - accuracy: 0.9891 - val_loss: 0.0702 - val_accuracy: 0.9853\n",
            "Epoch 660/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0607 - accuracy: 0.9885 - val_loss: 0.0707 - val_accuracy: 0.9848\n",
            "Epoch 661/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0601 - accuracy: 0.9893 - val_loss: 0.0692 - val_accuracy: 0.9851\n",
            "Epoch 662/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0592 - accuracy: 0.9894 - val_loss: 0.0749 - val_accuracy: 0.9830\n",
            "Epoch 663/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0628 - accuracy: 0.9874 - val_loss: 0.0702 - val_accuracy: 0.9845\n",
            "Epoch 664/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0608 - accuracy: 0.9889 - val_loss: 0.0741 - val_accuracy: 0.9834\n",
            "Epoch 665/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0581 - accuracy: 0.9901 - val_loss: 0.0667 - val_accuracy: 0.9864\n",
            "Epoch 666/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0566 - accuracy: 0.9904 - val_loss: 0.0683 - val_accuracy: 0.9858\n",
            "Epoch 667/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0674 - accuracy: 0.9866 - val_loss: 0.2706 - val_accuracy: 0.9253\n",
            "Epoch 668/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2138 - accuracy: 0.9395 - val_loss: 0.1640 - val_accuracy: 0.9486\n",
            "Epoch 669/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1374 - accuracy: 0.9600 - val_loss: 0.1125 - val_accuracy: 0.9701\n",
            "Epoch 670/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0932 - accuracy: 0.9762 - val_loss: 0.0848 - val_accuracy: 0.9792\n",
            "Epoch 671/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0715 - accuracy: 0.9855 - val_loss: 0.0741 - val_accuracy: 0.9838\n",
            "Epoch 672/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0641 - accuracy: 0.9879 - val_loss: 0.0694 - val_accuracy: 0.9856\n",
            "Epoch 673/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0596 - accuracy: 0.9899 - val_loss: 0.0677 - val_accuracy: 0.9861\n",
            "Epoch 674/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0587 - accuracy: 0.9900 - val_loss: 0.0667 - val_accuracy: 0.9862\n",
            "Epoch 675/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0579 - accuracy: 0.9901 - val_loss: 0.0668 - val_accuracy: 0.9860\n",
            "Epoch 676/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0565 - accuracy: 0.9904 - val_loss: 0.0662 - val_accuracy: 0.9868\n",
            "Epoch 677/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0555 - accuracy: 0.9910 - val_loss: 0.0656 - val_accuracy: 0.9867\n",
            "Epoch 678/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0549 - accuracy: 0.9912 - val_loss: 0.0655 - val_accuracy: 0.9870\n",
            "Epoch 679/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0557 - accuracy: 0.9909 - val_loss: 0.0674 - val_accuracy: 0.9862\n",
            "Epoch 680/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0570 - accuracy: 0.9902 - val_loss: 0.0665 - val_accuracy: 0.9861\n",
            "Epoch 681/1000\n",
            "11/11 [==============================] - 1s 81ms/step - loss: 0.0569 - accuracy: 0.9901 - val_loss: 0.0653 - val_accuracy: 0.9870\n",
            "Epoch 682/1000\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0556 - accuracy: 0.9911 - val_loss: 0.0658 - val_accuracy: 0.9865\n",
            "Epoch 683/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0559 - accuracy: 0.9904 - val_loss: 0.0654 - val_accuracy: 0.9867\n",
            "Epoch 684/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0564 - accuracy: 0.9905 - val_loss: 0.0755 - val_accuracy: 0.9823\n",
            "Epoch 685/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0638 - accuracy: 0.9875 - val_loss: 0.0698 - val_accuracy: 0.9853\n",
            "Epoch 686/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0603 - accuracy: 0.9894 - val_loss: 0.0667 - val_accuracy: 0.9863\n",
            "Epoch 687/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0548 - accuracy: 0.9914 - val_loss: 0.0643 - val_accuracy: 0.9871\n",
            "Epoch 688/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0556 - accuracy: 0.9907 - val_loss: 0.0717 - val_accuracy: 0.9838\n",
            "Epoch 689/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0594 - accuracy: 0.9888 - val_loss: 0.0729 - val_accuracy: 0.9834\n",
            "Epoch 690/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0593 - accuracy: 0.9895 - val_loss: 0.0684 - val_accuracy: 0.9861\n",
            "Epoch 691/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0587 - accuracy: 0.9894 - val_loss: 0.0661 - val_accuracy: 0.9864\n",
            "Epoch 692/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0552 - accuracy: 0.9910 - val_loss: 0.0651 - val_accuracy: 0.9866\n",
            "Epoch 693/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0557 - accuracy: 0.9905 - val_loss: 0.0643 - val_accuracy: 0.9869\n",
            "Epoch 694/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0537 - accuracy: 0.9918 - val_loss: 0.0626 - val_accuracy: 0.9878\n",
            "Epoch 695/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0536 - accuracy: 0.9917 - val_loss: 0.0687 - val_accuracy: 0.9854\n",
            "Epoch 696/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0551 - accuracy: 0.9908 - val_loss: 0.0672 - val_accuracy: 0.9861\n",
            "Epoch 697/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0578 - accuracy: 0.9898 - val_loss: 0.0709 - val_accuracy: 0.9839\n",
            "Epoch 698/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0602 - accuracy: 0.9886 - val_loss: 0.0675 - val_accuracy: 0.9851\n",
            "Epoch 699/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0628 - accuracy: 0.9877 - val_loss: 0.0713 - val_accuracy: 0.9844\n",
            "Epoch 700/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0596 - accuracy: 0.9893 - val_loss: 0.0655 - val_accuracy: 0.9869\n",
            "Epoch 701/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0558 - accuracy: 0.9907 - val_loss: 0.0649 - val_accuracy: 0.9870\n",
            "Epoch 702/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0560 - accuracy: 0.9906 - val_loss: 0.0637 - val_accuracy: 0.9872\n",
            "Epoch 703/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0540 - accuracy: 0.9915 - val_loss: 0.0641 - val_accuracy: 0.9874\n",
            "Epoch 704/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0539 - accuracy: 0.9914 - val_loss: 0.0622 - val_accuracy: 0.9882\n",
            "Epoch 705/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0531 - accuracy: 0.9914 - val_loss: 0.0620 - val_accuracy: 0.9886\n",
            "Epoch 706/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0526 - accuracy: 0.9919 - val_loss: 0.0675 - val_accuracy: 0.9855\n",
            "Epoch 707/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0578 - accuracy: 0.9890 - val_loss: 0.0753 - val_accuracy: 0.9832\n",
            "Epoch 708/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0579 - accuracy: 0.9895 - val_loss: 0.0701 - val_accuracy: 0.9849\n",
            "Epoch 709/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0576 - accuracy: 0.9894 - val_loss: 0.0642 - val_accuracy: 0.9867\n",
            "Epoch 710/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0553 - accuracy: 0.9903 - val_loss: 0.0730 - val_accuracy: 0.9825\n",
            "Epoch 711/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0622 - accuracy: 0.9875 - val_loss: 0.0705 - val_accuracy: 0.9848\n",
            "Epoch 712/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0584 - accuracy: 0.9896 - val_loss: 0.0682 - val_accuracy: 0.9860\n",
            "Epoch 713/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0575 - accuracy: 0.9899 - val_loss: 0.0738 - val_accuracy: 0.9834\n",
            "Epoch 714/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0627 - accuracy: 0.9876 - val_loss: 0.0762 - val_accuracy: 0.9818\n",
            "Epoch 715/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0701 - accuracy: 0.9846 - val_loss: 0.0721 - val_accuracy: 0.9838\n",
            "Epoch 716/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0621 - accuracy: 0.9881 - val_loss: 0.0700 - val_accuracy: 0.9849\n",
            "Epoch 717/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0587 - accuracy: 0.9895 - val_loss: 0.0672 - val_accuracy: 0.9857\n",
            "Epoch 718/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0553 - accuracy: 0.9909 - val_loss: 0.0650 - val_accuracy: 0.9872\n",
            "Epoch 719/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0533 - accuracy: 0.9918 - val_loss: 0.0619 - val_accuracy: 0.9877\n",
            "Epoch 720/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0529 - accuracy: 0.9915 - val_loss: 0.0658 - val_accuracy: 0.9862\n",
            "Epoch 721/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0545 - accuracy: 0.9908 - val_loss: 0.0677 - val_accuracy: 0.9854\n",
            "Epoch 722/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0533 - accuracy: 0.9915 - val_loss: 0.0636 - val_accuracy: 0.9874\n",
            "Epoch 723/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0548 - accuracy: 0.9909 - val_loss: 0.0741 - val_accuracy: 0.9836\n",
            "Epoch 724/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0681 - accuracy: 0.9853 - val_loss: 0.0935 - val_accuracy: 0.9760\n",
            "Epoch 725/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0765 - accuracy: 0.9822 - val_loss: 0.0668 - val_accuracy: 0.9862\n",
            "Epoch 726/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0582 - accuracy: 0.9897 - val_loss: 0.0666 - val_accuracy: 0.9860\n",
            "Epoch 727/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0548 - accuracy: 0.9912 - val_loss: 0.0619 - val_accuracy: 0.9880\n",
            "Epoch 728/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0528 - accuracy: 0.9918 - val_loss: 0.0633 - val_accuracy: 0.9877\n",
            "Epoch 729/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0532 - accuracy: 0.9916 - val_loss: 0.0658 - val_accuracy: 0.9865\n",
            "Epoch 730/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0538 - accuracy: 0.9909 - val_loss: 0.0642 - val_accuracy: 0.9872\n",
            "Epoch 731/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.0533 - accuracy: 0.9915 - val_loss: 0.0621 - val_accuracy: 0.9880\n",
            "Epoch 732/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0513 - accuracy: 0.9921 - val_loss: 0.0641 - val_accuracy: 0.9865\n",
            "Epoch 733/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0520 - accuracy: 0.9917 - val_loss: 0.0612 - val_accuracy: 0.9879\n",
            "Epoch 734/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0525 - accuracy: 0.9915 - val_loss: 0.0684 - val_accuracy: 0.9855\n",
            "Epoch 735/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0537 - accuracy: 0.9910 - val_loss: 0.0650 - val_accuracy: 0.9865\n",
            "Epoch 736/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0550 - accuracy: 0.9905 - val_loss: 0.0615 - val_accuracy: 0.9879\n",
            "Epoch 737/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0519 - accuracy: 0.9919 - val_loss: 0.0620 - val_accuracy: 0.9878\n",
            "Epoch 738/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0547 - accuracy: 0.9907 - val_loss: 0.0725 - val_accuracy: 0.9830\n",
            "Epoch 739/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0586 - accuracy: 0.9888 - val_loss: 0.0756 - val_accuracy: 0.9821\n",
            "Epoch 740/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0942 - accuracy: 0.9765 - val_loss: 0.1448 - val_accuracy: 0.9583\n",
            "Epoch 741/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1107 - accuracy: 0.9700 - val_loss: 0.0918 - val_accuracy: 0.9766\n",
            "Epoch 742/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0748 - accuracy: 0.9828 - val_loss: 0.0797 - val_accuracy: 0.9803\n",
            "Epoch 743/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0651 - accuracy: 0.9865 - val_loss: 0.0692 - val_accuracy: 0.9855\n",
            "Epoch 744/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0575 - accuracy: 0.9896 - val_loss: 0.0616 - val_accuracy: 0.9878\n",
            "Epoch 745/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0538 - accuracy: 0.9912 - val_loss: 0.0611 - val_accuracy: 0.9883\n",
            "Epoch 746/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0512 - accuracy: 0.9922 - val_loss: 0.0613 - val_accuracy: 0.9884\n",
            "Epoch 747/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0503 - accuracy: 0.9924 - val_loss: 0.0606 - val_accuracy: 0.9883\n",
            "Epoch 748/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0502 - accuracy: 0.9925 - val_loss: 0.0636 - val_accuracy: 0.9872\n",
            "Epoch 749/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0513 - accuracy: 0.9922 - val_loss: 0.0612 - val_accuracy: 0.9881\n",
            "Epoch 750/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0504 - accuracy: 0.9925 - val_loss: 0.0610 - val_accuracy: 0.9887\n",
            "Epoch 751/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0546 - accuracy: 0.9905 - val_loss: 0.0692 - val_accuracy: 0.9847\n",
            "Epoch 752/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0550 - accuracy: 0.9903 - val_loss: 0.0633 - val_accuracy: 0.9873\n",
            "Epoch 753/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0518 - accuracy: 0.9918 - val_loss: 0.0599 - val_accuracy: 0.9886\n",
            "Epoch 754/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0498 - accuracy: 0.9928 - val_loss: 0.0604 - val_accuracy: 0.9881\n",
            "Epoch 755/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0492 - accuracy: 0.9930 - val_loss: 0.0653 - val_accuracy: 0.9869\n",
            "Epoch 756/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0514 - accuracy: 0.9918 - val_loss: 0.0642 - val_accuracy: 0.9871\n",
            "Epoch 757/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0538 - accuracy: 0.9909 - val_loss: 0.0600 - val_accuracy: 0.9885\n",
            "Epoch 758/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0531 - accuracy: 0.9913 - val_loss: 0.0666 - val_accuracy: 0.9855\n",
            "Epoch 759/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0575 - accuracy: 0.9888 - val_loss: 0.0641 - val_accuracy: 0.9864\n",
            "Epoch 760/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0520 - accuracy: 0.9915 - val_loss: 0.0608 - val_accuracy: 0.9884\n",
            "Epoch 761/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0498 - accuracy: 0.9926 - val_loss: 0.0672 - val_accuracy: 0.9856\n",
            "Epoch 762/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0608 - accuracy: 0.9879 - val_loss: 0.0950 - val_accuracy: 0.9748\n",
            "Epoch 763/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0874 - accuracy: 0.9784 - val_loss: 0.0893 - val_accuracy: 0.9770\n",
            "Epoch 764/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0712 - accuracy: 0.9838 - val_loss: 0.0718 - val_accuracy: 0.9839\n",
            "Epoch 765/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0580 - accuracy: 0.9896 - val_loss: 0.0629 - val_accuracy: 0.9870\n",
            "Epoch 766/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0558 - accuracy: 0.9901 - val_loss: 0.0618 - val_accuracy: 0.9876\n",
            "Epoch 767/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0510 - accuracy: 0.9923 - val_loss: 0.0617 - val_accuracy: 0.9879\n",
            "Epoch 768/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0510 - accuracy: 0.9921 - val_loss: 0.0630 - val_accuracy: 0.9877\n",
            "Epoch 769/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0498 - accuracy: 0.9923 - val_loss: 0.0599 - val_accuracy: 0.9886\n",
            "Epoch 770/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0493 - accuracy: 0.9925 - val_loss: 0.0577 - val_accuracy: 0.9891\n",
            "Epoch 771/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0481 - accuracy: 0.9932 - val_loss: 0.0597 - val_accuracy: 0.9885\n",
            "Epoch 772/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0489 - accuracy: 0.9929 - val_loss: 0.0580 - val_accuracy: 0.9893\n",
            "Epoch 773/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0486 - accuracy: 0.9931 - val_loss: 0.0620 - val_accuracy: 0.9870\n",
            "Epoch 774/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0510 - accuracy: 0.9916 - val_loss: 0.0581 - val_accuracy: 0.9898\n",
            "Epoch 775/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0497 - accuracy: 0.9924 - val_loss: 0.0590 - val_accuracy: 0.9892\n",
            "Epoch 776/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0492 - accuracy: 0.9927 - val_loss: 0.0581 - val_accuracy: 0.9892\n",
            "Epoch 777/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0499 - accuracy: 0.9922 - val_loss: 0.0611 - val_accuracy: 0.9875\n",
            "Epoch 778/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0521 - accuracy: 0.9914 - val_loss: 0.0586 - val_accuracy: 0.9887\n",
            "Epoch 779/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0483 - accuracy: 0.9927 - val_loss: 0.0613 - val_accuracy: 0.9878\n",
            "Epoch 780/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0500 - accuracy: 0.9921 - val_loss: 0.0642 - val_accuracy: 0.9865\n",
            "Epoch 781/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0708 - accuracy: 0.9841 - val_loss: 0.1513 - val_accuracy: 0.9597\n",
            "Epoch 782/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1682 - accuracy: 0.9505 - val_loss: 0.1563 - val_accuracy: 0.9514\n",
            "Epoch 783/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1240 - accuracy: 0.9637 - val_loss: 0.0904 - val_accuracy: 0.9772\n",
            "Epoch 784/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0774 - accuracy: 0.9823 - val_loss: 0.0697 - val_accuracy: 0.9849\n",
            "Epoch 785/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0585 - accuracy: 0.9893 - val_loss: 0.0607 - val_accuracy: 0.9886\n",
            "Epoch 786/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0526 - accuracy: 0.9916 - val_loss: 0.0588 - val_accuracy: 0.9889\n",
            "Epoch 787/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0501 - accuracy: 0.9923 - val_loss: 0.0579 - val_accuracy: 0.9890\n",
            "Epoch 788/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0484 - accuracy: 0.9932 - val_loss: 0.0573 - val_accuracy: 0.9897\n",
            "Epoch 789/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0482 - accuracy: 0.9931 - val_loss: 0.0581 - val_accuracy: 0.9890\n",
            "Epoch 790/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0480 - accuracy: 0.9932 - val_loss: 0.0577 - val_accuracy: 0.9899\n",
            "Epoch 791/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0478 - accuracy: 0.9933 - val_loss: 0.0586 - val_accuracy: 0.9889\n",
            "Epoch 792/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0477 - accuracy: 0.9931 - val_loss: 0.0632 - val_accuracy: 0.9873\n",
            "Epoch 793/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0492 - accuracy: 0.9923 - val_loss: 0.0585 - val_accuracy: 0.9892\n",
            "Epoch 794/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0476 - accuracy: 0.9934 - val_loss: 0.0579 - val_accuracy: 0.9896\n",
            "Epoch 795/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0475 - accuracy: 0.9931 - val_loss: 0.0579 - val_accuracy: 0.9895\n",
            "Epoch 796/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0486 - accuracy: 0.9927 - val_loss: 0.0579 - val_accuracy: 0.9895\n",
            "Epoch 797/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0472 - accuracy: 0.9934 - val_loss: 0.0577 - val_accuracy: 0.9888\n",
            "Epoch 798/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0467 - accuracy: 0.9933 - val_loss: 0.0574 - val_accuracy: 0.9891\n",
            "Epoch 799/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0471 - accuracy: 0.9935 - val_loss: 0.0558 - val_accuracy: 0.9897\n",
            "Epoch 800/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0464 - accuracy: 0.9939 - val_loss: 0.0573 - val_accuracy: 0.9894\n",
            "Epoch 801/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0468 - accuracy: 0.9932 - val_loss: 0.0560 - val_accuracy: 0.9900\n",
            "Epoch 802/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0468 - accuracy: 0.9935 - val_loss: 0.0565 - val_accuracy: 0.9894\n",
            "Epoch 803/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0463 - accuracy: 0.9938 - val_loss: 0.0562 - val_accuracy: 0.9896\n",
            "Epoch 804/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0467 - accuracy: 0.9933 - val_loss: 0.0599 - val_accuracy: 0.9881\n",
            "Epoch 805/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0494 - accuracy: 0.9922 - val_loss: 0.0620 - val_accuracy: 0.9869\n",
            "Epoch 806/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0535 - accuracy: 0.9905 - val_loss: 0.0638 - val_accuracy: 0.9865\n",
            "Epoch 807/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0530 - accuracy: 0.9906 - val_loss: 0.0649 - val_accuracy: 0.9857\n",
            "Epoch 808/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0552 - accuracy: 0.9897 - val_loss: 0.0628 - val_accuracy: 0.9866\n",
            "Epoch 809/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0560 - accuracy: 0.9894 - val_loss: 0.0618 - val_accuracy: 0.9880\n",
            "Epoch 810/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0521 - accuracy: 0.9915 - val_loss: 0.0634 - val_accuracy: 0.9871\n",
            "Epoch 811/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0496 - accuracy: 0.9922 - val_loss: 0.0604 - val_accuracy: 0.9878\n",
            "Epoch 812/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0501 - accuracy: 0.9921 - val_loss: 0.0639 - val_accuracy: 0.9863\n",
            "Epoch 813/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0533 - accuracy: 0.9908 - val_loss: 0.0675 - val_accuracy: 0.9856\n",
            "Epoch 814/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0524 - accuracy: 0.9910 - val_loss: 0.0570 - val_accuracy: 0.9902\n",
            "Epoch 815/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0478 - accuracy: 0.9930 - val_loss: 0.0720 - val_accuracy: 0.9833\n",
            "Epoch 816/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0563 - accuracy: 0.9889 - val_loss: 0.0688 - val_accuracy: 0.9849\n",
            "Epoch 817/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0548 - accuracy: 0.9901 - val_loss: 0.0711 - val_accuracy: 0.9834\n",
            "Epoch 818/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0562 - accuracy: 0.9896 - val_loss: 0.0646 - val_accuracy: 0.9858\n",
            "Epoch 819/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0530 - accuracy: 0.9909 - val_loss: 0.0567 - val_accuracy: 0.9898\n",
            "Epoch 820/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0504 - accuracy: 0.9920 - val_loss: 0.0559 - val_accuracy: 0.9908\n",
            "Epoch 821/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0476 - accuracy: 0.9933 - val_loss: 0.0573 - val_accuracy: 0.9892\n",
            "Epoch 822/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0476 - accuracy: 0.9930 - val_loss: 0.0547 - val_accuracy: 0.9904\n",
            "Epoch 823/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0463 - accuracy: 0.9936 - val_loss: 0.0564 - val_accuracy: 0.9892\n",
            "Epoch 824/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0464 - accuracy: 0.9933 - val_loss: 0.0557 - val_accuracy: 0.9898\n",
            "Epoch 825/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0501 - accuracy: 0.9919 - val_loss: 0.0642 - val_accuracy: 0.9859\n",
            "Epoch 826/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0528 - accuracy: 0.9907 - val_loss: 0.0631 - val_accuracy: 0.9872\n",
            "Epoch 827/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0526 - accuracy: 0.9910 - val_loss: 0.0591 - val_accuracy: 0.9888\n",
            "Epoch 828/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0480 - accuracy: 0.9929 - val_loss: 0.0546 - val_accuracy: 0.9897\n",
            "Epoch 829/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0454 - accuracy: 0.9938 - val_loss: 0.0544 - val_accuracy: 0.9901\n",
            "Epoch 830/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0446 - accuracy: 0.9941 - val_loss: 0.0552 - val_accuracy: 0.9903\n",
            "Epoch 831/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0442 - accuracy: 0.9945 - val_loss: 0.0531 - val_accuracy: 0.9908\n",
            "Epoch 832/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0443 - accuracy: 0.9942 - val_loss: 0.0549 - val_accuracy: 0.9899\n",
            "Epoch 833/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0459 - accuracy: 0.9935 - val_loss: 0.0570 - val_accuracy: 0.9889\n",
            "Epoch 834/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0475 - accuracy: 0.9928 - val_loss: 0.0620 - val_accuracy: 0.9872\n",
            "Epoch 835/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0495 - accuracy: 0.9922 - val_loss: 0.0688 - val_accuracy: 0.9845\n",
            "Epoch 836/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0555 - accuracy: 0.9896 - val_loss: 0.0609 - val_accuracy: 0.9884\n",
            "Epoch 837/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0488 - accuracy: 0.9926 - val_loss: 0.0565 - val_accuracy: 0.9895\n",
            "Epoch 838/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0464 - accuracy: 0.9934 - val_loss: 0.0535 - val_accuracy: 0.9906\n",
            "Epoch 839/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0467 - accuracy: 0.9933 - val_loss: 0.0567 - val_accuracy: 0.9896\n",
            "Epoch 840/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0477 - accuracy: 0.9931 - val_loss: 0.0553 - val_accuracy: 0.9898\n",
            "Epoch 841/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0446 - accuracy: 0.9942 - val_loss: 0.0530 - val_accuracy: 0.9908\n",
            "Epoch 842/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0452 - accuracy: 0.9938 - val_loss: 0.0537 - val_accuracy: 0.9907\n",
            "Epoch 843/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.0444 - accuracy: 0.9943 - val_loss: 0.0560 - val_accuracy: 0.9898\n",
            "Epoch 844/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0435 - accuracy: 0.9944 - val_loss: 0.0537 - val_accuracy: 0.9906\n",
            "Epoch 845/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0431 - accuracy: 0.9947 - val_loss: 0.0549 - val_accuracy: 0.9896\n",
            "Epoch 846/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0444 - accuracy: 0.9940 - val_loss: 0.0625 - val_accuracy: 0.9871\n",
            "Epoch 847/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0738 - accuracy: 0.9838 - val_loss: 0.1852 - val_accuracy: 0.9490\n",
            "Epoch 848/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1699 - accuracy: 0.9504 - val_loss: 0.1065 - val_accuracy: 0.9704\n",
            "Epoch 849/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0930 - accuracy: 0.9757 - val_loss: 0.0796 - val_accuracy: 0.9810\n",
            "Epoch 850/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0631 - accuracy: 0.9869 - val_loss: 0.0632 - val_accuracy: 0.9863\n",
            "Epoch 851/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0527 - accuracy: 0.9912 - val_loss: 0.0566 - val_accuracy: 0.9900\n",
            "Epoch 852/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0476 - accuracy: 0.9934 - val_loss: 0.0546 - val_accuracy: 0.9900\n",
            "Epoch 853/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0452 - accuracy: 0.9941 - val_loss: 0.0560 - val_accuracy: 0.9890\n",
            "Epoch 854/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0464 - accuracy: 0.9934 - val_loss: 0.0539 - val_accuracy: 0.9902\n",
            "Epoch 855/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0437 - accuracy: 0.9945 - val_loss: 0.0525 - val_accuracy: 0.9910\n",
            "Epoch 856/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0433 - accuracy: 0.9947 - val_loss: 0.0542 - val_accuracy: 0.9903\n",
            "Epoch 857/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0446 - accuracy: 0.9941 - val_loss: 0.0612 - val_accuracy: 0.9875\n",
            "Epoch 858/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0476 - accuracy: 0.9930 - val_loss: 0.0537 - val_accuracy: 0.9903\n",
            "Epoch 859/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0456 - accuracy: 0.9936 - val_loss: 0.0537 - val_accuracy: 0.9902\n",
            "Epoch 860/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0450 - accuracy: 0.9937 - val_loss: 0.0540 - val_accuracy: 0.9903\n",
            "Epoch 861/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0452 - accuracy: 0.9936 - val_loss: 0.0551 - val_accuracy: 0.9893\n",
            "Epoch 862/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0456 - accuracy: 0.9935 - val_loss: 0.0638 - val_accuracy: 0.9862\n",
            "Epoch 863/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0495 - accuracy: 0.9919 - val_loss: 0.0694 - val_accuracy: 0.9837\n",
            "Epoch 864/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0549 - accuracy: 0.9896 - val_loss: 0.0597 - val_accuracy: 0.9874\n",
            "Epoch 865/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0471 - accuracy: 0.9930 - val_loss: 0.0535 - val_accuracy: 0.9908\n",
            "Epoch 866/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0433 - accuracy: 0.9944 - val_loss: 0.0571 - val_accuracy: 0.9892\n",
            "Epoch 867/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0468 - accuracy: 0.9930 - val_loss: 0.0543 - val_accuracy: 0.9903\n",
            "Epoch 868/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0437 - accuracy: 0.9943 - val_loss: 0.0529 - val_accuracy: 0.9912\n",
            "Epoch 869/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0416 - accuracy: 0.9954 - val_loss: 0.0515 - val_accuracy: 0.9916\n",
            "Epoch 870/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0427 - accuracy: 0.9946 - val_loss: 0.0563 - val_accuracy: 0.9894\n",
            "Epoch 871/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0448 - accuracy: 0.9937 - val_loss: 0.0581 - val_accuracy: 0.9885\n",
            "Epoch 872/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0479 - accuracy: 0.9925 - val_loss: 0.0880 - val_accuracy: 0.9777\n",
            "Epoch 873/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0657 - accuracy: 0.9858 - val_loss: 0.0668 - val_accuracy: 0.9857\n",
            "Epoch 874/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0599 - accuracy: 0.9882 - val_loss: 0.0596 - val_accuracy: 0.9883\n",
            "Epoch 875/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0512 - accuracy: 0.9911 - val_loss: 0.0642 - val_accuracy: 0.9860\n",
            "Epoch 876/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0498 - accuracy: 0.9917 - val_loss: 0.0556 - val_accuracy: 0.9897\n",
            "Epoch 877/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0451 - accuracy: 0.9939 - val_loss: 0.0535 - val_accuracy: 0.9906\n",
            "Epoch 878/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0440 - accuracy: 0.9940 - val_loss: 0.0518 - val_accuracy: 0.9915\n",
            "Epoch 879/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0429 - accuracy: 0.9945 - val_loss: 0.0526 - val_accuracy: 0.9911\n",
            "Epoch 880/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0427 - accuracy: 0.9947 - val_loss: 0.0507 - val_accuracy: 0.9916\n",
            "Epoch 881/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0413 - accuracy: 0.9953 - val_loss: 0.0509 - val_accuracy: 0.9916\n",
            "Epoch 882/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0415 - accuracy: 0.9950 - val_loss: 0.0509 - val_accuracy: 0.9916\n",
            "Epoch 883/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0445 - accuracy: 0.9938 - val_loss: 0.0719 - val_accuracy: 0.9848\n",
            "Epoch 884/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0601 - accuracy: 0.9879 - val_loss: 0.0803 - val_accuracy: 0.9803\n",
            "Epoch 885/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0590 - accuracy: 0.9877 - val_loss: 0.0633 - val_accuracy: 0.9864\n",
            "Epoch 886/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0526 - accuracy: 0.9905 - val_loss: 0.0566 - val_accuracy: 0.9888\n",
            "Epoch 887/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0477 - accuracy: 0.9925 - val_loss: 0.0577 - val_accuracy: 0.9882\n",
            "Epoch 888/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0466 - accuracy: 0.9931 - val_loss: 0.0560 - val_accuracy: 0.9896\n",
            "Epoch 889/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0488 - accuracy: 0.9920 - val_loss: 0.0561 - val_accuracy: 0.9895\n",
            "Epoch 890/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0461 - accuracy: 0.9932 - val_loss: 0.0524 - val_accuracy: 0.9905\n",
            "Epoch 891/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0438 - accuracy: 0.9942 - val_loss: 0.0518 - val_accuracy: 0.9908\n",
            "Epoch 892/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0426 - accuracy: 0.9949 - val_loss: 0.0526 - val_accuracy: 0.9904\n",
            "Epoch 893/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0439 - accuracy: 0.9941 - val_loss: 0.0535 - val_accuracy: 0.9904\n",
            "Epoch 894/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0441 - accuracy: 0.9939 - val_loss: 0.0585 - val_accuracy: 0.9885\n",
            "Epoch 895/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0491 - accuracy: 0.9918 - val_loss: 0.0555 - val_accuracy: 0.9898\n",
            "Epoch 896/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0441 - accuracy: 0.9941 - val_loss: 0.0507 - val_accuracy: 0.9915\n",
            "Epoch 897/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0428 - accuracy: 0.9947 - val_loss: 0.0511 - val_accuracy: 0.9912\n",
            "Epoch 898/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0436 - accuracy: 0.9941 - val_loss: 0.0515 - val_accuracy: 0.9911\n",
            "Epoch 899/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0419 - accuracy: 0.9949 - val_loss: 0.0538 - val_accuracy: 0.9902\n",
            "Epoch 900/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0427 - accuracy: 0.9946 - val_loss: 0.0499 - val_accuracy: 0.9918\n",
            "Epoch 901/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0428 - accuracy: 0.9943 - val_loss: 0.0518 - val_accuracy: 0.9906\n",
            "Epoch 902/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0423 - accuracy: 0.9943 - val_loss: 0.0604 - val_accuracy: 0.9867\n",
            "Epoch 903/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0469 - accuracy: 0.9926 - val_loss: 0.0582 - val_accuracy: 0.9879\n",
            "Epoch 904/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0494 - accuracy: 0.9915 - val_loss: 0.0694 - val_accuracy: 0.9844\n",
            "Epoch 905/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0534 - accuracy: 0.9904 - val_loss: 0.0566 - val_accuracy: 0.9885\n",
            "Epoch 906/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0483 - accuracy: 0.9922 - val_loss: 0.0600 - val_accuracy: 0.9869\n",
            "Epoch 907/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0501 - accuracy: 0.9912 - val_loss: 0.0572 - val_accuracy: 0.9890\n",
            "Epoch 908/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0453 - accuracy: 0.9937 - val_loss: 0.0545 - val_accuracy: 0.9901\n",
            "Epoch 909/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0434 - accuracy: 0.9943 - val_loss: 0.0538 - val_accuracy: 0.9901\n",
            "Epoch 910/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0436 - accuracy: 0.9941 - val_loss: 0.0521 - val_accuracy: 0.9912\n",
            "Epoch 911/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0456 - accuracy: 0.9932 - val_loss: 0.0527 - val_accuracy: 0.9909\n",
            "Epoch 912/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0434 - accuracy: 0.9942 - val_loss: 0.0528 - val_accuracy: 0.9907\n",
            "Epoch 913/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0450 - accuracy: 0.9935 - val_loss: 0.0534 - val_accuracy: 0.9895\n",
            "Epoch 914/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0445 - accuracy: 0.9938 - val_loss: 0.0517 - val_accuracy: 0.9913\n",
            "Epoch 915/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0428 - accuracy: 0.9944 - val_loss: 0.0505 - val_accuracy: 0.9914\n",
            "Epoch 916/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0421 - accuracy: 0.9946 - val_loss: 0.0528 - val_accuracy: 0.9908\n",
            "Epoch 917/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0433 - accuracy: 0.9942 - val_loss: 0.0509 - val_accuracy: 0.9912\n",
            "Epoch 918/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0425 - accuracy: 0.9943 - val_loss: 0.0530 - val_accuracy: 0.9905\n",
            "Epoch 919/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0440 - accuracy: 0.9938 - val_loss: 0.0619 - val_accuracy: 0.9862\n",
            "Epoch 920/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0492 - accuracy: 0.9916 - val_loss: 0.0573 - val_accuracy: 0.9889\n",
            "Epoch 921/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0492 - accuracy: 0.9916 - val_loss: 0.0543 - val_accuracy: 0.9900\n",
            "Epoch 922/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0452 - accuracy: 0.9933 - val_loss: 0.0538 - val_accuracy: 0.9900\n",
            "Epoch 923/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0482 - accuracy: 0.9920 - val_loss: 0.0552 - val_accuracy: 0.9897\n",
            "Epoch 924/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0468 - accuracy: 0.9928 - val_loss: 0.0558 - val_accuracy: 0.9895\n",
            "Epoch 925/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0460 - accuracy: 0.9931 - val_loss: 0.0556 - val_accuracy: 0.9896\n",
            "Epoch 926/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0447 - accuracy: 0.9937 - val_loss: 0.0613 - val_accuracy: 0.9871\n",
            "Epoch 927/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0482 - accuracy: 0.9920 - val_loss: 0.0510 - val_accuracy: 0.9919\n",
            "Epoch 928/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0431 - accuracy: 0.9942 - val_loss: 0.0541 - val_accuracy: 0.9900\n",
            "Epoch 929/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0437 - accuracy: 0.9935 - val_loss: 0.0493 - val_accuracy: 0.9918\n",
            "Epoch 930/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0410 - accuracy: 0.9952 - val_loss: 0.0492 - val_accuracy: 0.9921\n",
            "Epoch 931/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0399 - accuracy: 0.9955 - val_loss: 0.0504 - val_accuracy: 0.9915\n",
            "Epoch 932/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0413 - accuracy: 0.9949 - val_loss: 0.0523 - val_accuracy: 0.9903\n",
            "Epoch 933/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0434 - accuracy: 0.9941 - val_loss: 0.0517 - val_accuracy: 0.9907\n",
            "Epoch 934/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0418 - accuracy: 0.9948 - val_loss: 0.0561 - val_accuracy: 0.9884\n",
            "Epoch 935/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0427 - accuracy: 0.9942 - val_loss: 0.0516 - val_accuracy: 0.9909\n",
            "Epoch 936/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0415 - accuracy: 0.9949 - val_loss: 0.0512 - val_accuracy: 0.9911\n",
            "Epoch 937/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0417 - accuracy: 0.9948 - val_loss: 0.0510 - val_accuracy: 0.9916\n",
            "Epoch 938/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0411 - accuracy: 0.9951 - val_loss: 0.0538 - val_accuracy: 0.9901\n",
            "Epoch 939/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0416 - accuracy: 0.9946 - val_loss: 0.0513 - val_accuracy: 0.9909\n",
            "Epoch 940/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0398 - accuracy: 0.9955 - val_loss: 0.0493 - val_accuracy: 0.9921\n",
            "Epoch 941/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0398 - accuracy: 0.9955 - val_loss: 0.0490 - val_accuracy: 0.9918\n",
            "Epoch 942/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0403 - accuracy: 0.9953 - val_loss: 0.0527 - val_accuracy: 0.9903\n",
            "Epoch 943/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0407 - accuracy: 0.9947 - val_loss: 0.0502 - val_accuracy: 0.9916\n",
            "Epoch 944/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0853 - accuracy: 0.9822 - val_loss: 0.1365 - val_accuracy: 0.9613\n",
            "Epoch 945/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1619 - accuracy: 0.9512 - val_loss: 0.1389 - val_accuracy: 0.9565\n",
            "Epoch 946/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1069 - accuracy: 0.9697 - val_loss: 0.0804 - val_accuracy: 0.9803\n",
            "Epoch 947/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0678 - accuracy: 0.9845 - val_loss: 0.0649 - val_accuracy: 0.9864\n",
            "Epoch 948/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0536 - accuracy: 0.9905 - val_loss: 0.0542 - val_accuracy: 0.9900\n",
            "Epoch 949/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0451 - accuracy: 0.9939 - val_loss: 0.0514 - val_accuracy: 0.9915\n",
            "Epoch 950/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0427 - accuracy: 0.9949 - val_loss: 0.0506 - val_accuracy: 0.9916\n",
            "Epoch 951/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0416 - accuracy: 0.9951 - val_loss: 0.0514 - val_accuracy: 0.9912\n",
            "Epoch 952/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0412 - accuracy: 0.9948 - val_loss: 0.0517 - val_accuracy: 0.9905\n",
            "Epoch 953/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0415 - accuracy: 0.9947 - val_loss: 0.0479 - val_accuracy: 0.9924\n",
            "Epoch 954/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0398 - accuracy: 0.9957 - val_loss: 0.0487 - val_accuracy: 0.9922\n",
            "Epoch 955/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0395 - accuracy: 0.9959 - val_loss: 0.0489 - val_accuracy: 0.9920\n",
            "Epoch 956/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0404 - accuracy: 0.9954 - val_loss: 0.0492 - val_accuracy: 0.9919\n",
            "Epoch 957/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0393 - accuracy: 0.9956 - val_loss: 0.0515 - val_accuracy: 0.9905\n",
            "Epoch 958/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0416 - accuracy: 0.9945 - val_loss: 0.0495 - val_accuracy: 0.9918\n",
            "Epoch 959/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0416 - accuracy: 0.9947 - val_loss: 0.0513 - val_accuracy: 0.9909\n",
            "Epoch 960/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0438 - accuracy: 0.9935 - val_loss: 0.0507 - val_accuracy: 0.9915\n",
            "Epoch 961/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0475 - accuracy: 0.9925 - val_loss: 0.0637 - val_accuracy: 0.9857\n",
            "Epoch 962/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0485 - accuracy: 0.9919 - val_loss: 0.0509 - val_accuracy: 0.9907\n",
            "Epoch 963/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0435 - accuracy: 0.9939 - val_loss: 0.0506 - val_accuracy: 0.9910\n",
            "Epoch 964/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0404 - accuracy: 0.9950 - val_loss: 0.0493 - val_accuracy: 0.9914\n",
            "Epoch 965/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0406 - accuracy: 0.9950 - val_loss: 0.0480 - val_accuracy: 0.9926\n",
            "Epoch 966/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0384 - accuracy: 0.9961 - val_loss: 0.0472 - val_accuracy: 0.9927\n",
            "Epoch 967/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0382 - accuracy: 0.9962 - val_loss: 0.0519 - val_accuracy: 0.9905\n",
            "Epoch 968/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0404 - accuracy: 0.9952 - val_loss: 0.0475 - val_accuracy: 0.9926\n",
            "Epoch 969/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0390 - accuracy: 0.9960 - val_loss: 0.0475 - val_accuracy: 0.9923\n",
            "Epoch 970/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0385 - accuracy: 0.9962 - val_loss: 0.0470 - val_accuracy: 0.9928\n",
            "Epoch 971/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0424 - accuracy: 0.9944 - val_loss: 0.1580 - val_accuracy: 0.9537\n",
            "Epoch 972/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1983 - accuracy: 0.9475 - val_loss: 0.1310 - val_accuracy: 0.9602\n",
            "Epoch 973/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1028 - accuracy: 0.9713 - val_loss: 0.0723 - val_accuracy: 0.9825\n",
            "Epoch 974/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0622 - accuracy: 0.9864 - val_loss: 0.0649 - val_accuracy: 0.9856\n",
            "Epoch 975/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0511 - accuracy: 0.9912 - val_loss: 0.0534 - val_accuracy: 0.9901\n",
            "Epoch 976/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0444 - accuracy: 0.9939 - val_loss: 0.0492 - val_accuracy: 0.9922\n",
            "Epoch 977/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0413 - accuracy: 0.9953 - val_loss: 0.0501 - val_accuracy: 0.9916\n",
            "Epoch 978/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0404 - accuracy: 0.9954 - val_loss: 0.0474 - val_accuracy: 0.9927\n",
            "Epoch 979/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0410 - accuracy: 0.9952 - val_loss: 0.0482 - val_accuracy: 0.9916\n",
            "Epoch 980/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0395 - accuracy: 0.9954 - val_loss: 0.0507 - val_accuracy: 0.9917\n",
            "Epoch 981/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0410 - accuracy: 0.9949 - val_loss: 0.0507 - val_accuracy: 0.9914\n",
            "Epoch 982/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0400 - accuracy: 0.9951 - val_loss: 0.0485 - val_accuracy: 0.9922\n",
            "Epoch 983/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0393 - accuracy: 0.9955 - val_loss: 0.0479 - val_accuracy: 0.9926\n",
            "Epoch 984/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0388 - accuracy: 0.9956 - val_loss: 0.0474 - val_accuracy: 0.9925\n",
            "Epoch 985/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0388 - accuracy: 0.9959 - val_loss: 0.0497 - val_accuracy: 0.9910\n",
            "Epoch 986/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0442 - accuracy: 0.9934 - val_loss: 0.0596 - val_accuracy: 0.9879\n",
            "Epoch 987/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0459 - accuracy: 0.9927 - val_loss: 0.0498 - val_accuracy: 0.9916\n",
            "Epoch 988/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0396 - accuracy: 0.9956 - val_loss: 0.0463 - val_accuracy: 0.9930\n",
            "Epoch 989/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0375 - accuracy: 0.9964 - val_loss: 0.0470 - val_accuracy: 0.9926\n",
            "Epoch 990/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0379 - accuracy: 0.9962 - val_loss: 0.0466 - val_accuracy: 0.9929\n",
            "Epoch 991/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0379 - accuracy: 0.9959 - val_loss: 0.0499 - val_accuracy: 0.9919\n",
            "Epoch 992/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0399 - accuracy: 0.9951 - val_loss: 0.0465 - val_accuracy: 0.9930\n",
            "Epoch 993/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0375 - accuracy: 0.9962 - val_loss: 0.0475 - val_accuracy: 0.9928\n",
            "Epoch 994/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0383 - accuracy: 0.9958 - val_loss: 0.0452 - val_accuracy: 0.9931\n",
            "Epoch 995/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0380 - accuracy: 0.9959 - val_loss: 0.0476 - val_accuracy: 0.9926\n",
            "Epoch 996/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0383 - accuracy: 0.9955 - val_loss: 0.0502 - val_accuracy: 0.9912\n",
            "Epoch 997/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0384 - accuracy: 0.9957 - val_loss: 0.0505 - val_accuracy: 0.9907\n",
            "Epoch 998/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0398 - accuracy: 0.9950 - val_loss: 0.0543 - val_accuracy: 0.9889\n",
            "Epoch 999/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0436 - accuracy: 0.9934 - val_loss: 0.0548 - val_accuracy: 0.9886\n",
            "Epoch 1000/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0439 - accuracy: 0.9934 - val_loss: 0.0516 - val_accuracy: 0.9899\n",
            "CPU times: user 14min 33s, sys: 41.2 s, total: 15min 15s\n",
            "Wall time: 14min 6s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9YV8mgUXDwO"
      },
      "source": [
        "## 4) Visualization(Loss, Accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSEBhQLJW0b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "dc924049-114a-400e-d0fd-66e72fc45cd0"
      },
      "source": [
        "epochs = range(1, len(hist.history['loss']) + 1)\n",
        "\n",
        "plt.figure(figsize = (9, 6))\n",
        "plt.plot(epochs, hist.history['loss'])\n",
        "plt.plot(epochs, hist.history['val_loss'])\n",
        "\n",
        "plt.title('Training & Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "FvHLNFU0KfEu",
        "outputId": "b9a247c4-57d3-4128-9ae1-bb6d2b3f9001"
      },
      "source": [
        "epochs = range(1, len(hist.history['loss']) + 1)\n",
        "\n",
        "plt.figure(figsize = (9, 6))\n",
        "plt.plot(epochs, hist.history['accuracy'])\n",
        "plt.plot(epochs, hist.history['val_accuracy'])\n",
        "\n",
        "plt.title('Training & Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH43j2ZpXBbi"
      },
      "source": [
        "## 5) Model Evaluate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08Bp9GHwXIG2",
        "outputId": "1d6b7c4b-bf5f-4454-ddca-3b5ca09cd88c"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test,\n",
        "                                batch_size = batch_size)\n",
        "\n",
        "print('Loss = {:.5f}'.format(loss))\n",
        "print('Accuracy = {:.5f}'.format(accuracy))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0500 - accuracy: 0.9899\n",
            "Loss = 0.05004\n",
            "Accuracy = 0.98993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjztcGV6YgcZ"
      },
      "source": [
        "## 6) Model Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg0630NOYt0e"
      },
      "source": [
        "abs_path = '/content/drive/MyDrive/Project/Project_Gotcha/models/'\n",
        "model_name = 'lstm_1_june_16_50.h5'\n",
        "final_path = abs_path + model_name"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYDr_nSBYhqJ"
      },
      "source": [
        "model.save(final_path)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vfo5idsZazR"
      },
      "source": [
        "# 3. Model Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71J-QkVsZEhz"
      },
      "source": [
        "## 1) Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "FL_TV9eV9sFP",
        "outputId": "938c98f3-e1f5-41da-bbf9-81e33dd0f153"
      },
      "source": [
        "test_path = '/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/old/real_test.csv'\n",
        "\n",
        "test_sample = pd.read_csv(test_path)\n",
        "test_sample.head()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>131.090</td>\n",
              "      <td>158.404</td>\n",
              "      <td>104.906</td>\n",
              "      <td>183.220</td>\n",
              "      <td>110.116</td>\n",
              "      <td>183.185</td>\n",
              "      <td>162.316</td>\n",
              "      <td>193.695</td>\n",
              "      <td>204.122</td>\n",
              "      <td>201.516</td>\n",
              "      <td>97.0971</td>\n",
              "      <td>183.205</td>\n",
              "      <td>124.488</td>\n",
              "      <td>221.078</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>108.863</td>\n",
              "      <td>273.276</td>\n",
              "      <td>127.096</td>\n",
              "      <td>335.911</td>\n",
              "      <td>144.066</td>\n",
              "      <td>402.462</td>\n",
              "      <td>99.7320</td>\n",
              "      <td>270.668</td>\n",
              "      <td>110.185</td>\n",
              "      <td>329.394</td>\n",
              "      <td>97.0830</td>\n",
              "      <td>380.339</td>\n",
              "      <td>128.364</td>\n",
              "      <td>150.563</td>\n",
              "      <td>128.435</td>\n",
              "      <td>150.634</td>\n",
              "      <td>108.793</td>\n",
              "      <td>150.565</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>assault-punch_flip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>131.075</td>\n",
              "      <td>153.246</td>\n",
              "      <td>104.930</td>\n",
              "      <td>183.228</td>\n",
              "      <td>110.141</td>\n",
              "      <td>183.186</td>\n",
              "      <td>162.346</td>\n",
              "      <td>198.904</td>\n",
              "      <td>206.708</td>\n",
              "      <td>202.745</td>\n",
              "      <td>97.1013</td>\n",
              "      <td>183.205</td>\n",
              "      <td>117.939</td>\n",
              "      <td>223.647</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.188</td>\n",
              "      <td>275.846</td>\n",
              "      <td>129.709</td>\n",
              "      <td>337.194</td>\n",
              "      <td>145.385</td>\n",
              "      <td>402.447</td>\n",
              "      <td>99.7283</td>\n",
              "      <td>273.283</td>\n",
              "      <td>110.202</td>\n",
              "      <td>329.406</td>\n",
              "      <td>97.0978</td>\n",
              "      <td>380.340</td>\n",
              "      <td>129.675</td>\n",
              "      <td>149.242</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>111.427</td>\n",
              "      <td>148.008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>assault-punch_flip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>131.052</td>\n",
              "      <td>153.234</td>\n",
              "      <td>107.520</td>\n",
              "      <td>183.257</td>\n",
              "      <td>115.369</td>\n",
              "      <td>183.250</td>\n",
              "      <td>163.639</td>\n",
              "      <td>205.435</td>\n",
              "      <td>205.419</td>\n",
              "      <td>202.846</td>\n",
              "      <td>97.0939</td>\n",
              "      <td>183.219</td>\n",
              "      <td>125.801</td>\n",
              "      <td>223.715</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>114.041</td>\n",
              "      <td>274.578</td>\n",
              "      <td>132.325</td>\n",
              "      <td>335.921</td>\n",
              "      <td>147.994</td>\n",
              "      <td>402.449</td>\n",
              "      <td>99.6768</td>\n",
              "      <td>271.959</td>\n",
              "      <td>111.434</td>\n",
              "      <td>329.372</td>\n",
              "      <td>98.3188</td>\n",
              "      <td>380.352</td>\n",
              "      <td>129.680</td>\n",
              "      <td>149.272</td>\n",
              "      <td>131.060</td>\n",
              "      <td>151.908</td>\n",
              "      <td>111.436</td>\n",
              "      <td>149.258</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>assault-punch_flip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>137.531</td>\n",
              "      <td>153.186</td>\n",
              "      <td>110.139</td>\n",
              "      <td>183.290</td>\n",
              "      <td>120.575</td>\n",
              "      <td>184.531</td>\n",
              "      <td>158.432</td>\n",
              "      <td>222.338</td>\n",
              "      <td>198.883</td>\n",
              "      <td>213.235</td>\n",
              "      <td>99.6877</td>\n",
              "      <td>183.223</td>\n",
              "      <td>127.118</td>\n",
              "      <td>223.729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>119.318</td>\n",
              "      <td>274.603</td>\n",
              "      <td>140.127</td>\n",
              "      <td>337.208</td>\n",
              "      <td>150.532</td>\n",
              "      <td>402.451</td>\n",
              "      <td>103.6160</td>\n",
              "      <td>271.968</td>\n",
              "      <td>114.061</td>\n",
              "      <td>328.141</td>\n",
              "      <td>97.1001</td>\n",
              "      <td>381.564</td>\n",
              "      <td>131.072</td>\n",
              "      <td>147.994</td>\n",
              "      <td>140.091</td>\n",
              "      <td>150.609</td>\n",
              "      <td>116.664</td>\n",
              "      <td>149.275</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>assault-punch_flip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>140.158</td>\n",
              "      <td>153.219</td>\n",
              "      <td>111.470</td>\n",
              "      <td>183.291</td>\n",
              "      <td>121.878</td>\n",
              "      <td>184.541</td>\n",
              "      <td>149.284</td>\n",
              "      <td>224.998</td>\n",
              "      <td>192.374</td>\n",
              "      <td>223.621</td>\n",
              "      <td>101.0190</td>\n",
              "      <td>183.244</td>\n",
              "      <td>128.389</td>\n",
              "      <td>230.197</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>121.917</td>\n",
              "      <td>277.190</td>\n",
              "      <td>141.439</td>\n",
              "      <td>337.226</td>\n",
              "      <td>150.589</td>\n",
              "      <td>401.232</td>\n",
              "      <td>107.5290</td>\n",
              "      <td>275.861</td>\n",
              "      <td>117.926</td>\n",
              "      <td>329.426</td>\n",
              "      <td>98.3216</td>\n",
              "      <td>381.571</td>\n",
              "      <td>137.537</td>\n",
              "      <td>148.007</td>\n",
              "      <td>140.221</td>\n",
              "      <td>150.621</td>\n",
              "      <td>120.601</td>\n",
              "      <td>146.688</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>assault-punch_flip</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0        1        2        3  ...       33   34   35               index\n",
              "0  131.090  158.404  104.906  183.220  ...  150.565  0.0  0.0  assault-punch_flip\n",
              "1  131.075  153.246  104.930  183.228  ...  148.008  0.0  0.0  assault-punch_flip\n",
              "2  131.052  153.234  107.520  183.257  ...  149.258  0.0  0.0  assault-punch_flip\n",
              "3  137.531  153.186  110.139  183.290  ...  149.275  0.0  0.0  assault-punch_flip\n",
              "4  140.158  153.219  111.470  183.291  ...  146.688  0.0  0.0  assault-punch_flip\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn-hwzUqWLvM",
        "outputId": "c2ae839c-7f26-46e7-bd39-7bfaca162c4b"
      },
      "source": [
        "test_sample = deleteJoint(test_sample)\n",
        "test_sample = makeJointXY(test_sample)\n",
        "test_sample = jointAngles(test_sample)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32it [00:00, 16314.30it/s]\n",
            "32it [00:00, 41259.68it/s]\n",
            "32it [00:00, 128931.54it/s]\n",
            "32it [00:00, 106184.91it/s]\n",
            "32it [00:00, 78033.56it/s]\n",
            "32it [00:00, 101296.40it/s]\n",
            "32it [00:00, 30643.32it/s]\n",
            "32it [00:00, 106017.16it/s]\n",
            "32it [00:00, 114520.25it/s]\n",
            "32it [00:00, 108240.10it/s]\n",
            "32it [00:00, 62397.83it/s]\n",
            "32it [00:00, 84043.66it/s]\n",
            "32it [00:00, 16422.09it/s]\n",
            "100%|| 32/32 [00:00<00:00, 87381.33it/s]\n",
            "100%|| 32/32 [00:00<00:00, 130435.11it/s]\n",
            "100%|| 32/32 [00:00<00:00, 5284.79it/s]\n",
            "100%|| 32/32 [00:00<00:00, 90995.07it/s]\n",
            "100%|| 32/32 [00:00<00:00, 17200.79it/s]\n",
            "100%|| 32/32 [00:00<00:00, 126501.16it/s]\n",
            "100%|| 32/32 [00:00<00:00, 20177.05it/s]\n",
            "100%|| 32/32 [00:00<00:00, 112128.43it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "niF4-G5yWyDq",
        "outputId": "d546143e-cca9-49f3-ff58-4bb8cfabe488"
      },
      "source": [
        "test_sample.head()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Ang_RShoulder</th>\n",
              "      <th>Ang_RElbow</th>\n",
              "      <th>Ang_RHip</th>\n",
              "      <th>Ang_RKnee</th>\n",
              "      <th>Ang_LShoulder</th>\n",
              "      <th>Ang_LElbow</th>\n",
              "      <th>Ang_LHip</th>\n",
              "      <th>Ang_LKnee</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>assault-punch_flip</td>\n",
              "      <td>2.936190</td>\n",
              "      <td>-3.127849</td>\n",
              "      <td>-2.902233</td>\n",
              "      <td>-3.175192</td>\n",
              "      <td>-0.942728</td>\n",
              "      <td>-0.113306</td>\n",
              "      <td>-2.906344</td>\n",
              "      <td>-3.569468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>assault-punch_flip</td>\n",
              "      <td>2.841084</td>\n",
              "      <td>-2.935511</td>\n",
              "      <td>-2.890232</td>\n",
              "      <td>-3.213897</td>\n",
              "      <td>-1.092086</td>\n",
              "      <td>0.009511</td>\n",
              "      <td>-2.899397</td>\n",
              "      <td>-3.577908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>assault-punch_flip</td>\n",
              "      <td>2.709890</td>\n",
              "      <td>-2.648894</td>\n",
              "      <td>-2.923201</td>\n",
              "      <td>-3.199961</td>\n",
              "      <td>-0.950486</td>\n",
              "      <td>-0.104408</td>\n",
              "      <td>-2.851410</td>\n",
              "      <td>-3.595384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>assault-punch_flip</td>\n",
              "      <td>-3.807971</td>\n",
              "      <td>-2.135505</td>\n",
              "      <td>-2.920881</td>\n",
              "      <td>-3.304341</td>\n",
              "      <td>-0.969132</td>\n",
              "      <td>-0.078561</td>\n",
              "      <td>-2.884323</td>\n",
              "      <td>-3.632855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>assault-punch_flip</td>\n",
              "      <td>-3.997458</td>\n",
              "      <td>-2.134255</td>\n",
              "      <td>-2.938008</td>\n",
              "      <td>-3.313986</td>\n",
              "      <td>-1.038531</td>\n",
              "      <td>-0.019006</td>\n",
              "      <td>-2.907328</td>\n",
              "      <td>-3.692921</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                index  Ang_RShoulder  ...  Ang_LHip  Ang_LKnee\n",
              "0  assault-punch_flip       2.936190  ... -2.906344  -3.569468\n",
              "1  assault-punch_flip       2.841084  ... -2.899397  -3.577908\n",
              "2  assault-punch_flip       2.709890  ... -2.851410  -3.595384\n",
              "3  assault-punch_flip      -3.807971  ... -2.884323  -3.632855\n",
              "4  assault-punch_flip      -3.997458  ... -2.907328  -3.692921\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W9uMsB9XhZR",
        "outputId": "80337a0c-375a-4586-91d0-dfb7182b6476"
      },
      "source": [
        "X_pre = makeX(test_sample)\n",
        "\n",
        "X_pre.shape"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 32/32 [00:00<00:00, 27213.65it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 32, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfZ8Zbo7ZH4d"
      },
      "source": [
        "## 2) Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMp0HbxoZKth"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "np.set_printoptions(precision=6, suppress=True) # np.array    ( -> )"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whf3h3DVZSyU"
      },
      "source": [
        "loaded_model = load_model(final_path)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDxW_s0TK6jP",
        "outputId": "32131e0f-ee02-4748-b739-a30f12f7f2f7"
      },
      "source": [
        "predict_dict = {}\n",
        "\n",
        "predictions = loaded_model.predict(X_pre)\n",
        "\n",
        "for idx, c in enumerate(classes):\n",
        "    predict_dict[c] = predictions[0][idx]\n",
        "\n",
        "print(predict_dict)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 0.008207416, 1: 0.002974391, 2: 0.9652065, 3: 0.0101480475, 4: 0.013463641}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_bCNdlPMRJO",
        "outputId": "e9b8c5c4-2659-475a-8eb5-634e1bf6b9d1"
      },
      "source": [
        "predict_class = np.argmax(predictions[0], axis=-1)\n",
        "\n",
        "classes[predict_class]"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHo8krbynhsN"
      },
      "source": [
        ""
      ],
      "execution_count": 109,
      "outputs": []
    }
  ]
}